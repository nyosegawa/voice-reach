# 01 — システムアーキテクチャ詳細設計

## 1. アーキテクチャ概要

VoiceReachは4つの主要レイヤーで構成される。各レイヤーの技術選定は14本の調査レポート（Phase A/B）に基づく。

```
┌─────────────────────────────────────────────────────────┐
│ Layer 4: プレゼンテーション層                             │
│   患者UI / 介護者UI / 管理ダッシュボード                  │
├─────────────────────────────────────────────────────────┤
│ Layer 3: インテリジェンス層                               │
│   三段ハイブリッドLLM推論 / 意図空間ナビゲーション / PVP  │
├─────────────────────────────────────────────────────────┤
│ Layer 2: コンテキスト統合層                               │
│   VLM環境認識 / 4ch感情推定 / イベント検出 / 信号統合     │
├─────────────────────────────────────────────────────────┤
│ Layer 1: センシング層                                    │
│   視線推定 / 表情追跡 / デュアルセンサ指入力 / マイク     │
└─────────────────────────────────────────────────────────┘
```

---

## 2. 推奨ハードウェア構成

### 2.1 メインデバイス: Mac mini M4 Pro (24GB)

調査レポートB6の結論に基づき、Apple Silicon Mac miniを推奨プラットフォームとする。

| 項目 | 仕様 |
|------|------|
| チップ | Apple M4 Pro |
| メモリ | 24GB ユニファイドメモリ |
| メモリ帯域幅 | 273 GB/s |
| ストレージ | 512GB SSD |
| 消費電力 | 10-80W（通常動作 25-40W） |
| サイズ | 12.7 x 12.7 x 5cm |
| ファン騒音 | 低負荷時ほぼ無音（~20-25dB） |
| 参考価格 | ¥218,800（税込） |

**選定理由**（レポートB6）:
- MLXフレームワーク（Apple公式）が本番グレードに成熟
- ユニファイドメモリによるCPU-GPU間ゼロコピー
- 全パイプライン同時実行で~9-11GB使用（24GBで十分な余裕）
- ピークGPU使用率~50%
- ベッドサイド設置に最適な静音・小型設計

### 2.2 全パイプライン同時実行メモリ配分

| タスク | 使用メモリ | GPU使用率 | 性能 |
|--------|-----------|----------|------|
| 視線推定（MediaPipe + CNN） | ~0.5GB | ~10%（常時） | 30fps |
| ローカルLLM（Qwen3-1.7B Q4） | ~1.2GB | ~15%（バースト） | ~320 tok/s |
| ローカルLLM（Qwen3-0.6B Q5） | ~0.5GB | ~5%（バースト） | ~525 tok/s |
| TTS（CosyVoice 2/3） | ~0.5-1GB | ~10%（バースト） | ストリーミング150ms |
| VLM（MiniCPM-V 4.0 Q4） | ~2.5GB | ~10-15%（断続） | ~45 tok/s |
| OS + アプリケーション | ~4-6GB | — | — |
| **合計** | **~9-11GB** | **ピーク~50%** | — |

### 2.3 コスト帯別構成

| 構成 | デバイス | 価格帯 | 性能評価 |
|------|---------|--------|---------|
| エントリー | Mac mini M4 (24GB) | ~¥155K | LLM ~200 tok/s、VLMは軽量モデル推奨 |
| **推奨** | **Mac mini M4 Pro (24GB)** | **~¥277K** | **LLM ~320 tok/s、全パイプライン余裕** |
| プレミアム | Mac mini M4 Max (48GB) | ~¥400K+ | 将来拡張性最大、4B以上のLLMも可能 |

### 2.4 推論フレームワーク

| 優先度 | フレームワーク | 用途 | 理由 |
|--------|--------------|------|------|
| 1 | **vllm-mlx** | LLM / VLM推論 | OpenAI互換API、KVキャッシュ、連続バッチング、MLX比21-87%高速 |
| 2 | MLX (mlx-lm) | LoRAファインチューニング | Apple公式、安定性が高い、ドキュメント充実 |
| 3 | llama.cpp (Metal) | クロスプラットフォーム対応 | Windows/Linux版への移植時に使用 |

---

## 3. Layer 1: センシング層

### 3.1 カメラサブシステム

#### 内向きカメラ（患者の目・顔）

```
Webカメラ映像（30fps、720p以上）
  ↓
MediaPipe Face Mesh（468点ランドマーク、~5ms）
  ├→ 目領域クロップ → MobileOne s0 + L2CS-Net dual loss → 画面座標（~3ms）
  ├→ まばたき検出 → EAR（Eye Aspect Ratio）→ パターン分析
  ├→ 瞳孔径変動 → 感情チャネル1
  ├→ 顔色変動 → open-rppg → rPPG心拍推定 → 感情チャネル2
  └→ 視線パターン時系列 → 感情チャネル3
```

**視線推定パイプライン**（レポートA1, B4）:

| 段階 | 技術 | 精度 | レイテンシ |
|------|------|------|-----------|
| Tier 1（MVP） | MediaPipe + MobileOne s0 + L2CS-Net | 4°（未校正）→ 2.5°（5点校正） | <8ms |
| Tier 2（6ヶ月） | + FAZE meta-learning 3点校正 | 2-3° | <10ms |
| Tier 3（12ヶ月） | + OpenFace 3.0 / foundation model | sub-2° | <15ms |

要件:
- 解像度: 720p以上（瞳孔検出には1080p推奨）
- フレームレート: 30fps以上
- 設置距離: 患者の顔から40〜80cm
- 照明: 200 lux以上推奨（IR対応カメラは夜間も動作可能）

#### 外向きカメラ（周囲環境）

```
外向きカメラ映像（10fps、720p）
  ↓
3層ハイブリッドVLM処理（レポートB5）
  ├→ Layer 1: OpenCV動体検出（常時、CPU ~5%）
  │    └→ 変化なし → 省電力待機
  │    └→ 変化あり → Layer 2をトリガー
  ├→ Layer 2: ローカルVLM（MiniCPM-V 4.0, 4.1B）
  │    ├→ 定期スナップショット（5-10秒間隔）
  │    ├→ 人物認識 / 物体認識 / シーン記述
  │    └→ テキスト → コンテキスト統合層へ
  └→ Layer 3: クラウドVLM（Gemini 2.5 Flash、患者同意時のみ）
       └→ ローカルVLMで判断困難な場面のフォールバック
```

要件:
- 広角レンズ推奨（90°以上の画角）
- フレームレート: 10fps（省電力のため間欠処理可）
- 設置: ベッドサイドまたはモニター上部
- **プライバシー**: カメラ映像は原則ローカル処理。クラウド送信時は匿名化 + 明示的同意

### 3.2 マイクサブシステム

```
マイク入力
  ├→ 音声区間検出（VAD）
  │    ├→ 発話区間 → 音声認識（ASR）→ テキスト
  │    └→ 非発話区間 → 環境音分類
  ├→ 話者認識 → 誰が話しているか
  └→ 音響特徴量 → 声のトーン・感情推定（補助）
```

### 3.3 指入力サブシステム（Input Abstraction Layer）

VoiceReachはInput Abstraction Layer（IAL）を導入し、入力デバイスの切り替えを透過的に行う（レポートA6, B7）。

```
                    ┌───────────────────────────┐
                    │  Input Abstraction Layer   │
                    │  統一イベントインターフェース │
                    └────────────┬──────────────┘
          ┌──────────┬──────────┼──────────┬──────────┐
          ▼          ▼          ▼          ▼          ▼
    指タップ      まばたき      EMG       BCI        音声
   (Stage 1-3)  (Stage 3-4)  (Stage 2-4) (Stage 4)  (残存発声)
```

#### デュアルセンサ指入力（レポートA6）

| コンポーネント | 製品 | 役割 | 価格 |
|--------------|------|------|------|
| MCU | Seeed XIAO RP2040 | USB HID + ADC + I2C | $6 |
| 1次センサ | 27mm PZT圧電ディスク | タップ検出（動的圧力） | $1 |
| 2次センサ | Interlink FSR-402 | ロングプレス検出（静的圧力） | $8 |
| 精密ADC | ADS1115（Stage 3用） | 16bit高分解能 | $15 |
| 合計BOM | | | **~$41** |

```
[PZT圧電ディスク] → ADC Ch1 → タップ検出（シングル/ダブル/トリプル）
[FSR-402]         → ADC Ch2 → ロングプレス検出（持続圧力）
        ↓
    4層フィルタリング
      Layer 1: ハードウェアRCフィルタ（160Hz LPF）
      Layer 2: 時間的デバウンス（30-200ms、ステージ別）
      Layer 3: 波形テンプレートマッチング（不随意運動除去）
      Layer 4: コンテキスト認識ゲーティング（視線位置連携）
        ↓
    USB HID → VoiceReachアプリ
```

ジェスチャーマッピング:
- シングルタップ → 選択確定
- ダブルタップ → 戻る/キャンセル
- ロングプレス（1秒以上） → メニュー/展開
- トリプルタップ → 緊急コール

### 3.4 IoT/外部データサブシステム

```
IoT連携（MQTT / REST API）
  ├→ 環境センサー: 室温、湿度、照度
  ├→ ベッドセンサー: 角度、体圧分布
  ├→ スマートホーム: エアコン状態、照明、TV入力
  └→ 医療機器: SpO2、心拍モニタ、呼吸器アラーム

カレンダー/スケジュール API
  └→ 介護予定、投薬時間、通院、面会
```

---

## 4. Layer 2: コンテキスト統合層

### 4.1 信号統合エンジン

すべてのセンサーデータをテキスト化された「コンテキストフレーム」に変換する。

```json
{
  "timestamp": "2026-02-17T14:30:00+09:00",
  "patient_state": {
    "gaze_zone": "center-right",
    "gaze_target_object": "television",
    "fatigue_level": 0.3,
    "emotion": {
      "valence": 0.1,
      "arousal": 0.4,
      "confidence": 0.6,
      "frustration": 0.0,
      "channels": {
        "pupil_dilation": { "delta": 0.12, "confidence": 0.7 },
        "rppg_heart_rate": { "bpm": 78, "baseline_delta": 5, "confidence": 0.8 },
        "gaze_pattern": { "fixation_duration_ms": 450, "saccade_rate": 2.1 },
        "blink_pattern": { "rate": 18, "baseline": 15, "voluntary_ratio": 0.1 }
      }
    }
  },
  "environment": {
    "people_present": [
      {"name": "花子", "relation": "wife", "position": "bedside-left"}
    ],
    "objects_detected": ["food_tray", "television_on", "remote_control"],
    "scene_description": "妻が食事トレイを持ってベッドサイドに来ている",
    "vlm_source": "local_minicpm_v4",
    "ambient": {
      "temperature": 23.5,
      "humidity": 45,
      "lighting": "bright"
    }
  },
  "audio": {
    "last_speech": {
      "speaker": "花子",
      "text": "お昼ごはん、おかゆとうどんどっちがいい？",
      "timestamp": "2026-02-17T14:29:55+09:00"
    },
    "background": "television_sports"
  },
  "schedule": {
    "current": "lunch_time",
    "next": {"event": "medication", "time": "15:00"}
  },
  "conversation_history": [
    {"speaker": "patient", "text": "おはよう", "time": "07:15"},
    {"speaker": "花子", "text": "おはよう、よく眠れた？", "time": "07:15"}
  ]
}
```

### 4.2 イベント検出エンジン

コンテキストフレームの変化を検出し、候補再生成をトリガーする。

```
トリガーイベント:
  - 新しい人物の入室/退室（VLM検出）
  - 患者への発話検出（ASR + 話者方向推定）
  - 患者がUIに視線を向けた（操作意図の推定）
  - 一定時間の沈黙（30秒以上）
  - 環境の大きな変化（食事の到着、医療処置の開始等）
  - 緊急アラート（バイタル異常、呼吸器アラーム）
  - スケジュールイベント（投薬時間到来等）

非トリガー:
  - 環境の微小な変化（室温の0.1度変動等）
  - 視線がUI以外を見ている（休憩中の可能性）
  - 介護者同士の会話（患者に向けた発話でない場合）
```

---

## 5. Layer 3: インテリジェンス層

### 5.1 LLM推論パイプライン

```
[System Prompt]
  PVP（Personal Voice Profile）: ~2000トークン

[Context]
  コンテキストフレーム（最新）: ~500トークン
  直近の会話履歴（最大10ターン）: ~500トークン

[Task Instruction]
  候補生成ルール + 意図軸分散指示: ~300トークン

[Previous Round Feedback]（2巡目以降）
  前回候補 + 視線滞留データ + 感情データ: ~200トークン

合計: ~3500トークン入力 → 候補4つ出力（~80-150トークン）
```

### 5.2 三段ハイブリッド推論（レポートB1, B2）

```
トリガーイベント発生
  │
  ├→ [Stage 1: ~150ms] Qwen3-0.6B (Q5_K_M) + vllm-mlx
  │   └→ 暫定候補4つ → UI即表示
  │   └→ KVキャッシュ: PVP+指示をプレフィックスキャッシュ（TTFT実質0ms）
  │
  ├→ [Stage 2: ~350ms] Qwen3-1.7B (Q4_K_M) + vllm-mlx
  │   └→ 改善候補4つ → UIを差し替え（アニメーション付き）
  │   └→ 日本語品質B+、意図軸分散が概ね安定
  │
  └→ [Stage 3: ~800ms] Gemini 2.5 Flash（クラウド）
      └→ 高品質候補4つ → UIを差し替え
      └→ 日本語品質A-、コスト月額~$2-3
      └→ フォールバック: Claude Haiku 4.5（品質A、~$5/月）→ GPT-4.1 nano
```

ユーザーが暫定候補をすぐに選んだ場合は後続Stage結果を破棄。選ばなかった場合は到着順にUIを更新。

### 5.3 量子化戦略（レポートB1）

| モデルサイズ | 推奨量子化 | メモリ | 理由 |
|------------|----------|--------|------|
| Qwen3-0.6B | Q5_K_M | ~0.5GB | 小型のため量子化耐性が低い。品質維持優先 |
| Qwen3-1.7B | Q4_K_M | ~1.2GB | バランス良好。日本語品質と速度の最適点 |
| Qwen3-4B（将来） | Q4_K_M | ~2.5GB | M4 Max等での品質向上オプション |

### 5.4 プロンプトキャッシュ設計

```yaml
# Anthropic / Google 共通のキャッシュ戦略
layer_1_static:   # ほぼ変化しない（日/週単位）
  content: "PVP + 基本タスク指示"
  size: ~2300 tokens
  cache: always    # ローカル: KVキャッシュ / クラウド: プロンプトキャッシュ

layer_2_session:   # セッション中変化しにくい（時間単位）
  content: "環境状態 + 対話相手情報"
  size: ~300 tokens
  cache: session   # セッション中キャッシュ

layer_3_dynamic:   # 毎回変化する
  content: "会話履歴 + フィードバック + 最新コンテキスト"
  size: ~900 tokens
  cache: none      # キャッシュしない

# クラウドキャッシュ割引
# Anthropic: 読み込み90%OFF (5分TTL、自動延長)
# Google: 読み込み90%OFF (Context Caching API)
# → PVP部分のコストを実質1/10に削減
```

### 5.5 先読みプリフェッチ

```
現在の候補4つを表示中に、裏側で並行して:
  - 各候補が選ばれなかった場合の2巡目候補をプリフェッチ
  - 「自由入力モード」に遷移した場合の初期画面をプリロード
  - 次に来そうな会話ターンの候補を先行生成
```

---

## 6. Layer 4: プレゼンテーション層

### 6.1 患者UI

Electron / Webベースのフルスクリーンアプリケーション。

```
画面構成:
  ┌──────────────────────────────────────┐
  │ [ステータスバー]                      │
  │  接続状態 | 時刻 | バッテリー | モード │
  ├──────────────────────────────────────┤
  │                                      │
  │          メインコンテンツ領域          │
  │    （候補表示 / 文字入力 / 設定）      │
  │                                      │
  ├──────────────────────────────────────┤
  │ [会話ログ] 直近の発話履歴             │
  └──────────────────────────────────────┘
```

### 6.2 介護者UI

スマートフォン / タブレット / スマートウォッチ向けWeb PWA。

### 6.3 管理ダッシュボード

PVP管理、ベースラインキャリブレーション、進行度追跡、ログ閲覧等。

---

## 7. データフロー図

```
                    ┌───────────────────┐
                    │     クラウド       │
                    │  Gemini 2.5 Flash │
                    │  Claude Haiku 4.5 │
                    └────────┬──────────┘
                             │ HTTPS（PVP + コンテキスト、匿名化済み）
                             │
┌────────────────────────────┴──────────────────────────────┐
│              患者端末（Mac mini M4 Pro）                    │
│                                                           │
│  ┌──────────┐  ┌──────────┐  ┌───────────────────────┐   │
│  │ カメラ入力 │  │ マイク入力│  │ 指入力デバイス         │   │
│  │ (内/外)   │  │          │  │ (PZT+FSR/XIAO RP2040)│   │
│  └─────┬────┘  └─────┬────┘  └───────────┬───────────┘   │
│        │             │                   │               │
│  ┌─────┴─────────────┴───────────────────┴────────────┐  │
│  │         センシングエンジン（ローカル、MLX）          │  │
│  │  MediaPipe + MobileOne s0 | open-rppg | VAD/ASR    │  │
│  └────────────────────┬──────────────────────────────┘  │
│                       │                                  │
│  ┌────────────────────┴──────────────────────────────┐  │
│  │         コンテキスト統合エンジン                    │  │
│  │  MiniCPM-V 4.0 (VLM) | 4ch感情 | イベント検出     │  │
│  └────────────────────┬──────────────────────────────┘  │
│                       │                                  │
│  ┌────────────────────┴──────────────────────────────┐  │
│  │         三段ハイブリッドLLM推論                     │  │
│  │  Qwen3-0.6B → Qwen3-1.7B → (クラウドフォールバック)│  │
│  └────────────────────┬──────────────────────────────┘  │
│                       │                                  │
│  ┌────────────────────┴──────────────────────────────┐  │
│  │         音声合成エンジン                            │  │
│  │  CosyVoice 2/3（ストリーミング150ms）              │  │
│  └────────────────────┬──────────────────────────────┘  │
│                       │                                  │
│  ┌────────────────────┴──────────────────────────────┐  │
│  │         患者UI（Electron/Web）                     │  │
│  └───────────────────────────────────────────────────┘  │
│                                                         │
│         MQTT / WebSocket                                │
│              │                                          │
└──────────────┼──────────────────────────────────────────┘
               │
  ┌────────────┴────────────┐
  │   介護者端末群            │
  │  スマホ / ウォッチ / PC   │
  └─────────────────────────┘
```

---

## 8. 技術選定の根拠

### 視線推定: MediaPipe + MobileOne s0 + L2CS-Net（レポートA1, B4）

- MediaPipe Face Meshは468点のランドマークを~5msで検出。クロスプラットフォーム対応
- MobileOne s0はバックボーンCNNとして高速（~3ms）かつ軽量
- L2CS-Netのdual loss（ピッチ・ヨー分離学習）により精度向上
- ゾーンベース設計のため、2.5°（校正後）で十分実用的
- FAZE meta-learningによる3点校正で将来的にsub-2°を目指す

### LLM: 三段ハイブリッド構成（レポートB1, B2）

- **ローカル（Qwen3シリーズ）**: Apache 2.0、日本語100言語対応、MLX最適化
- **クラウド（Gemini 2.5 Flash）**: TTFT 0.28s最速、月額$2-3の低コスト
- **フォールバック（Claude Haiku 4.5）**: 日本語品質A、指示追従A+
- ファインチューニングではなくIn-Context Learning採用。モデル更新不要

### 音声合成: CosyVoice 2/3 + GPT-SoVITS v4（レポートA4, B3）

- CosyVoice 2/3: ストリーミング150ms、Apache-2.0、感情制御ネイティブ対応
- GPT-SoVITS v4: MIT、few-shot品質が高い、微調整用
- EmoKnob: モデル非依存の感情制御フレームワーク

### 指入力: PZT + FSR デュアルセンサ（レポートA6）

- 微小な圧力変化を検出可能（ALS進行期にも対応）
- 4層フィルタリングで不随意運動（ファシキュレーション）を除去
- コンテキスト認識ゲーティングはVoiceReach固有の優位性
- BOM $41で福祉補助金（圧電素子式 ¥53,400）の範囲内

### VLM: ハイブリッド3層構成（レポートB5）

- プライバシー保護のため映像データは原則ローカル処理
- MiniCPM-V 4.0（4.1B）: GPT-4.1-mini超え、メモリ2.5GB
- Gemini 2.5 Flash: イベント駆動フォールバック、月額数百円
- 「今見えているもの」を話題にした候補生成はAAC革新的拡張

### エッジデバイス: Apple Silicon Mac mini（レポートB6）

- MLXフレームワーク（Apple公式）が本番グレードに成熟
- WWDC25で"Explore LLMs on Apple silicon with MLX"セッション開催
- vllm-mlxで400+ tok/s達成、OpenAI互換API
- M5チップ以降でTTFT 3x改善見込み。ソフトウェア互換性維持

---

## 9. 非機能要件

| 項目 | 要件 | 根拠 |
|------|------|------|
| レイテンシ（候補表示） | Stage 1: 150ms / Stage 2: 350ms / Stage 3: 800ms | B1, B2レポートの実測値ベース |
| 可用性 | オフラインで70%以上の機能動作 | Qwen3-1.7Bによるローカル完結 |
| セキュリティ | 生体データはローカル保持。クラウドにはPVP+コンテキストのみ | VLM映像は一切クラウド不送信（同意時除く） |
| 消費電力 | 通常動作時25-40W | Mac mini M4 Pro実測値 |
| 設置環境 | 室内照明（200 lux以上）、ファン騒音30dB以下 | ALS患者ベッドサイド要件 |
| 対応OS | macOS 15以降（推奨）/ Linux / Windows（llama.cpp経由） | MLX最優先、クロスプラットフォームはフォールバック |
| 月額運用コスト | クラウドAPI: $5-8 | Gemini主体 + Claude時々、キャッシュ利用 |
