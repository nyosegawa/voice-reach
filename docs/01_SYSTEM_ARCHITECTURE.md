# 01 — システムアーキテクチャ詳細設計

## 1. アーキテクチャ概要

VoiceReachは4つの主要レイヤーで構成される。

```
┌─────────────────────────────────────────────────────────┐
│ Layer 4: プレゼンテーション層                             │
│   患者UI / 介護者UI / 管理ダッシュボード                  │
├─────────────────────────────────────────────────────────┤
│ Layer 3: インテリジェンス層                               │
│   LLM候補生成 / 意図空間ナビゲーション / PVPエンジン       │
├─────────────────────────────────────────────────────────┤
│ Layer 2: コンテキスト統合層                               │
│   環境認識 / 感情推定 / イベント検出 / 信号統合           │
├─────────────────────────────────────────────────────────┤
│ Layer 1: センシング層                                    │
│   視線推定 / 表情追跡 / 指入力 / マイク / IoT / カメラ   │
└─────────────────────────────────────────────────────────┘
```

---

## 2. Layer 1: センシング層

### 2.1 カメラサブシステム

#### 内向きカメラ（患者の目・顔）

```
Webカメラ映像（30fps、720p以上）
  ↓
MediaPipe Face Mesh（468点ランドマーク）
  ├→ 目領域のクロップ → 視線推定CNN → 画面座標
  ├→ 顔面ランドマーク時系列 → 表情微変化検出
  ├→ まばたき検出 → タイミング・頻度解析
  ├→ 瞼開閉度 → 疲労検出
  └→ 顔色変動解析 → rPPG心拍推定
```

要件：
- 解像度: 720p以上（瞳孔検出には1080p推奨）
- フレームレート: 30fps以上
- 設置距離: 患者の顔から40〜80cm
- 照明: 可視光環境（200 lux以上推奨）

#### 外向きカメラ（周囲環境）

```
外向きカメラ映像（10fps、720p）
  ↓
VLM（Vision-Language Model）
  ├→ 人物認識（顔認識 + 服装/体格推定）
  ├→ 物体認識（食事トレイ、リモコン、医療機器等）
  ├→ イベント検出（入室、退室、物の移動等）
  └→ シーン記述テキスト生成
```

要件：
- 広角レンズ推奨（90°以上の画角）
- フレームレート: 10fps（省電力のため間欠処理可）
- 設置: ベッドサイドまたはモニター上部

### 2.2 マイクサブシステム

```
マイク入力
  ├→ 音声区間検出（VAD）
  │    ├→ 発話区間 → 音声認識（ASR）→ テキスト
  │    └→ 非発話区間 → 環境音分類
  ├→ 話者認識 → 誰が話しているか
  └→ 音響特徴量 → 声のトーン・感情推定（補助）
```

### 2.3 指入力サブシステム

```
ピエゾ素子 → ADC → USB HID デバイス → PC
  ↓
入力イベント生成
  ├→ シングルタップ（短い圧力変化）→ 選択確定
  ├→ ダブルタップ（0.5秒以内に2回）→ 戻る/キャンセル
  └→ ロングプレス（1秒以上の持続圧力）→ メニュー/展開
```

設計要件：
- 閾値: ソフトウェア側で動的調整可能（0〜100の感度スケール）
- 不随意運動フィルタ: 筋痙攣やファシキュレーションによる誤入力を除外
- デバウンス: 50〜200ms（調整可能）
- 装着位置: 指先、手の甲、前腕など残存筋力に応じて変更可能

### 2.4 IoT/外部データサブシステム

```
IoT連携（MQTT / REST API）
  ├→ 環境センサー: 室温、湿度、照度
  ├→ ベッドセンサー: 角度、体圧分布
  ├→ スマートホーム: エアコン状態、照明、TV入力
  └→ 医療機器: SpO2、心拍モニタ、呼吸器アラーム

カレンダー/スケジュール API
  └→ 介護予定、投薬時間、通院、面会
```

---

## 3. Layer 2: コンテキスト統合層

### 3.1 信号統合エンジン

すべてのセンサーデータをテキスト化された「コンテキストフレーム」に変換する。

```json
{
  "timestamp": "2026-02-17T14:30:00+09:00",
  "patient_state": {
    "gaze_zone": "center-right",
    "gaze_target_object": "television",
    "fatigue_level": 0.3,
    "emotion": {
      "valence": 0.1,
      "arousal": 0.4,
      "confidence": 0.6,
      "frustration": 0.0
    },
    "blink_rate": 18,
    "baseline_blink_rate": 15
  },
  "environment": {
    "people_present": [
      {"name": "花子", "relation": "wife", "position": "bedside-left"}
    ],
    "objects_detected": ["food_tray", "television_on", "remote_control"],
    "scene_description": "妻が食事トレイを持ってベッドサイドに来ている",
    "ambient": {
      "temperature": 23.5,
      "humidity": 45,
      "lighting": "bright"
    }
  },
  "audio": {
    "last_speech": {
      "speaker": "花子",
      "text": "お昼ごはん、おかゆとうどんどっちがいい？",
      "timestamp": "2026-02-17T14:29:55+09:00"
    },
    "background": "television_sports"
  },
  "schedule": {
    "current": "lunch_time",
    "next": {"event": "medication", "time": "15:00"}
  },
  "conversation_history": [
    {"speaker": "patient", "text": "おはよう", "time": "07:15"},
    {"speaker": "花子", "text": "おはよう、よく眠れた？", "time": "07:15"}
  ]
}
```

### 3.2 イベント検出エンジン

コンテキストフレームの変化を検出し、候補再生成をトリガーする。

```
トリガーイベント:
  - 新しい人物の入室/退室
  - 患者への発話検出
  - 患者がUIに視線を向けた（操作意図の推定）
  - 一定時間の沈黙（30秒以上）
  - 環境の大きな変化（食事の到着、医療処置の開始等）
  - 緊急アラート（バイタル異常、呼吸器アラーム）
  - スケジュールイベント（投薬時間到来等）

非トリガー:
  - 環境の微小な変化（室温の0.1度変動等）
  - 視線がUI以外を見ている（休憩中の可能性）
  - 介護者同士の会話（患者に向けた発話でない場合）
```

---

## 4. Layer 3: インテリジェンス層

### 4.1 LLM推論パイプライン

```
[System Prompt]
  PVP（Personal Voice Profile）: ~2000トークン

[Context]
  コンテキストフレーム（最新）: ~500トークン
  直近の会話履歴（最大10ターン）: ~500トークン

[Task Instruction]
  候補生成ルール + 意図軸分散指示: ~300トークン

[Previous Round Feedback]（2巡目以降）
  前回候補 + 視線滞留データ + 感情データ: ~200トークン

合計: ~3500トークン入力 → 候補4つ出力
```

### 4.2 二段構成推論（レイテンシ最適化）

```
トリガーイベント発生
  ↓ 同時実行
  ├→ [即座] ローカルLLM（Phi-3等）で暫定候補4つ生成（~200ms）
  │         → UIに即表示
  └→ [裏側] クラウドLLM（Claude等）で高品質候補4つ生成（~1-2秒）
            → 到着次第、UIを差し替え（アニメーション付き）
```

ユーザーが暫定候補をすぐに選んだ場合はクラウド結果を破棄。選ばなかった場合はクラウド結果で更新。これによりレイテンシを200msに抑えつつ、品質を確保する。

### 4.3 先読みプリフェッチ

```
現在の候補4つを表示中に、裏側で並行して:
  - 各候補が選ばれなかった場合の2巡目候補をプリフェッチ
  - 「自由入力モード」に遷移した場合の初期画面をプリロード
  - 次に来そうな会話ターンの候補を先行生成
```

---

## 5. Layer 4: プレゼンテーション層

### 5.1 患者UI

Electron / Webベースのフルスクリーンアプリケーション。

```
画面構成:
  ┌──────────────────────────────────────┐
  │ [ステータスバー]                      │
  │  接続状態 | 時刻 | バッテリー | モード │
  ├──────────────────────────────────────┤
  │                                      │
  │          メインコンテンツ領域          │
  │    （候補表示 / 文字入力 / 設定）      │
  │                                      │
  ├──────────────────────────────────────┤
  │ [会話ログ] 直近の発話履歴             │
  └──────────────────────────────────────┘
```

### 5.2 介護者UI

スマートフォン / タブレット / スマートウォッチ向けWeb PWA。

### 5.3 管理ダッシュボード

PVP管理、ベースラインキャリブレーション、進行度追跡、ログ閲覧等。

---

## 6. データフロー図

```
                    ┌──────────┐
                    │  クラウド  │
                    │  LLM API │
                    └────┬─────┘
                         │ HTTPS（PVP + コンテキスト）
                         │
┌────────────────────────┴────────────────────────────┐
│                   患者端末（PC/タブレット）            │
│                                                     │
│  ┌──────────┐  ┌──────────┐  ┌──────────────────┐  │
│  │ カメラ入力 │  │ マイク入力│  │  指入力デバイス   │  │
│  │ (内/外)   │  │          │  │  (ピエゾ/USB)    │  │
│  └─────┬────┘  └─────┬────┘  └────────┬─────────┘  │
│        │             │               │             │
│  ┌─────┴─────────────┴───────────────┴──────────┐  │
│  │           センシングエンジン（ローカル）        │  │
│  │  視線推定 | 表情追跡 | ASR | 環境認識          │  │
│  └──────────────────┬────────────────────────────┘  │
│                     │                               │
│  ┌──────────────────┴────────────────────────────┐  │
│  │           コンテキスト統合エンジン              │  │
│  └──────────────────┬────────────────────────────┘  │
│                     │                               │
│  ┌──────────────────┴────────────────────────────┐  │
│  │           ローカルLLM（オフライン用）           │  │
│  └──────────────────┬────────────────────────────┘  │
│                     │                               │
│  ┌──────────────────┴────────────────────────────┐  │
│  │           患者UI（Electron/Web）               │  │
│  └───────────────────────────────────────────────┘  │
│                                                     │
│         MQTT / WebSocket                            │
│              │                                      │
└──────────────┼──────────────────────────────────────┘
               │
  ┌────────────┴────────────┐
  │   介護者端末群            │
  │  スマホ / ウォッチ / PC   │
  └─────────────────────────┘
```

---

## 7. 技術選定の根拠

### 視線推定: MediaPipe + 軽量CNN

- MediaPipe Face Meshは468点のランドマークをリアルタイムで検出可能
- ブラウザ上（WebAssembly）でも動作し、クロスプラットフォーム対応
- ゾーンベース設計のため、ピクセル精度は不要。精度2〜5°で十分実用的

### LLM: クラウド + オンデバイスの二段構成

- クラウドLLM: 高品質な候補生成、PVPの活用に十分なコンテキスト長
- オンデバイスLLM: オフライン動作保証、レイテンシ削減
- ファインチューニングではなくIn-Context Learningを採用し、モデル更新不要

### 指入力: ピエゾ素子

- 微小な圧力変化を検出可能（ALS進行期にも対応）
- 閾値のソフトウェア制御で進行度に適応
- 安価で信頼性が高い
- 福祉現場での使用実績あり

---

## 8. 非機能要件

| 項目 | 要件 |
|---|---|
| レイテンシ（候補表示まで） | ローカル: 200ms以下、クラウド: 2秒以下 |
| 可用性 | オフラインで基本機能動作。緊急コールは常時利用可 |
| セキュリティ | 生体データはローカル保持。クラウドにはPVP + コンテキストのみ送信 |
| 消費電力 | ノートPC想定で8時間以上のバッテリー駆動 |
| 対応環境 | 室内照明（200 lux以上）で安定動作。夜間モードは別途設計 |
| 対応OS | Windows 10以降 / macOS 12以降 / Linux（Ubuntu 22.04以降） |
