# 08 — 音声保存プロトコルと音声合成

## 1. なぜ音声保存がタイムクリティカルか

ALS患者の構音障害は不可逆的に進行する。VoiceReachのPersonal Voice Profile（PVP）で「何を言うか」は後から構築できるが、「どんな声で言うか」は発話能力が残っているうちに録音しなければ取り返しがつかない。

```
          発症              構音障害の進行         発話不能
 ──────────┼─────────────────────┼─────────────────┼──
           │                     │                 │
           │◀── 録音可能期間 ──▶│                 │
           │                     │                 │
           │  ここを逃すと       │  もう録れない    │
           │  声は失われる       │                 │
```

2024-2026年の音声合成技術の急速な進歩により、必要な録音時間は大幅に短縮された（従来の数時間から60-90分へ）。しかし、録音の機会そのものは依然として限られている。このため、**音声保存はプロダクトの完成を待たずにPhase 0として先行実施し、診断直後から開始すべきである**。

---

## 2. 録音プロトコル

### 2.1 録音要件サマリ

2026年時点のTTSモデルは3-10秒のゼロショット参照音声から高品質なクローニングが可能になっているが（レポートA4 Q1）、微調整（ファインチューニング）を行うことでMOS 4.0-4.3の品質を達成できる（レポートB3 Q4）。録音プロトコルはモデル非依存を原則としつつ、微調整に最適化した設計とする。

| 録音レベル | 録音時間 | 内容 | 期待品質（MOS） |
|-----------|---------|------|----------------|
| 最小実行可能 | 20分 | 50文（音素カバレッジ）+ 10分自由会話 | 3.5-3.8 |
| **推奨** | **60分** | **100文 + 感情セッション + 自由会話** | **4.0-4.3** |
| 最適 | 60-90分 | 推奨 + 追加感情バリエーション + 長時間自然会話 | 4.1-4.4 |

> **根拠**: レポートB3 Section 5.1より、30-60分の高品質データ + 微調整でMOS 4.0-4.3を達成。60分を超えると収穫逓減（diminishing returns）が顕著になる。データ品質がデータ量を上回る重要因子である（レポートA4 Q5）。

### 2.2 推奨録音セッション構成（60分）

```
セッション1: 音素カバレッジ（20分）
  - 日本語全モーラを網羅する100文の朗読
    - 清音・濁音・半濁音・拗音・促音（っ）・撥音（ん）
    - 全ピッチアクセント型: 頭高・中高・尾高・平板
    - 文末助詞: よ、ね、か、な、ぞ、わ
    - 文長バリエーション: 短文（5モーラ）～長文（40モーラ以上）
    - 文型: 平叙文、疑問文、感嘆文
  - 自然な会話文として設計（不自然な音素練習文ではなく）
  - 設計参考: JVSコーパス文セット設計（レポートA4, Paper #25-26）

セッション2: 感情トーンバリエーション（20分）
  - ニュートラル/穏やか（基本）: 10文朗読 + 3分モノローグ
  - 嬉しい/温かい: 10文朗読 + 3分楽しい思い出の語り
  - 真剣/強調: 10文朗読 + 3分重要なテーマについての語り
  - 優しい/穏やか: 10文朗読 + 3分家族への語りかけ
  - （任意）楽しい/ユーモラス: 5文 + 2分冗談や面白い話

セッション3: 個人特有の表現（10分）
  - 口癖や頻用フレーズを繰り返し発話
  - 笑い声（鼻で笑う、声を出して笑う等）
  - 相槌パターン（「うん」「そうだね」「へぇ」等）
  - 感嘆表現（「すごい」「まじか」「やばい」等）

セッション4: 自由会話（10分以上、可能な限り長く）
  - 家族との自然な会話を録音
  - 最も「その人らしい」声が録れる
  - トピック提案を用意するが強制しない
```

> **根拠**: レポートB3 Section 5.1の録音プロトコル推奨に基づく。感情セッションはEmoKnobフレームワーク（EMNLP 2024, レポートA4 Paper #15）での感情方向ベクトル抽出に必要。

### 2.3 録音環境要件

| パラメータ | 最小要件 | 推奨 | 最適 |
|-----------|---------|------|------|
| サンプリングレート | 22 kHz | 44.1 kHz | **48 kHz** |
| ビット深度 | 16-bit PCM | 16-bit PCM | **24-bit PCM** |
| SNR（信号対雑音比） | >30 dB | >35 dB | >40 dB |
| 背景雑音フロア | <-50 dB | <-60 dB | <-70 dB |
| 音量（RMS） | -23 to -18 dB | -20 to -16 dB | 一定を維持 |
| ピークレベル | <-3 dB | <-6 dB | <-6 dB |
| フォーマット | WAV（無圧縮） | WAV | WAV |
| マイク | スマートフォンマイク | USBコンデンサーマイク | スタジオコンデンサーマイク |
| マイク距離 | 20-50 cm | 20-30 cm | 20-30 cm + ポップフィルター |

> **根拠**: レポートA4 Q5の録音品質要件テーブルに基づく。**推奨録音フォーマットは48kHz / 24-bit WAV**とする（レポートB3 Section 5.1）。生録音データは絶対に圧縮・加工しない。

```
録音環境のセットアップ:
  必須:
    - 静かな部屋（エアコン音、テレビ音等を消す）
    - マイク: USBコンデンサーマイク推奨
      （例: Blue Yeti, Audio-Technica AT2020USB+）
    - なければスマートフォンでも可（品質は低下するが録音しないよりはるかに良い）

  推奨:
    - ポップフィルター（破裂音のノイズ防止）
    - 反響の少ない部屋（布が多い部屋が良い）
    - 複数セッションに分割（疲労防止）
    - 録音アプリによるリアルタイム品質チェック
```

### 2.4 録音アプリの品質チェック機能

```
リアルタイムモニタリング:
  - SNRモニタリング（<30 dBで警告）
  - ピークレベルモニタリング（クリッピング検出で警告）
  - 背景ノイズ検出
  - 録音完了度トラッキング（音素カバレッジ表示）
  - 文ごとの品質スコア表示

キャプチャすべきメタデータ:
  - 文ごとのタイムスタンプ
  - 自己申告の感情/気分
  - 録音環境の説明
  - マイクの種類と距離
  - 患者の発話状態評価（1-5スケール）
```

> **根拠**: レポートB3 Section 5.1の品質チェック仕様に基づく。

### 2.5 既存音声データの活用

新規録音が十分にできない場合や、すでに構音障害が進行している場合は、既存の音声データを活用する。

```
活用可能なソース:
  - 留守番電話メッセージ
  - ビデオ通話の録画（Zoom, LINE等）
  - ホームビデオ
  - 結婚式のスピーチ
  - 仕事のプレゼン録画
  - SNSの音声メッセージ
  - 会議録音

前処理パイプライン:
  1. 音源収集 → 話者分離で患者の音声のみ抽出
  2. ノイズ除去（RNNoise / DTLN）
  3. 音質正規化（ラウドネス、サンプリングレート統一）
  4. VAD + セグメント化（発話単位に分割）
  5. 強制アライメント + テキスト対応付け（Whisper large-v3で文字起こし → 手動修正）
  6. 品質スコアリング（S/N比、明瞭度で品質を数値化）
  7. 高品質セグメントのみを訓練データに使用
```

> **根拠**: レポートA4 Section 5.2の推奨アーキテクチャ（Phase 0 前処理パイプライン）に基づく。

### 2.6 構音障害が進行している場合

```
声が既に変化している場合の選択肢:

選択肢A: 現在の声を使う
  - 本人が「今の声でいい」と判断した場合
  - 録音し、現在の声でクローニング
  - 構音障害のある音声からでもクローニングは可能
    （レポートA4 Paper #21: 合成構音障害音声の30%がSLPに誤判定）

選択肢B: 過去の声を復元する
  - 既存の音声データ（ビデオ等）から抽出
  - 構音障害音声 + 過去録音を組み合わせた復元も可能性あり
    （レポートA4 Paper #22: 劣化音声からのクリーン音声生成フレームワーク）
  - 本人/家族の同意のもとで実施

選択肢C: 両方を用意する
  - 状況に応じて切替可能にする
  - 進行に合わせて、過去の声への移行を段階的に
```

---

## 3. 音声合成システム

### 3.1 技術候補の比較（2026年2月時点）

レポートA4（OSS調査）およびB3（モデル比較）の調査結果に基づく、VoiceReach向けTier 1候補。

| モデル | アーキテクチャ | ライセンス | 日本語品質 | 最小データ | ストリーミング | 感情制御 | VoiceReachスコア |
|--------|-------------|----------|----------|----------|-------------|---------|-----------------|
| **CosyVoice 2/3** | Codec LM + Flow Matching | Apache-2.0 | Excellent（9言語ネイティブ対応） | 3-5秒（ゼロショット） | **150ms** | ネイティブ対応 | **8.80/10** |
| **GPT-SoVITS v4** | GPT + SoVITS 2段階 | MIT | Excellent（明示的JP対応） | 5秒（ゼロショット）/ 1分（微調整） | なし（バッチ） | スタイル転写 | **8.25/10** |
| **Fish Speech v1.5** | Codec LM | Apache（コード）/ CC-BY-NC-SA（モデル） | Excellent（10万時間+の日本語訓練データ） | 10秒 | 制限あり | 中程度 | **8.00/10** |
| **F5-TTS** | Flow Matching + DiT | MIT | Good（多言語訓練） | 3-5秒 | なし（非AR） | 限定的 | **7.50/10** |

Tier 2候補:

| モデル | 強み | 弱み |
|--------|------|------|
| **Chatterbox** | MIT、23言語、パラ言語タグ（[laugh], [cough]等）、透かし対応 | 日本語特化の評価が少ない |
| **Style-Bert-VITS2 JP Extra** | **日本語最高品質**（800時間JP事前訓練、WavLM弁別器）、スタイル制御 | AGPL、微調整必須（ゼロショット不可） |
| **IndexTTS2** | 感情-音色分離が最強 | 日本語対応が不明確 |

> **根拠**: レポートB3 Section 4.2の加重スコアリングマトリクス。評価軸は日本語品質(25%)、クローン品質(20%)、感情制御(15%)、リアルタイム性(15%)、ライセンス(10%)、効率性(10%)、統合容易性(5%)。

### 3.2 推奨アーキテクチャ: マルチモデルパイプライン

レポートB3 Section 4.3の推奨戦略に基づき、用途に応じた3層のフォールバック構成を採用する。

```
┌─────────────────────────────────────────────────────────────┐
│                    音声合成パイプライン                         │
│                                                              │
│  入力: テキスト + 感情パラメータ + 話者ID                      │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐    │
│  │ 日本語テキストフロントエンド                           │    │
│  │  MeCab/Sudachi → 読み推定 → アクセント推定 → 音素列   │    │
│  └─────────────────────────┬────────────────────────────┘    │
│                             │                                 │
│  ┌─────────────────────────┴────────────────────────────┐    │
│  │ 感情マッパー                                          │    │
│  │  PVP valence/arousal → EmoKnob方向ベクトル            │    │
│  │  OR 手動トーン選択 → 感情エンベディング                │    │
│  │  OR コンテキスト自動検出（対話相手、状況から推定）      │    │
│  └─────────────────────────┬────────────────────────────┘    │
│                             │                                 │
│  ┌─────────────────────────┴────────────────────────────┐    │
│  │ ルーター（レイテンシ/品質トレードオフ）                 │    │
│  │  ├─ キャッシュヒット? → 即時再生                       │    │
│  │  ├─ リアルタイム必要? → CosyVoice 2（ストリーミング）  │    │
│  │  └─ 品質優先? → GPT-SoVITS v4（微調整済み）           │    │
│  └─────────────────────────┬────────────────────────────┘    │
│                             │                                 │
│  ┌─────────────────────────┴────────────────────────────┐    │
│  │ 後処理                                                │    │
│  │  - ラウドネス正規化（ITU-R BS.1770）                   │    │
│  │  - 音質強化（必要に応じて）                            │    │
│  │  - 透かし（オプション、安全対策）                       │    │
│  └─────────────────────────┬────────────────────────────┘    │
│                             │                                 │
│  出力: オーディオストリーム → スピーカー                       │
└─────────────────────────────────────────────────────────────┘
```

### 3.3 プライマリTTSエンジン: CosyVoice 2/3

**選定理由**（レポートA4 Section 6.2, レポートB3 Section 5.2）:

| 項目 | 仕様 |
|------|------|
| ストリーミングレイテンシ | **150ms**（ファーストチャンク） |
| ライセンス | Apache-2.0 |
| 日本語対応 | 9言語ネイティブ対応（CosyVoice 3は1M時間訓練） |
| 感情制御 | ネイティブ対応（感情プロンプトによるコンディショニング） |
| アーキテクチャ | Codec LM + Flow Matching ハイブリッド（有限スカラー量子化） |
| 必要GPU | RTX 3060相当（~6GB VRAM） / **Apple Silicon M4 Pro（MLX対応見込み）** |
| 開発状況 | Alibaba Speech Labが活発に開発中（v3: 2025年5月） |

CosyVoice 2のストリーミング合成（150ms）はAAC用途のリアルタイム要件（<500ms）を大幅に上回る性能を持つ。統一ストリーミング/非ストリーミングアーキテクチャにより、品質劣化なくリアルタイム合成が可能（レポートA4 Paper #8）。

> **整合性**: 01_SYSTEM_ARCHITECTURE.md Section 8「音声合成: CosyVoice 2/3 + GPT-SoVITS v4」、Section 2.2のTTSメモリ配分「~0.5-1GB、ストリーミング150ms」と一致。

### 3.4 品質向上エンジン: GPT-SoVITS v4

**選定理由**（レポートB3 Section 5.2）:

| 項目 | 仕様 |
|------|------|
| ライセンス | MIT（最も寛容） |
| コミュニティ | GitHub 55Kスター（TTS分野最大） |
| 日本語対応 | Excellent（日本語コミュニティが活発） |
| 最小データ | 5秒（ゼロショット）/ 1分（few-shot） |
| 微調整 | 患者の60分録音データでの微調整に最適 |
| 用途 | 高品質な事前生成フレーズキャッシュの作成 |

GPT-SoVITS v4はストリーミング非対応のため、リアルタイム合成ではCosyVoiceが優先される。しかし、頻用フレーズの事前生成や、品質最優先のケースで使用する。

### 3.5 日本語特化オプション: Style-Bert-VITS2 JP Extra

| 項目 | 仕様 |
|------|------|
| ライセンス | AGPL-3.0 |
| 特徴 | 日本語800時間事前訓練、WavLM弁別器、スタイル制御 |
| 日本語品質 | **最高**（ピッチアクセント処理が最優秀） |
| 必要データ | 10-30分の微調整 |
| 評価 | arXiv 2505.17320にて正式評価（レポートA4 Paper #24） |

AGPL-3.0ライセンスの制約があるため、メインパイプラインには組み込まず、日本語品質の比較基準（ベンチマーク）として使用する。将来的にライセンス変更や互換モデルが登場した場合は再検討する。

### 3.6 感情・トーン制御

VoiceReachでは「同じテキストを、相手や状況に応じて異なるトーンで発話する」ことが核心的要件である。

#### 感情制御の実装戦略

```
層1: ネイティブ感情制御（CosyVoice 2/3）
  - 感情プロンプトによるコンディショニング
  - 録音セッション2で収集した感情サンプルを参照音声として使用
  - 基本的な感情カテゴリ（穏やか、嬉しい、真剣、優しい）をカバー

層2: EmoKnobフレームワーク（モデル非依存）
  - 話者エンベディング空間での感情方向ベクトル（レポートA4 Paper #15）
  - 連続的な感情制御（離散カテゴリではなく）
  - テキスト記述による感情指定にも対応
  - 任意のバックボーンモデルに後付け適用可能

層3: PVP連携による自動選択
  - PVPのvalence/arousalパラメータ → 感情ベクトルへ自動マッピング
  - 対話相手（妻、医師、孫など）に応じたデフォルトトーンをPVPから取得
  - 手動トーン選択があればそれを優先
```

#### トーン制御の具体例

```
テキスト: "ありがとう"

相手: 妻   → 穏やかで温かいトーン、ややゆっくり
  → EmoKnob: valence=+0.6, arousal=-0.2, warmth=+0.8
相手: 医師 → 丁寧でニュートラルなトーン
  → EmoKnob: valence=+0.2, arousal=0.0, formality=+0.5
相手: 孫   → 明るく楽しいトーン、やや高め
  → EmoKnob: valence=+0.8, arousal=+0.4, playfulness=+0.6

感情: positive, high arousal → 元気で明るい
感情: positive, low arousal  → 穏やかで温かい
感情: negative              → 静かで控えめ
```

> **根拠**: EmoKnob（Columbia大学, EMNLP 2024）は話者エンベディング空間で感情方向を抽出し、任意のモデルに適用可能（レポートA4 Paper #15）。Marco-Voice（Alibaba, 2025）の回転感情エンベディングも将来的な改良候補（レポートA4 Paper #16）。

---

## 4. 声の保存とストレージ

### 4.1 保存すべきデータ

```
患者ごとのボイスバンク構成:
  1. 生録音データ（WAV、48kHz/24bit、無圧縮） → 永久保存      ~2-5 GB
  2. 話者エンベディング（numpy配列、モデル非依存形式） → 永久保存  ~1 MB
  3. クローンモデルのチェックポイント → バージョン管理            ~2-4 GB
     - CosyVoice用微調整モデル
     - GPT-SoVITS用微調整モデル
  4. 感情エンベディング（EmoKnob方向ベクトル等） → バージョン管理  ~10 MB
  5. 事前生成フレーズキャッシュ（オーディオ） → 定期更新           ~500 MB
  6. メタデータ（録音条件、品質スコア、セッション情報）
```

### 4.2 ストレージ戦略

```
ローカル: 全データ（生録音 + モデル + キャッシュ）
  - Mac mini M4 Proの512GB SSDに十分収まる
  - 暗号化ストレージ（FileVault / LUKS）

クラウドバックアップ: 暗号化して保存
  - 生録音データはE2E暗号化
  - モデルチェックポイントのバージョン管理

複数コピー: 家族にもコピーを提供
  - USBドライブまたは暗号化クラウドストレージ共有
```

### 4.3 将来の技術進歩への備え

```
TTSモデルの進化速度（6-12ヶ月で世代交代）を考慮した設計:

1. 生録音データは絶対に保存する
   - 将来のより優れた合成技術で再モデル構築が可能
   - 特徴量だけでなく必ず生データを保存

2. モデル非依存の話者表現を保持
   - numpy配列での話者エンベディング保存
   - 新モデルが登場した際に生録音から再抽出可能

3. TTSインターフェースの抽象化
   - モデル切り替えを透過的に行う設計
   - 01_SYSTEM_ARCHITECTURE.md Section 8の方針と整合
```

> **根拠**: レポートB3 Section 5.5のマイグレーション戦略。TTSモデルの急速な進化に対応するため、抽象TTSインターフェースを定義し、モデル交換を容易にする。

---

## 5. 音声保存の早期開始プログラム（Phase 0）

### 5.1 診断直後パッケージ

プロダクト完成を待たずに提供する、音声保存のための独立パッケージ。

```
提供物:
  1. 録音ガイドブック（PDF）
     - 録音環境のセットアップ方法
     - 読み上げ100文セット（音素網羅、自然な会話文として設計）
     - 感情セッションのトピック提案
     - 自由会話のトピック提案

  2. 録音アプリ（スマートフォン / デスクトップ）
     - ガイド付き録音セッション
     - リアルタイム品質チェック（SNR、ピーク、ノイズ）
     - セッション管理（進捗表示、音素カバレッジ表示）
     - 48kHz/24bit WAV録音

  3. データ保管サービス
     - 暗号化クラウドストレージ
     - 家族への共有機能
     - 将来のVoiceReach統合への引き継ぎ

提供タイミング: ALS診断後できるだけ早く
コスト: 無料または最小限（社会的使命として）
```

### 5.2 医療機関との連携

```
理想的なフロー:
  1. 神経内科医がALS診断を告知
  2. 診断後のサポートパッケージの一つとして音声保存を案内
  3. 言語聴覚士（ST）が録音セッションをサポート
  4. 構音障害の進行モニタリングと並行して追加録音
  5. 発話不能になった時点でクローン音声に切替
```

### 5.3 既存の音声保存サービスとの連携

日本国内で利用可能な音声保存プログラム（レポートA4 Section 3.1）:

| サービス | 提供元 | 録音要件 | 日本語 | ALS適合性 |
|---------|--------|---------|--------|----------|
| **ALS SAVE VOICE** | Ory Lab + 東芝 + WITH ALS | ~10文/5-20分（CoEStation） | 日本語ネイティブ | 非常に高い |
| **Apple Personal Voice** | Apple | 150文/15分、オンデバイス処理 | iOS日本語対応 | 非常に高い |
| **Acapela My-Own-Voice v3** | Acapela Group | 50文/10-20分（DNN） | 非対応 | 高い（無料） |

VoiceReachの録音データはこれらのサービスとは独立に管理するが、Apple Personal Voiceで録音済みの患者に対しては、AVSpeechSynthesizer経由での統合も検討する。

---

## 6. ハードウェア要件と性能目標

### 6.1 推論ハードウェア

01_SYSTEM_ARCHITECTURE.md Section 2と整合したハードウェア構成:

| ターゲット | デバイス | TTS VRAM | 備考 |
|-----------|---------|---------|------|
| **推奨** | **Mac mini M4 Pro (24GB)** | ~0.5-1GB（ユニファイドメモリ） | 全パイプライン同時実行で~9-11GB使用 |
| 代替 | NVIDIA RTX 3060 12GB | ~6GB | Linux/Windows環境向け |
| エントリー | Mac mini M4 (24GB) | ~0.5-1GB | 軽量モデル推奨 |

### 6.2 性能目標

| 指標 | 目標値 | 根拠 |
|------|--------|------|
| ファーストチャンクレイテンシ | <200ms（ストリーミング） | CosyVoice 2: 150ms（レポートB3 Q3） |
| バッチ合成RTF | <0.5 | GPT-SoVITS: ~0.5-1.0 |
| MOS自然性 | >=4.0（60分データ+微調整） | レポートB3 Q4 |
| MOS話者類似度 | >=3.5 | レポートB3 Section 5.4 |
| GPU使用率ピーク | ~10%（バースト） | 01_SYSTEM_ARCHITECTURE.md Section 2.2 |

---

## 7. 倫理的考慮

### 7.1 本人の声の所有権

```
原則:
  - 録音データの所有権は患者本人に帰属
  - 患者の同意なしに第三者が使用することはできない
  - 患者の意思決定能力が低下した場合は法的代理人が管理

禁止事項:
  - クローン音声を使った患者本人以外の発話
  - 患者の同意のない範囲での音声データの利用
  - 商用目的での音声データの二次利用
```

### 7.2 患者死亡後のデータ

```
事前指示が必要な事項:
  - 死亡後の音声データの取り扱い（削除/保存/家族への引き渡し）
  - クローン音声モデルの存続可否
  - 家族が「故人の声」として使用する権利の有無
  - メモリアル目的（動画メッセージの作成等）の許可範囲

これらは事前指示書（Advance Directive）の一部として
患者の意思決定能力があるうちに記録しておく。
```

### 7.3 安全対策

```
音声クローンの悪用防止:
  - Perth透かし技術の組み込み検討（Chatterbox実装参考）
  - クローン音声であることを示すメタデータの付与
  - 使用ログの記録と監査トレイル
```

> **根拠**: レポートA4 Paper #20（Chatterbox）のPerTh透かし技術。日本の法律でのクローン音声の位置づけ（パブリシティ権等）は未確定であり、レポートA4 Open Question #7として継続調査中。

---

## 8. オープンクエスチョン

1. **日本語感情TTSのベンチマーク不在**: 感情制御付き音声クローニングの品質を日本語で比較する公開ベンチマークが存在しない。VoiceReach独自の評価を実施する必要がある（レポートA4 Open Question #1, レポートB3 Open Question #3）。

2. **構音障害音声からの復元**: 構音障害が部分的に進行した音声 + 発症前の録音から、高品質な声を復元できるか。2025年の構音障害論文（レポートA4 Paper #22）は可能性を示唆するが、日本語での実践検証が必要。

3. **Fish Speech / OpenAudioのライセンス**: モデル重みがCC-BY-NC-SAであり商用利用を制限。商用ライセンスモデルのリリース予定を注視する（レポートA4 Open Question #6）。

4. **Apple Personal Voice APIアクセス**: サードパーティアプリからApple Personal Voiceにアクセス可能か。AVSpeechSynthesizer経由で可能だが、品質と制御性に制限がある可能性（レポートA4 Open Question #8）。

5. **CosyVoice 2 vs. 3のデプロイ成熟度**: CosyVoice 3の方がベンチマーク上は優れるが、v2の方が安定運用実績がある。v2から開始してv3へマイグレーションする戦略が妥当か（レポートB3 Open Question #1）。

6. **Apple Silicon上でのモデル推論**: CosyVoice 2やGPT-SoVITSがApple Silicon（M4 Pro）上でMLX経由で動作するか。F5-TTSのMLXポートの存在から技術的には可能と推測されるが、検証が必要（レポートB3 Open Question #6）。

---

## 参考文献

### 調査レポート

- レポートA4: `research/phase_a/a4_voice_preservation_synthesis/report.md` — 音声保存・合成の先行研究、OSS調査、録音プロトコル設計
- レポートB3: `research/phase_b/b3_voice_synthesis_models/report.md` — 音声合成モデル詳細比較、加重スコアリング、推奨アーキテクチャ

### 主要学術論文

- CosyVoice 2: Du et al. (Alibaba), arXiv:2412.10117 — ストリーミング150ms、Apache-2.0
- CosyVoice 3: Du et al. (Alibaba), arXiv:2505.17589 — 1M時間、1.5Bパラメータ、9言語
- GPT-SoVITS: RVC-Boss, GitHub 55Kスター — MIT、few-shot日本語
- EmoKnob: Chen et al. (Columbia), EMNLP 2024, arXiv:2410.00316 — モデル非依存感情制御
- Style-Bert-VITS2 JP Extra: litagin02 et al. — 日本語800時間事前訓練
- F5-TTS: Chen et al., arXiv:2410.06885 — Flow Matching + DiT、MIT
- Marco-Voice: Alibaba AIDC, arXiv:2508.02038 — 話者-感情分離
- IndexTTS2: IndexTeam, arXiv:2506.21619 — 感情-音色分離
- VALL-E 2: Chen et al. (Microsoft), arXiv:2406.05370 — 人間パリティ達成
- Japanese TTS Benchmark: arXiv:2505.17320 — VITS vs Style-BERT-VITS2
- 構音障害音声クローニング: arXiv:2503.01266 — ALS関連
- ClonEval: Cai et al., arXiv:2504.20581 — オープン音声クローニングベンチマーク

### 主要OSSリポジトリ

- CosyVoice: https://github.com/FunAudioLLM/CosyVoice (19.6Kスター, Apache-2.0)
- GPT-SoVITS: https://github.com/RVC-Boss/GPT-SoVITS (55Kスター, MIT)
- Fish Speech: https://github.com/fishaudio/fish-speech (24.9Kスター, Apache-2.0)
- F5-TTS: https://github.com/SWivid/F5-TTS (14.1Kスター, MIT)
- Chatterbox: https://github.com/resemble-ai/chatterbox (22.7Kスター, MIT)
- Style-Bert-VITS2: https://github.com/litagin02/Style-Bert-VITS2 (1.2Kスター, AGPL-3.0)
- EmoKnob: https://github.com/tonychenxyz/emoknob

### 音声保存プログラム

- ALS SAVE VOICE (日本): https://prtimes.jp/main/html/rd/p/000000022.000019066.html
- Apple Personal Voice: https://machinelearning.apple.com/research/personal-voice
- Acapela My-Own-Voice: https://mov.acapela-group.com/
- Toshiba CoEStation: https://coestation.jp/
