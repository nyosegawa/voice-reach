# 05 — 感情検出・外部環境認識・コンテキスト統合

## 1. 感情検出の設計方針

### 1.1 汎用FERがALS患者に使えない理由

一般的な表情認識（Facial Expression Recognition）モデルは健常者の大きな表情変化を前提に訓練されている。ALS患者は顔面筋力の低下により表情の振幅が著しく小さくなるため、汎用FERは多くの場合「無表情」と判定してしまう。

### 1.2 パーソナルベースライン + 差分検出

VoiceReachでは絶対的な表情分類ではなく、**その患者の現在の「ニュートラル」を基準に、そこからの微小な偏差を検出する**相対的アプローチを取る。

```
汎用FER:       😊 Happy / 😢 Sad / 😠 Angry → ALS患者には不適
VoiceReach:   ベースラインからの偏差 → valence/arousal連続値
```

---

## 2. 検出可能な信号の階層

### Layer 1: 眼球周辺信号（最も長く残る）

ALSでも眼筋は比較的最後まで保たれるため、最も信頼できる情報源。

| 信号 | 意味の推定 | 検出方法 | 必要精度 |
|---|---|---|---|
| 瞳孔径の変化 | 興味/覚醒/ストレス | カメラ（1080p推奨） | 高 |
| まばたき頻度 | 緊張/疲労/動揺 | まばたき検出 | 低（容易） |
| まばたき持続時間 | 不快（長い）/快（短い） | タイミング解析 | 中 |
| 視線の安定度 | 関心（安定）/不満（不安定） | 視線推定から取得 | 低 |
| 視線滞留時間 | 関心度の直接指標 | 既存パイプライン | 低 |
| 瞼の開き具合 | 覚醒度/疲労/興味 | ランドマーク検出 | 中 |

### Layer 2: 顔面微細信号（中期まで残る）

| 信号 | 意味の推定 | 検出方法 | 必要精度 |
|---|---|---|---|
| 眉間の微細な皺 | 不快/集中/痛み | ランドマーク精密追跡 | 高 |
| 眉の高さの微変化 | 驚き/疑問 | MediaPipe 468点 | 中 |
| 口角のわずかな非対称 | 不満/違和感 | ランドマーク左右差 | 高 |
| 鼻翼のわずかな動き | 嫌悪/不快 | 高解像度映像解析 | 高 |

### Layer 3: 自律神経系信号（最後まで残る）

筋肉が完全に動かなくなっても自律神経系が制御する反応は残る。

| 信号 | 意味の推定 | 検出方法 | 必要精度 |
|---|---|---|---|
| 顔面血流変化（紅潮） | 感情的覚醒/怒り | rPPG（カメラ心拍推定） | 高 |
| 皮膚温度微変化 | ストレス/リラックス | サーモグラフィ（オプション） | 高 |
| 心拍数変動 | 快/不快/興奮 | rPPGから抽出 | 中 |
| 呼吸パターン | 興奮/落ち着き | 呼吸器データ or カメラ | 中 |

---

## 3. パーソナルベースラインの構築

### 3.1 初期キャリブレーション

発症初期〜中期に実施し、本人のベースラインを多角的に記録する。

```
セッション構成（約30分、分割可）:

1. リラックス状態（5分間の安静）
   → ニュートラル時の全指標を記録

2. 快の刺激（好きな音楽、家族の写真、面白い動画）
   → ポジティブ方向の変化パターンを記録

3. 不快の刺激（不快音、不快な温度変化）※穏やかなもの
   → ネガティブ方向の変化パターンを記録

4. 興味の刺激（新しい情報、クイズ）
   → 「関心」パターンを記録

5. 退屈の刺激（単調な刺激の繰り返し）
   → 「無関心」パターンを記録
```

### 3.2 動的再キャリブレーション

月次で自動的にベースラインを更新する。

```
処理フロー:
  1. 前月のベースラインデータを読み込み
  2. 今月の全センサーデータの統計値を算出
  3. 各チャネルの振幅変化を検出
     - 振幅縮小 → 閾値を下方修正
     - チャネル消失（ノイズレベル以下）→ 無効化
  4. 残存チャネルへの重み再配分
  5. 更新されたベースラインを保存

チャネル信頼性スコアの例:
  チャネル          6ヶ月前  3ヶ月前  現在    状態
  ─────────────────────────────────────────────
  口角変化           0.9      0.5     0.1    ほぼ消失
  眉の動き           0.8      0.6     0.3    低下中
  瞼の開き           0.7      0.7     0.6    安定
  まばたきパターン    0.8      0.8     0.8    安定 ★
  瞳孔径変化         0.7      0.7     0.7    安定 ★
  視線安定度         0.9      0.9     0.8    安定 ★
  rPPG              0.6      0.6     0.6    安定 ★
  顔面血流           0.5      0.5     0.5    安定 ★
```

---

## 4. 感情検出エンジンのアーキテクチャ

```
┌───────────────────────────────────────────────────┐
│              感情検出エンジン                       │
│                                                   │
│  [即時信号: 0.1秒単位]                             │
│   ├ 視線位置・滞留時間                             │
│   ├ まばたきタイミング                             │
│   └ 瞳孔径変化                                    │
│                                                   │
│  [短期信号: 1〜5秒単位]                            │
│   ├ 顔面ランドマークの微変化                       │
│   ├ 視線パターン（往復、逃避、固着）               │
│   └ まばたき頻度変動                               │
│                                                   │
│  [中期信号: 10秒〜1分単位]                         │
│   ├ rPPG心拍変動                                  │
│   ├ 顔面血流変化                                  │
│   └ 呼吸パターン                                  │
│                                                   │
│  各信号 × パーソナルベースライン → 偏差スコア       │
│  偏差スコア × チャネル信頼性重み → 統合感情推定     │
│                                                   │
│  出力:                                            │
│   valence:      -1.0（不快）〜 +1.0（快）         │
│   arousal:      0.0（無関心）〜 1.0（強い関心）    │
│   confidence:   0.0 〜 1.0（推定の確からしさ）     │
│   frustration:  0.0 〜 1.0（累積フラストレーション） │
└───────────────────────────────────────────────────┘
```

出力が離散的な「Happy/Sad/Angry」ではなくvalence×arousalの連続値である理由：
- 離散カテゴリはALS患者の微小な変化には粒度が粗すぎる
- 間違った分類は患者の尊厳を傷つけかねない
- 「ちょっと不快寄りかもしれない（confidence 0.4）」という控えめな推定の方が誠実

---

## 5. 外部環境認識

### 5.1 人物認識（対話相手の自動切替）

```
顔認識パイプライン:
  1. 顔検出 → バウンディングボックス
  2. 顔特徴量抽出 → 128次元埋め込みベクトル
  3. 登録済み顔データベースと照合
  4. 閾値以上の類似度 → 人物特定
  5. 未登録 → 「不明な人物」として処理

登録方法:
  - 初期セットアップ時に家族・介護者の顔を登録
  - 名前と関係性（wife, doctor, nurse等）を紐付け
  - 以降、自動で対話相手を検出しPVPのスタイルを切替
```

### 5.2 シーン認識（VLMによる状況記述）

```
外向きカメラ映像 → VLM → テキスト記述

例:
  入力: ベッドサイドの映像フレーム
  出力: "女性がお盆の上に食事（おかゆ、味噌汁）を載せて
         ベッドの左側に立っている。テレビがついており
         スポーツ中継が映っている。窓のカーテンは開いている。"
```

処理頻度: イベント検出時のみ（常時処理は不要）

### 5.3 患者視線 × 外部環境の統合

```
患者がテレビ方向を見ている + テレビがON
  → "チャンネル変えて" / "音量上げて" / "消して"

患者が窓方向を見ている + 天気が晴れ
  → "外に出たい" / "いい天気だね" / "カーテン閉めて"

患者がドア方向を見ている + 誰かが入ってきた
  → 相手に応じた挨拶候補

患者が時計をチラチラ見ている
  → "今何時？" / "予定ある？" / "薬の時間？"
```

### 5.4 イベント検出テーブル

| 検出イベント | トリガー条件 | 候補への影響 |
|---|---|---|
| 人物入室 | 新しい顔を検出 | 挨拶候補を優先、PVPスタイル切替 |
| 人物退室 | 既知の顔が消失 | 別れの挨拶候補 |
| 食事到着 | 食器/トレイ検出 | 食事関連の候補 |
| 医療処置準備 | 吸引器/注射器等検出 | "お願いします"等 |
| テレビON/OFF | 画面の明滅検出 + 音声 | メディア操作候補 |
| 部屋に誰もいない | 全顔消失 | 緊急コールを目立つ位置に |
| 複数人在室 | 3人以上検出 | 短い相槌候補を優先 |

---

## 6. マイクによる環境認識

### 6.1 音声認識（リスニングモード）

```
マイク入力
  → VAD（音声区間検出）
  → 話者分離（患者向けの発話か判別）
  → ASR（音声→テキスト）
  → LLMに会話コンテキストとして渡す
```

### 6.2 環境音分類

```
分類カテゴリ:
  - テレビ/ラジオ音声
  - 電話の着信音
  - ドアチャイム
  - 医療機器のアラーム音
  - 雨/風の音
  - 生活音（調理音、食器の音等）

活用:
  アラーム音検出 → "何の音？" "止めて" 候補を追加
  電話着信 → "出て" "出なくていい" 候補を追加
```

---

## 7. フラストレーション検出とエスケープ

### 7.1 フラストレーション累積の検出

```
フラストレーション指標:
  - まばたき頻度が通常の1.5倍以上
  - 視線が候補から離れて画面外に向かう
  - 心拍が持続的に上昇（rPPG）
  - 3巡以上候補を選択していない
  - 候補間の視線往復が激しい（迷い/困惑）

これらの加重和 → frustration スコア（0.0〜1.0）
```

### 7.2 エスケープ機構

```
frustration > 0.7 の場合、システムが自動で提案:

┌─────────────────────────────────────┐
│ うまく候補が出せていないようです。     │
│                                     │
│ [自分で入力する]   [カテゴリから選ぶ]  │
│ [相手に聞いてもらう] [少し休む]        │
└─────────────────────────────────────┘

「相手に聞いてもらう」:
  → 介護者デバイスに通知
  → 介護者がYes/No質問で絞り込み
  → 患者は最小限の操作で意図到達
```

---

## 8. パッシブ・コンテキスト読み取り（ゼロ操作）

明示的な入力なしで、カメラが常時モニタリングして検出できる情報。

```
検出内容                    → 介護者への通知例
───────────────────────────────────────────────
表情の微変化（眉間の緊張）   → "不快感や痛みがあるかもしれません"
まばたき頻度増加            → "疲れているようです"
視線の落ち着きなさ          → "何か気になることがありそうです"
目を閉じている時間が長い    → "眠たそうです"
心拍上昇が持続             → "興奮または不安の兆候があります"
```

これらは患者が何も操作しなくても基本的な状態が伝わる仕組みとして、特に夜間や介護者が別室にいる場合に重要。
