# 05 — 感情検出・VLM環境認識・コンテキスト統合

## 1. 設計方針

### 1.1 汎用FERがALS患者に使えない理由

一般的な表情認識（Facial Expression Recognition）モデルは健常者の大きな表情変化を前提に訓練されている。ALS患者は顔面筋力の低下により表情の振幅が著しく小さくなるため、汎用FERは多くの場合「無表情」と判定してしまう。2025年のメタ分析では、ALS・脳卒中における深層学習FERモデルの精度は73.2%にとどまり、他の神経疾患と比較して低いことが報告されている（レポートA5, 文献15: BioMedical Engineering OnLine, 2025）。

### 1.2 パーソナルベースライン + 相対変化量アプローチ

VoiceReachでは絶対的な表情分類ではなく、**その患者の現在の「ニュートラル」を基準に、そこからの微小な偏差を検出する**相対的アプローチを取る。

```
汎用FER:       😊 Happy / 😢 Sad / 😠 Angry → ALS患者には不適
VoiceReach:   ベースラインからの偏差 → valence/arousal連続値
```

この方針は文献的に強く支持される。パーソナライズされた感情認識モデルは汎化モデルよりも特定のコンテキストで優れた性能を示し（レポートA5, 文献25: JMIR AI, 2024）、キャリブレーションにより個人の顔貌の交絡効果を軽減できることが確認されている（レポートA5, 文献24: Frontiers in Psychology, 2026）。

### 1.3 ALS患者における特別な利点

瞳孔の自律神経制御はALSで特徴的に温存されることが報告されている（レポートA5, 文献16: Int. J. Mol. Sci., 2022）。これは自律神経系信号（瞳孔径、rPPG心拍）に基づく感情推定がALS患者に対して特に有望であることを意味する。また、ALS患者は比較的静止しているため、rPPGの動きアーティファクトの問題が軽減されるという利点もある。

---

## 2. 4チャネル感情検出

VoiceReachの感情検出は、単一カメラから抽出する**4つの独立したチャネル**を統合する（レポートA5 推奨構成）。いずれも表情筋の機能に依存せず、ALS進行後も長期間利用可能な信号である。

### 2.1 チャネル一覧

| チャネル | 信号源 | 推定対象 | 検出基盤 | 時間解像度 |
|---------|--------|---------|----------|-----------|
| CH1: 瞳孔径変動 | 瞳孔の拡大・縮小 | 覚醒度（arousal） | MediaPipe Iris + PupilSense参考実装 | 即時（0.1秒） |
| CH2: rPPG心拍 | 顔面皮膚の微細な色変化 | 快/不快/興奮 | open-rppg エンジン | 中期（10秒〜1分） |
| CH3: 視線パターン | 固視・サッケード・回避 | 関心/困惑/不安 | MediaPipe + 視線推定パイプライン | 短期（1〜5秒） |
| CH4: まばたきパターン | 頻度・持続時間・タイミング | 緊張/疲労/動揺 | EAR（Eye Aspect Ratio） | 短期（1〜5秒） |

### 2.2 CH1: 瞳孔径変動

瞳孔径は**覚醒度（arousal）の信頼性の高い指標**であることが文献上のコンセンサスとなっている（レポートA5, 文献12: Healthcare, 2023）。機械学習を用いた研究では、arousal予測85%、valence予測91%の効率が報告されている（レポートA5, 文献9: arXiv, 2025）。

**実装方針:**

- **ランドマーク取得**: MediaPipe Irisの5つの瞳孔キーポイント + 71の眼球キーポイントをリアルタイムで使用
- **瞳孔径推定**: PupilSense/EyeDentify（2024-2025, ETRA 2025）のアプローチを参考に、ResNetベースの回帰モデルで高精度化。EyeDentifyデータセット（51名、212,073画像）でResNet-18がMAE 0.1340を達成している（レポートA5, 文献10）
- **光量補正**: 虹彩サイズとの比率正規化により環境光の影響（瞳孔対光反射: PLR）を分離。画像アップスケーリングにより予測精度が向上することも確認されている（レポートA5, 文献11: VISAPP 2025）

### 2.3 CH2: rPPG心拍推定

rPPG（遠隔光電容積脈波）はWebカメラから顔面皮膚の微細な色変化を検出し、心拍数・心拍変動を推定する技術である。

**最新精度:**

| 指標 | モデル | 精度 | 出典 |
|------|--------|------|------|
| 心拍数（HR） | VitalLens 2.0 | MAE 1.57 bpm | レポートA5, 文献4 |
| HRV-SDNN | VitalLens 2.0 | MAE 10.18 ms | レポートA5, 文献4 |
| Webカメラ相関 | 231名オンライン録画 | r > 0.75（パルスオキシメータ対比） | レポートA5, 文献1 |

**採用エンジン: open-rppg**

open-rppg（GitHub: KegangWangCCNU/open-rppg）を第一候補とする（レポートA5, セクション3.1）。

| 項目 | 内容 |
|------|------|
| 対応モデル | 最新7種（iBVPNet, PhysMamba, RhythmFormer等） |
| 推論基盤 | JAXベース低遅延 |
| 対応入力 | リアルタイムWebカメラ |
| ライセンス | 要確認 |
| フォールバック | VitalLens API（最高精度、商用APIベース） |

**VoiceReachでの利用方針:**

- 心拍数の**変化トレンド**（上昇・安定・下降）を主に利用し、精密なHRV値への依存は避ける
- 額・頬領域のROI選択により全顔ROIより高精度を実現（レポートA5, 文献7: npj Digital Medicine, 2025）
- 低照度環境では精度が低下するため、近赤外LED補助を検討（夜間モード用）

**制限事項:**

- 低照度環境で精度が著しく低下（レポートA5, 文献3: npj Digital Medicine, 2025）
- 高心拍数時に精度が低下する傾向がある
- HRV推定はHR推定より困難（PRV信号のノイズ・時間分解能の制約）

### 2.4 CH3: 視線パターン

視線推定パイプライン（`docs/01_SYSTEM_ARCHITECTURE.md` Layer 1）から取得される視線位置の時系列データを、感情推定の入力信号として二次利用する。

| パターン | 推定される状態 | 検出方法 |
|---------|--------------|---------|
| 長時間固視 | 強い関心/集中 | 固視持続時間の閾値判定 |
| 高頻度サッケード | 困惑/迷い/不安 | サッケード回数のカウント |
| 視線回避（画面外への逃避） | 不満/フラストレーション | 視線ゾーンの遷移パターン |
| 候補間の激しい往復 | 決めかねている/困惑 | 往復回数と速度 |
| 安定した走査 | 落ち着いた状態 | 視線軌跡の滑らかさ |

### 2.5 CH4: まばたきパターン

まばたきはEAR（Eye Aspect Ratio）ベースで検出する。随意的まばたき（意図的操作）と不随意的まばたき（感情指標）を区別する。

| 信号 | 推定される状態 | ベースラインとの比較 |
|------|--------------|-------------------|
| 頻度増加 | 緊張/疲労/動揺 | 通常頻度の1.3倍以上で検出 |
| 頻度減少 | 集中/凝視 | 通常頻度の0.7倍以下で検出 |
| 長い閉眼持続時間 | 不快/眠気 | 200ms超過で検出 |
| 短い閉眼持続時間 | 快/覚醒 | 100ms未満で検出 |
| 不規則なタイミング | ストレス | タイミング分散の増加 |

---

## 3. パーソナルベースラインの構築

### 3.1 初期キャリブレーション

発症初期〜中期に実施し、本人のベースラインを4チャネルすべてで記録する。文献では、ビルトインキャリブレーションと刺激前ベースライン手順の組み合わせが推奨されている（レポートA5, 文献24: Frontiers in Psychology, 2026）。

```
セッション構成（約30分、分割可能）:

Phase 1: 安静時ベースライン（5分）
  - 全4チャネルのニュートラル値を記録
  - 30秒ウィンドウで平均・標準偏差を算出

Phase 2: 快の刺激（5分）
  - 好きな音楽、家族の写真、面白い動画
  - 各チャネルのポジティブ方向変化パターンを記録
  - ベースラインからの差分ベクトルを算出

Phase 3: 不快の刺激（5分）
  - 不快音、不快な温度変化（穏やかなもの）
  - 各チャネルのネガティブ方向変化パターンを記録

Phase 4: 興味の刺激（5分）
  - 新しい情報、クイズ
  - 覚醒度の上昇パターンを記録

Phase 5: 退屈の刺激（5分）
  - 単調な刺激の繰り返し
  - 覚醒度の低下パターンを記録
```

### 3.2 動的再キャリブレーション

ALS進行に伴うチャネルの振幅変化を追跡し、自動的にベースラインを更新する。

```
更新頻度:
  - 日次: 1日の使用データから移動平均ベースラインを更新
  - 週次: チャネルごとの振幅変化を評価
  - 月次: チャネル信頼性スコアの再計算と重み再配分

処理フロー:
  1. 前回のベースラインデータを読み込み
  2. 期間内の全チャネルデータの統計値を算出
  3. 各チャネルの振幅変化を検出
     - 振幅縮小 → 閾値を下方修正（より小さな変化を検出可能に）
     - チャネル消失（ノイズレベル以下）→ 無効化
  4. 残存チャネルへの重み再配分
  5. 更新されたベースラインを保存

チャネル信頼性スコアの推移例:
  チャネル          6ヶ月前  3ヶ月前  現在    状態
  ──────────────────────────────────────────────────
  CH1: 瞳孔径変動    0.7      0.7     0.7    安定 ★
  CH2: rPPG心拍      0.6      0.6     0.6    安定 ★
  CH3: 視線パターン   0.9      0.9     0.8    安定 ★
  CH4: まばたき       0.8      0.8     0.8    安定 ★

  ※参考: 表情筋ベース信号（VoiceReachでは補助扱い）
  口角変化           0.9      0.5     0.1    ほぼ消失
  眉の動き           0.8      0.6     0.3    低下中
  眉間の緊張         0.7      0.4     0.2    低下中
```

4チャネルすべてが自律神経系または眼球運動ベースであるため、ALS進行後も信頼性スコアが安定する設計となっている。

---

## 4. 感情検出エンジンのアーキテクチャ

### 4.1 パイプライン全体図

```
┌───────────────────────────────────────────────────────────────┐
│                    感情検出エンジン                             │
│                                                               │
│  ┌──────────────────┐          ┌──────────────────────┐       │
│  │ MediaPipe Face   │          │ open-rppg エンジン    │       │
│  │ Mesh + Iris      │          │ (JAXベース低遅延)     │       │
│  └──────┬───────────┘          └──────────┬───────────┘       │
│         │                                 │                   │
│   ┌─────┴─────────────┐            ┌──────┴──────────┐       │
│   │ CH1: 瞳孔径変動    │            │ CH2: rPPG心拍   │       │
│   │ (PupilSense参考)   │            │ (HR変化トレンド) │       │
│   │ 光量補正: 虹彩比率  │            │ ROI: 額・頬領域  │       │
│   └─────┬─────────────┘            └──────┬──────────┘       │
│         │                                 │                   │
│   ┌─────┴─────────────┐            ┌──────┴──────────┐       │
│   │ CH3: 視線パターン  │            │ CH4: まばたき   │       │
│   │ (固視/サッケード/  │            │ (EAR検出)       │       │
│   │  回避パターン)     │            │ 随意/不随意分離  │       │
│   └─────┬─────────────┘            └──────┬──────────┘       │
│         │                                 │                   │
│   ┌─────┴─────────────────────────────────┴──────────┐       │
│   │  パーソナルベースライン差分算出                      │       │
│   │  各チャネルのz-score計算（1秒ウィンドウ平均化）     │       │
│   └──────────────────────┬───────────────────────────┘       │
│                          │                                    │
│   ┌──────────────────────┴───────────────────────────┐       │
│   │  チャネル信頼性重み付き融合                         │       │
│   │  Late Fusion + Attention機構                      │       │
│   │  (各チャネルの信頼性スコアをattention weightに使用) │       │
│   └──────────────────────┬───────────────────────────┘       │
│                          │                                    │
│   ┌──────────────────────┴───────────────────────────┐       │
│   │  入力パターン信号との統合（補助チャネル）            │       │
│   │  (選択速度、キャンセル頻度、候補巡回回数)           │       │
│   └──────────────────────┬───────────────────────────┘       │
│                          │                                    │
│   出力:                                                       │
│     valence:      -1.0（不快）〜 +1.0（快）                   │
│     arousal:      0.0（無関心）〜 1.0（強い関心）              │
│     confidence:   0.0 〜 1.0（推定の確からしさ）               │
│     frustration:  0.0 〜 1.0（累積フラストレーション）          │
└───────────────────────────────────────────────────────────────┘
```

### 4.2 出力仕様

出力が離散的な「Happy/Sad/Angry」ではなく**valence x arousal の連続値**である理由:

- 離散カテゴリはALS患者の微小な変化には粒度が粗すぎる
- 間違った分類は患者の尊厳を傷つけかねない
- 「ちょっと不快寄りかもしれない（confidence 0.4）」という控えめな推定の方が誠実
- confidenceが低い場合は推定を行わない方針を徹底する

### 4.3 リアルタイム推定フロー

```
入力信号（4チャネル同時取得）
  → 1秒ウィンドウ平均化
  → パーソナルベースラインとの差分算出
  → 各チャネルのz-score計算
  → チャネル信頼性重みで加重平均（Late Fusion）
  → valence / arousal / confidence / frustration 出力
  → confidence < 閾値 → 推定結果を抑制（推定しない）
```

---

## 5. VLM環境認識

### 5.1 概要

VoiceReachの外向きカメラ（10fps, 720p）はVLM（Vision-Language Model）を用いてベッドサイドの環境を認識し、文脈に応じた発話候補生成のトリガーとする。本設計はレポートB5の調査結果に基づく。

プライバシー保護の観点から、カメラ映像は**原則ローカル処理**とし、クラウド送信時は患者の明示的同意に基づく（レポートB5, セクション Q5）。

### 5.2 3層ハイブリッドアーキテクチャ

レポートB5の推奨に基づき、以下の3層構成を採用する（`docs/01_SYSTEM_ARCHITECTURE.md` Layer 1 外向きカメラと整合）。

```
外向きカメラ (10fps, 720p)
  │
  ├→ Layer 1: 軽量動体検出（常時、CPU ~5%）
  │    [OpenCV背景差分]
  │    ├→ 変化なし → 省電力待機
  │    └→ 変化あり → Layer 2をトリガー
  │
  ├→ Layer 2: ローカルVLM分析（GPU ~10-15%、断続的）
  │    [MiniCPM-V 4.0 (4.1B, Q4量子化, ~2.5GB)]
  │    ├→ 定期スナップショット: 5-10秒間隔でシーン記述更新
  │    ├→ イベント駆動: Layer 1のトリガーで即時実行
  │    ├→ 人物認識 / 物体認識 / シーン記述テキスト生成
  │    └→ テキスト → コンテキスト統合層へ
  │
  └→ Layer 3: クラウドVLM（患者同意時のみ）
       [Gemini 2.5 Flash]
       ├→ ローカルVLMで判断困難な場面のフォールバック
       └→ 月額コスト目安: 数百円（1日100画像分析の場合）
```

### 5.3 ローカルVLMの選定

レポートB5のベンチマーク評価に基づく採用優先順位:

| 優先度 | モデル | パラメータ | メモリ（Q4） | M4 Pro推論速度 | 選定理由 |
|--------|--------|----------|-------------|---------------|---------|
| 1 | **MiniCPM-V 4.0** | 4.1B | ~2.5GB | ~45 tok/s | GPT-4.1-mini超え。他タスクとの共存が容易 |
| 2 | Qwen2.5-VL-3B | 3B | ~2.0GB | ~55 tok/s | 日本語品質が最も高い3Bクラス |
| 3 | Moondream 2B | 1.8B | ~1.3GB | ~70 tok/s | 超軽量。リソース制約時の代替 |

MiniCPM-V 4.0はOCRBenchでGPT-4o超えの性能を示しており（レポートB5, セクション Q2）、テレビ画面のテロップや医療機器ディスプレイの読み取りにも対応可能である。Apache 2.0ライセンス。

### 5.4 クラウドVLMのフォールバック

| 優先度 | モデル | 入力コスト（$/1M tok） | 選定理由 |
|--------|--------|---------------------|---------|
| 1 | **Gemini 2.5 Flash** | ~$0.15 | 圧倒的コスト効率（GPT-4oの約1/17） |
| 2 | GPT-4o | ~$2.50 | 安定した性能。日本語品質が高い |

### 5.5 定期スナップショット + イベント駆動

| 処理方式 | 間隔 | GPU負荷 | 用途 |
|---------|------|---------|------|
| 定期スナップショット | 5-10秒間隔 | ~10-15% | シーン記述の定常更新 |
| イベント駆動（動体検出トリガー） | 即時 | バースト | 入室・退室・物体移動等の重要変化 |

**レイテンシ実測予測**（M4 Pro + MiniCPM-V 4.0）:

| 指標 | 目標 | 予測値 |
|------|------|--------|
| シーン記述更新間隔 | 10秒以内 | 5-8秒 |
| イベント検出 → 候補生成 | 5秒以内 | 2-4秒 |
| メモリ使用量（VLM分） | 4GB以下 | ~2.5GB |

### 5.6 VLMが認識する環境要素

#### 人物認識

```
外向きカメラ映像 → VLM → 服装・体格の記述
  + 顔認識DB照合 → 個人識別

例:
  VLM出力: "白衣を着た女性がベッドサイドに来ている"
  顔認識DB: "山田看護師"
  統合結果: {"name": "山田", "relation": "nurse", "position": "bedside-left"}
```

VLMは「誰か来た」「女性が2人いる」程度の認識は実用的だが、個人識別（「山田さん」等）はVLMの範疇外であり、別途顔認識モデルとの組み合わせが必要（レポートB5, セクション Q3）。

#### 物体認識 / シーン記述

```
入力: ベッドサイドの映像フレーム
出力: "テーブルに食事トレイ（おかゆ、味噌汁、焼き魚）。
       テレビがついており天気予報が映っている。
       窓のカーテンは開いている。"
```

日常物体の検出はローカルVLMでも高精度。医療機器の細かい表示（点滴の残量、モニター数値等）の読み取りにはOCR能力の高いMiniCPM-V 4.0が有効（レポートB5, セクション Q3）。

### 5.7 イベント検出テーブル

| 検出イベント | トリガー条件 | 候補への影響 |
|---|---|---|
| 人物入室 | 動体検出 + VLMで新しい人物を認識 | 挨拶候補を優先、PVPスタイル切替 |
| 人物退室 | 既知の人物が消失 | 別れの挨拶候補 |
| 食事到着 | 食器/トレイ検出 | 食事関連の候補（「おいしそうだね」等） |
| 医療処置準備 | 吸引器/注射器等検出 | 「お願いします」等 |
| テレビ内容変化 | 画面テキスト/映像変化 | メディア操作候補、話題候補 |
| 部屋に誰もいない | 全人物消失 | 緊急コールを目立つ位置に |
| 複数人在室 | 3人以上検出 | 短い相槌候補を優先 |
| 天候/時間変化 | 窓からの光量変化 | 天気・季節の話題候補 |

### 5.8 VLMによるAAC革新

VLMの環境認識は、従来AACの決定的な弱点を補う（レポートB5, セクション Q6）。

| シナリオ | VLMの出力 | 生成される発話候補 |
|---|---|---|
| 食事トレイが置かれた | 「テーブルに食事トレイ。味噌汁、ご飯、焼き魚」 | 「今日のお昼はおいしそうだね」「味噌汁から先に食べたい」 |
| テレビでニュースが流れている | 「テレビに天気予報。明日は晴れ」 | 「明日は天気いいんだね」「散歩日和だね」 |
| 面会者が来た | 「男性が入室。花束を持っている」 | 「来てくれてありがとう」「きれいな花だね」 |
| 窓の外の景色 | 「窓の外は桜が咲いている」 | 「桜がきれいだね」「もう春だなぁ」 |
| 医療スタッフが機器を操作 | 「看護師が点滴を調整中」 | 「よろしくお願いします」 |

従来AAC（事前に用意されたフレーズ）では、「今この瞬間」の文脈に即した発話が困難であった。VLM環境認識により、会話の自然さが飛躍的に向上し、視線入力の負担を大幅に軽減する（選ぶだけで済む）。

### 5.9 プライバシー保護

ALS患者のベッドサイドという極めてプライバシーが高い環境において、映像データの取り扱いには厳格なルールを適用する（レポートB5, セクション Q5）。

| 原則 | 実装 |
|------|------|
| ローカル処理優先 | カメラ映像は原則ローカルVLMで処理。クラウドには送信しない |
| 明示的同意 | クラウドVLM利用時は患者の明示的同意を取得 |
| 匿名化 | クラウド送信時は人物情報を匿名化処理 |
| 短期保持 | 映像データはローカルで数分のみ保持。分析後は即削除 |
| ネットワーク制御 | VLMプロセスのネットワークアクセスをデフォルト遮断 |
| ゼロリテンション | クラウドAPI利用時はゼロリテンションポリシーを確認 |

---

## 6. マイクによる環境認識

### 6.1 音声認識（リスニングモード）

```
マイク入力
  → VAD（音声区間検出）
  → 話者分離（患者向けの発話か判別）
  → ASR（音声 → テキスト）
  → LLMに会話コンテキストとして渡す
```

### 6.2 環境音分類

```
分類カテゴリ:
  - テレビ/ラジオ音声
  - 電話の着信音
  - ドアチャイム
  - 医療機器のアラーム音
  - 雨/風の音
  - 生活音（調理音、食器の音等）

活用:
  アラーム音検出 → "何の音？" "止めて" 候補を追加
  電話着信 → "出て" "出なくていい" 候補を追加
```

---

## 7. コンテキスト統合層

### 7.1 統合の全体像

感情検出（セクション2-4）、VLM環境認識（セクション5）、音声認識（セクション6）、およびIoT/スケジュール情報を統合し、LLMプロンプトに注入するコンテキストフレームを生成する。

```
┌──────────────────────────────────────────────────────────────┐
│                 コンテキスト統合層                             │
│                                                              │
│  ┌──────────────┐  ┌────────────────┐  ┌──────────────────┐ │
│  │ 感情検出      │  │ VLM環境認識     │  │ 音声認識          │ │
│  │ valence      │  │ シーン記述      │  │ 直近の発話テキスト │ │
│  │ arousal      │  │ 人物情報        │  │ 話者情報          │ │
│  │ confidence   │  │ 検出物体        │  │ 環境音カテゴリ    │ │
│  │ frustration  │  │ イベント        │  │                  │ │
│  └──────┬───────┘  └───────┬────────┘  └────────┬─────────┘ │
│         │                  │                    │           │
│  ┌──────┴──────────────────┴────────────────────┴─────────┐ │
│  │                     信号統合エンジン                      │ │
│  │  + 時刻/スケジュール情報                                 │ │
│  │  + IoTセンサー（室温、湿度、照度）                        │ │
│  │  + 会話履歴                                              │ │
│  └──────────────────────┬───────────────────────────────┘   │
│                         │                                    │
│  ┌──────────────────────┴───────────────────────────────┐   │
│  │          コンテキストフレーム（JSON）                   │   │
│  │  → LLMプロンプトの [Context] セクションに注入           │   │
│  └──────────────────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────────┘
```

### 7.2 コンテキストフレームの構造

`docs/01_SYSTEM_ARCHITECTURE.md` セクション4.1で定義されたJSON構造に準拠する。

```json
{
  "timestamp": "2026-02-17T14:30:00+09:00",
  "patient_state": {
    "emotion": {
      "valence": 0.1,
      "arousal": 0.4,
      "confidence": 0.6,
      "frustration": 0.0,
      "channels": {
        "pupil_dilation": { "delta": 0.12, "confidence": 0.7 },
        "rppg_heart_rate": { "bpm": 78, "baseline_delta": 5, "confidence": 0.8 },
        "gaze_pattern": { "fixation_duration_ms": 450, "saccade_rate": 2.1 },
        "blink_pattern": { "rate": 18, "baseline": 15, "voluntary_ratio": 0.1 }
      }
    },
    "gaze_zone": "center-right",
    "gaze_target_object": "television",
    "fatigue_level": 0.3
  },
  "environment": {
    "people_present": [
      {"name": "花子", "relation": "wife", "position": "bedside-left"}
    ],
    "objects_detected": ["food_tray", "television_on", "remote_control"],
    "scene_description": "妻が食事トレイを持ってベッドサイドに来ている",
    "vlm_source": "local_minicpm_v4"
  },
  "audio": {
    "last_speech": {
      "speaker": "花子",
      "text": "お昼ごはん、おかゆとうどんどっちがいい？"
    }
  },
  "schedule": {
    "current": "lunch_time",
    "next": {"event": "medication", "time": "15:00"}
  }
}
```

### 7.3 患者視線 x 環境認識の統合

VLMの環境認識結果と視線推定を組み合わせることで、患者が「何を見ているか」から意図を推定する。

```
患者がテレビ方向を見ている + テレビがON
  → "チャンネル変えて" / "音量上げて" / "消して"

患者が窓方向を見ている + 天気が晴れ
  → "外に出たい" / "いい天気だね" / "カーテン閉めて"

患者がドア方向を見ている + 誰かが入ってきた
  → 相手に応じた挨拶候補

患者が時計をチラチラ見ている
  → "今何時？" / "予定ある？" / "薬の時間？"

患者が食事トレイを見ている + 食事が到着
  → "いただきます" / "おいしそう" / "味噌汁から"
```

### 7.4 LLMプロンプトへの注入

コンテキストフレームは、三段ハイブリッドLLM推論（`docs/01_SYSTEM_ARCHITECTURE.md` セクション5）の `[Context]` セクションとして注入される。

```
[System Prompt]
  PVP（Personal Voice Profile）: ~2000トークン

[Context]  ← コンテキスト統合層の出力
  コンテキストフレーム（最新）: ~500トークン
  直近の会話履歴（最大10ターン）: ~500トークン

[Task Instruction]
  候補生成ルール + 意図軸分散指示: ~300トークン
```

感情データの注入により、LLMは以下のような候補調整を行う:

| 感情状態 | 候補への影響 |
|---------|-------------|
| valence高 + arousal高 | 積極的・肯定的な候補を優先 |
| valence低 + arousal高 | 不満表明候補を含める |
| valence低 + arousal低 | 「疲れた」「休みたい」等の候補 |
| frustration高 | エスケープ機構を発動（セクション8） |
| confidence低 | 感情情報を候補生成に反映しない |

---

## 8. フラストレーション検出とエスケープ

### 8.1 フラストレーション累積の検出

4チャネル感情検出からフラストレーション専用のスコアを算出する。

```
フラストレーション指標（4チャネルから算出）:
  - CH1: 瞳孔径が持続的に拡大（ストレス反応）
  - CH2: rPPG心拍が持続的に上昇
  - CH3: 視線が候補から離れて画面外に向かう / 候補間の往復が激しい
  - CH4: まばたき頻度が通常の1.5倍以上

補助指標（入力パターン）:
  - 3巡以上候補を選択していない
  - 候補選択後のキャンセルが連続

これらの加重和 → frustration スコア（0.0〜1.0）
```

### 8.2 エスケープ機構

```
frustration > 0.7 の場合、システムが自動で提案:

┌─────────────────────────────────────┐
│ うまく候補が出せていないようです。     │
│                                     │
│ [自分で入力する]   [カテゴリから選ぶ]  │
│ [相手に聞いてもらう] [少し休む]        │
└─────────────────────────────────────┘

「相手に聞いてもらう」:
  → 介護者デバイスに通知
  → 介護者がYes/No質問で絞り込み
  → 患者は最小限の操作で意図到達
```

---

## 9. パッシブ・コンテキスト読み取り（ゼロ操作）

明示的な入力なしで、4チャネル感情検出 + VLM環境認識が常時モニタリングして介護者に伝達する情報。

```
検出ソース                    → 介護者への通知例
───────────────────────────────────────────────────────
CH1: 瞳孔径持続拡大           → "興奮または不安の兆候があります"
CH2: rPPG心拍上昇持続         → "心拍が上昇しています"
CH3: 視線の落ち着きなさ        → "何か気になることがありそうです"
CH4: まばたき頻度増加          → "疲れているようです"
CH4: 長い閉眼持続              → "眠たそうです"
VLM: 部屋に誰もいない          → "お一人です"（緊急コール待機）
VLM: 医療機器アラーム検出      → "機器アラームが鳴っています"
```

これらは患者が何も操作しなくても基本的な状態が伝わる仕組みとして、特に夜間や介護者が別室にいる場合に重要である。

---

## 10. 技術選定まとめと出典

### 10.1 感情検出の技術選定

| コンポーネント | 採用技術 | 根拠 |
|--------------|---------|------|
| 瞳孔径取得 | MediaPipe Iris + PupilSense参考実装 | レポートA5, セクション RQ3 |
| rPPGエンジン | open-rppg（JAXベース） | レポートA5, セクション 6.1 |
| rPPGフォールバック | VitalLens API | レポートA5, セクション 6.1 |
| 感情推定アーキテクチャ | Late Fusion + Attention | レポートA5, セクション RQ6 |
| ベースラインアプローチ | パーソナルベースライン + 動的再キャリブレーション | レポートA5, セクション RQ5 |

### 10.2 VLM環境認識の技術選定

| コンポーネント | 採用技術 | 根拠 |
|--------------|---------|------|
| 動体検出 | OpenCV背景差分 | レポートB5, セクション Q4 |
| ローカルVLM | MiniCPM-V 4.0 (4.1B) | レポートB5, セクション 5.2 |
| クラウドVLM | Gemini 2.5 Flash | レポートB5, セクション 5.2 |
| 推論フレームワーク | MLX-VLM / vllm-mlx | レポートB5, セクション 3.2 |

### 10.3 主要参考文献

感情検出関連（レポートA5より）:

1. rPPG in the wild: Remote HR imaging via online webcams. *Behavior Research Methods*, 2024.
2. VitalLens 2.0: High-Fidelity rPPG for HRV Estimation. *arXiv*, 2025.
3. Reliability of rPPG under low illumination and elevated HR. *npj Digital Medicine*, 2025.
4. PupilSense: Webcam-Based Pupil Diameter Estimation. *ETRA 2025*.
5. Emotion Detection Based on Pupil Variation. *Healthcare (MDPI)*, 2023.
6. Sensory Involvement in ALS. *Int. J. Mol. Sci.*, 2022.
7. Personalized vs Generalized Emotion Recognition. *JMIR AI*, 2024.
8. Facial obstructions and baseline correction in affective computing. *Frontiers in Psychology*, 2026.
9. FER DL algorithms in neurological disorders: meta-analysis. *BioMedical Engineering OnLine*, 2025.
10. MER-MFVA: rPPG + Action Unit + 眼球運動の統合. *Pattern Recognition*, 2025.

VLM環境認識関連（レポートB5より）:

11. FastVLM: Efficient Vision Encoding for VLMs. Apple ML Research, CVPR 2025.
12. MiniCPM-V 4.5: Cooking Efficient MLLMs. OpenBMB, 2025.
13. VLM for Edge Networks: A Comprehensive Survey. *arXiv:2502.07855*, 2025.
14. FedVLM: Scalable Personalized VLMs through Federated Learning. *arXiv:2507.17088*, 2025.
15. Future Technologies in AAC: A Scoping Review. *Frontiers in Communication*, 2025.
