# 02 — 視線推定・入力インターフェース設計

> 本ドキュメントは調査レポート A1（視線推定先行研究）、B4（視線推定モデル）、A6（指入力・IAL）、B7（BCI/EMG等将来入力）の知見に基づく。仮説ベースの記述は排除し、文献・ベンチマーク・実測値を根拠とする。

---

## 1. 設計思想

### 1.1 脱 Dwell Click

従来の視線入力では「一定時間じっと見つめる」Dwell Click が標準だが、この方式には臨床上の根本的問題がある。

- **眼精疲労**: 視線を固定し続ける不自然な眼球運動が疲労を蓄積する
- **誤操作**: 画面を何気なく見ただけで意図しない操作が発生する
- **速度限界**: 1クリックに2〜3秒を要し、入力速度に上限がある
- **休めない**: 画面を見ている限り常に「入力状態」であり、視覚的な休憩ができない

Eye-Tracking + Switch-Scanning の併用研究（2022年、PMC）では、視線単独（2.34 WPM、エラー率26.9%）に対し、視線+スイッチ併用（1.54 WPM、エラー率7.1%）で**エラー率を74%削減**できることが示されている（レポートA6）。速度はやや低下するが、正確性の大幅向上は ALS 患者のコミュニケーション品質に直結する。

VoiceReach では「目で見る＝入力確定」という等式を切り離す。目はポインティング（どのゾーンに関心があるか）のみを担当し、確定は指タップで行う。

### 1.2 ゾーンベース設計

ピクセル精度の視線追従ではなく、画面をゾーンに分割して「どのゾーンを見ているか」を判定する。

```
Webカメラの精度（レポートA1, B4に基づく実測値）:
  未校正: ~4°（画面上で約4cm @ 60cm距離）
  5点校正後: ~2.5°（画面上で約2.6cm）
  FAZE meta-learning後: sub-2°

フルHD画面のゾーン分割と精度要件:
  4分割: 各ゾーン約12°× 7° → 5°で十分 → 未校正でも信頼度 >95%
  9分割: 各ゾーン約 8°×4.5° → 3°が必要 → 5点校正で信頼度 >85%
 16分割: 各ゾーン約 6°×3.4° → 2°が必要 → meta-learning校正で実用可能
```

これにより、近赤外線専用デバイスなしでも実用的な入力が実現する。

---

## 2. 視線推定パイプライン

### 2.1 技術選定の結論

調査レポート B4 の加重スコアリングに基づき、**MediaPipe Face Mesh + MobileOne s0 バックボーン + L2CS-Net dual loss** を選定した（加重スコア 4.25/5.0、次点 GazeFollower 3.75）。

| 評価軸 | 重み | MediaPipe + MobileOne | OpenFace 3.0 | GazeFollower | L2CS-Net (ResNet-50) |
|--------|------|----------------------|--------------|-------------|---------------------|
| 精度（未校正） | 25% | 3/5 (~4°) | **5/5** (2.56°) | **5/5** (0.92cm) | 4/5 (3.92°) |
| CPUレイテンシ | 20% | **5/5** (<8ms) | **5/5** (real-time) | 3/5 (~15ms) | 2/5 (~30ms) |
| ライセンス | 15% | **5/5** (Apache+MIT) | 2/5 (Academic) | 4/5 (Open) | **5/5** (Apache) |
| クロスプラットフォーム | 15% | **5/5** (WebAssembly) | 2/5 (C++) | 3/5 (Python) | 4/5 (PyTorch) |
| 校正サポート | 10% | 3/5 (手動) | 2/5 (なし) | **5/5** (内蔵) | 2/5 (なし) |
| ALS適応性 | 10% | 4/5 (モジュラー) | 3/5 (固定) | 3/5 (汎用) | 3/5 (汎用) |
| コミュニティ | 5% | **5/5** (Google) | 3/5 (CMU) | 2/5 (新規) | 3/5 (限定) |
| **加重スコア** | | **4.25** | **3.65** | **3.75** | **3.30** |

**選定理由**: Apache-2.0 + MIT の許容ライセンス、WebAssembly による Electron 展開、<8ms の CPU 推論速度、モジュラー設計による個別コンポーネント更新の容易さ（レポートB4）。

### 2.2 処理フロー

```
Webカメラ映像（30fps、720p以上）
  ↓
[Stage 1] MediaPipe Face Mesh（~5ms、Apache-2.0）
  468点ランドマーク + 虹彩ランドマーク（#468-#477）
  → 顔検出・ランドマーク・虹彩の一括処理
  ↓
[Stage 2] 目領域抽出（<1ms、JavaScript）
  左目: ランドマーク #33, #133, #160, #159, #158, #144, #145, #153
  右目: ランドマーク #362, #263, #387, #386, #385, #373, #374, #380
  瞳孔中心: ランドマーク #468(左), #473(右)
  → 64×64 pixels の目パッチを正規化クロップ
  ↓
[Stage 3] 頭部姿勢推定（<1ms、PnP）
  6点（鼻先、顎先、左右目頭、左右口角）から
  Perspective-n-Point で頭部の 6DoF を推定
  → pitch, yaw, roll + 並進ベクトル
  ↓
[Stage 4] 視線推定 CNN（~3ms、ONNX Runtime）
  MobileOne s0 バックボーン + L2CS-Net dual loss
  入力: 目パッチ(64×64×2) + 頭部姿勢(3D) + 顔位置(2D)
  出力: 視線角度(pitch, yaw) + 信頼度スコア
  ↓
[Stage 5] 校正レイヤー（<1ms）
  アフィン変換 + 適応オフセット
  → 視線角度 → 画面座標
  ↓
[Stage 6] スムージング（<1ms）
  適応型カルマンフィルタ
  → サッケード中はフィルタ緩和、注視中は強化
  ↓
[Stage 7] ゾーンマッピング（<1ms）
  ヒステリシス付きゾーン境界
  → ゾーンID + 信頼度
```

**合計パイプラインレイテンシ: ~10ms**（33ms のフレームバジェットに対し十分な余裕）

### 2.3 視線推定モデル詳細

L2CS-Net の dual loss 設計（レポートB4）:

```
損失関数:
  Total Loss = L_cls + α × L_reg

  L_cls = CrossEntropy(gaze_bins_predicted, gaze_bins_truth)
    - 視線角度を N 個のビンに離散化（yaw 90ビン、pitch 90ビン）
    - 分類による粗いが頑健な推定

  L_reg = SmoothL1(gaze_angle_predicted, gaze_angle_truth)
    - 回帰による細粒度の推定

  α = 1.0（重み係数）

VoiceReach 拡張:
  + L_zone = CrossEntropy(zone_predicted, zone_truth)
    - ゾーン直接予測の補助タスク
    - ゾーン境界付近の判定精度を向上

モデル仕様:
  バックボーン: MobileOne s0（Apple MLX / ONNX Runtime）
  パラメータ数: ~3-5M
  推論速度: ~3ms（CPU）
  ONNX エクスポート: 対応（Electron/Node.js での展開可能）
```

**ベンチマーク上の位置づけ**（レポートA1, B4）:

```
                     精度（角度誤差、度）
                0      1      2      3      4      5      6
                |      |      |      |      |      |      |
Tobii IR        |=|    |      |      |      |      |      |  (0.3-0.5°)
OpenFace 3.0    |      |  ==|=|     |      |      |      |  (2.56°)
VoiceReach目標  |      |   |===|    |      |      |      |  (2-3° 校正後)
L2CS-Net        |      |      |     |=|    |      |      |  (3.92°)
Webcam未校正    |      |      |      |      |===|==|     |  (4-6°)
OpenFace 2.0    |      |      |      |      |      |     |===> (9.1°)
```

### 2.4 キャリブレーション

3段階の校正アーキテクチャ（レポートA1, B4）:

```
Phase 1: 初回校正（~30秒）
  - 画面上の5点（四隅+中央）を順に注視
  - 各点で3秒間の視線データを収集（30fps × 3s = ~90フレーム/点）
  - 音声ガイダンス付き（ALS患者の注意負荷を軽減）
  - アフィン補正パラメータを算出
  - 品質スコアが閾値未満の場合: 9点拡張校正を提案
  - 結果: ~4° → ~2.5° に改善

Phase 2: 連続適応校正（使用中、常時）
  - タップ操作と視線位置のペアを自動蓄積
  - 指数加重バッファ（直近サンプルに高い重み）
  - バックグラウンドで最終FC層を N サンプルごとに再学習
  - 忘却係数: 古いサンプルを漸減（病状進行に追従するため）
  - 結果: 使用時間に比例して精度が向上

Phase 3: FAZE meta-learning（Tier 2 以降）
  - NVIDIA FAZE（ICCV 2019 Oral）の meta-learning 手法を適用
  - 事前学習: MAML で人物非依存の初期化を獲得
  - 適応: 3-5点の校正サンプルで 1 秒以内に個人適応
  - GazeCapture で k=9 で 3.18° 達成（未校正比19%改善）
  - 結果: sub-2° を目指す

簡易再キャリブレーション（~5秒）:
  - 画面中央の1点を注視するだけ
  - オフセット補正のみ更新
  - 体位変換後や装置移動後に実行
```

### 2.5 精度ロードマップ（Tier別）

| Tier | 時期 | 技術構成 | 精度 | レイテンシ | ゾーン信頼度 |
|------|------|---------|------|-----------|------------|
| **Tier 1 (MVP)** | リリース時 | MediaPipe + MobileOne s0 + L2CS dual loss + 5点校正 + カルマンフィルタ | 4°(未校正) → 2.5°(校正後) | <8ms | 4ゾーン >95% / 9ゾーン >85% |
| **Tier 2 (6ヶ月)** | +6ヶ月 | + FAZE meta-learning + 連続適応校正 + GAN低照度補正 + 適応ゾーン数 | 2-3°(継続校正) | <10ms | 4ゾーン >97% / 9ゾーン >90% |
| **Tier 3 (12ヶ月)** | +12ヶ月 | + OpenFace 3.0評価 or DINOv2 light decoder + ALS固有学習データ + 眼瞼下垂適応 + 近赤外LED夜間モード | sub-2°(最適条件) | <15ms | 9ゾーン >95% / 16ゾーン >80% |

**根拠**: MPIIGaze ベンチマークで OpenFace 3.0 が 2.56°、GazeFollower が校正後 0.92cm（~1.5-2° @ 60cm）を達成。Webcam + 校正 + 転移学習で 2.08° の報告もある（Oxford, 2025）。ALS 患者は頭部が比較的静止しているため校正精度が維持しやすい（レポートB4）。

---

## 3. 入力デバイス

### 3.1 Input Abstraction Layer（IAL）

VoiceReach は **Input Abstraction Layer（IAL）** を導入し、入力デバイスの追加・切り替えを透過的に行う（レポートA6, B7）。

```
+-----------------------------------------------------+
|           Input Abstraction Layer (IAL)               |
|                                                       |
|  統一イベントインターフェース:                          |
|    - SELECT(target_id, confidence)                    |
|    - CONFIRM()                                        |
|    - CANCEL()                                         |
|    - EMERGENCY()                                      |
|    - SCROLL(direction)                                |
|                                                       |
|  各入力ソースのアダプター:                              |
|   ┌──────┐ ┌──────┐ ┌────────┐ ┌─────┐ ┌─────┐      |
|   │ 視線 │ │ 指   │ │まばたき│ │ EMG │ │ BCI │      |
|   │      │ │ タップ│ │        │ │     │ │(EEG)│      |
|   └──┬───┘ └──┬───┘ └───┬────┘ └──┬──┘ └──┬──┘      |
|      +--------+--------+---------+---------+         |
|                    |                                  |
|            設定による切替/併用                          |
+-----------------------------------------------------+
```

**IAL 設計原則**:
1. **統一イベント**: すべての入力ソースは SELECT, CONFIRM 等の共通イベント型に変換
2. **ホットスワップ**: 入力ソースの切替は再起動不要
3. **併用モード**: 複数入力の信頼度スコアを重み付け合成
4. **進行度連動**: パーソナルベースラインの変化に応じて自動的に重み付けを調整
5. **プラグイン方式**: 新しいデバイスはアダプター追加のみで統合可能

### 3.2 指センサー: デュアルセンサ構成

調査レポート A6 に基づき、PZT 圧電ディスク + FSR-402 のデュアルセンサ構成を採用する。PPS Switch 2025（日本市場のゴールドスタンダード、¥46,970）のデュアルセンサ哲学（圧電 + 空圧）を踏襲しつつ、汎用部品で実現する。

#### ハードウェア構成（BOM: ~$41）

| コンポーネント | 製品 | 役割 | 価格 |
|--------------|------|------|------|
| MCU | Seeed XIAO RP2040 | USB HID + 12bit ADC + I2C | $6 |
| 1次センサ | 27mm PZT 圧電ディスク | タップ検出（動的圧力変化） | $1 |
| 2次センサ | Interlink FSR-402 | ロングプレス検出（静的圧力、0.1N-10N） | $8 |
| 精密ADC | ADS1115 16bit（Stage 3用） | 微弱信号検出（分解能 ~8μV） | $15 |
| 筐体 | 3Dプリント | センサ + MCU 収納 | $5 |
| ケーブル | USB-C | 電源 + データ | $3 |
| 医療用テープ | 3M マイクロポア | センサ固定 | $3 |

**日本の福祉補助金適合**: BOM $41（約¥6,000）は圧電素子式入力装置の補助基準額 ¥53,400 の範囲内（レポートA6）。

#### センサ信号フロー

```
[PZT圧電ディスク]  →  ADC Ch1  →  タップ検出（シングル/ダブル/トリプル）
  出力: ~±90V（強打時）、1MΩ負荷抵抗で安全範囲に
  特性: 動的変化のみ検出、静的圧力不可（タップ検出に最適）

[FSR-402]           →  ADC Ch2  →  ロングプレス検出（持続圧力）
  感度: 0.1N（~10g）から検出可能
  特性: 静的圧力を検出可能

両信号 → MCU → 4層フィルタリング → ジェスチャー分類 → USB HID
```

### 3.3 4層フィルタリング

ALS 患者のファシキュレーション（筋線維束攣縮）を除去するための多層フィルタリング戦略（レポートA6）。

```
Layer 1: ハードウェア RC フィルタ
  10KΩ + 100nF = 160Hz カットオフ LPF
  意図的タップの周波数成分（<100Hz）を通過、高周波ノイズを除去
  計算コスト: ゼロ

Layer 2: 時間的デバウンス
  最小活性化時間: 30-50ms（Stage 1-2）/ 20ms（Stage 3）
  最大活性化時間: 2秒（超過は不随意活動と判定）
  不応期: 100-200ms（確定タップ後）
  二重閾値: 振幅閾値 AND 持続時間閾値の両方を充足

Layer 3: 波形テンプレートマッチング（不随意運動除去）
  初回校正時に20回の意図的タップから波形テンプレートを構築
  入力信号との相互相関 > 0.6 で受理

  | 特徴 | 意図的タップ | ファシキュレーション |
  |------|------------|-------------------|
  | 立ち上がり時間 | 速い (<20ms) | 緩やか (>50ms) |
  | ピーク形状 | 鋭い単一ピーク | 不規則、多峰性 |
  | 持続時間 | 50-300ms | 不定、持続的 |
  | 反復パターン | リズミカル（マルチタップ時） | ランダム |

Layer 4: コンテキスト認識ゲーティング（VoiceReach固有の優位性）
  - 視線が UI 要素上にない → 閾値を50%引き上げ
  - 候補が表示されていない → ダブル確認を要求
  - 最終操作から30分超 → 覚醒モード（3タップで起動）
  - ジェスチャー信頼度を視線-UI整合スコアで重み付け
```

### 3.4 操作体系

| 操作 | 検出条件 | 機能 |
|------|---------|------|
| シングルタップ | 閾値超えの圧力変化（50ms〜300ms） | 選択確定 |
| ダブルタップ | 500ms 以内に2回のタップ | 戻る/キャンセル |
| ロングプレス | 1秒以上の持続圧力（FSR-402 で検出） | コンテキストメニュー/バリエーション展開 |
| トリプルタップ | 1秒以内に3回のタップ | 緊急コール |

### 3.5 進行度適応

ALSFRS-R（Revised ALS Functional Rating Scale）の Handwriting サブスケールに対応したステージ別プロファイル（レポートA6）:

```
Stage 1（ALSFRS-R 4-3: 筋力あり）:
  閾値: baseline × 1.0
  デバウンス: 100ms
  不随意運動フィルタ: LOW
  コンテキスト重み: 0.3
  装着位置: 指先（人差し指 or 親指）
  ADC: 内蔵12bit

Stage 2（ALSFRS-R 2: 筋力低下）:
  閾値: baseline × 0.5
  デバウンス: 150ms
  不随意運動フィルタ: MEDIUM
  コンテキスト重み: 0.5
  装着位置: 親指パッド → 手の甲 → 前腕

Stage 3（ALSFRS-R 1: 微小な動きのみ）:
  閾値: baseline × 0.2
  デバウンス: 200ms
  不随意運動フィルタ: HIGH
  コンテキスト重み: 0.7
  装着位置: 残存筋力のある任意の部位（前腕、頬、こめかみ）
  ADC: ADS1115 16bit 有効化（分解能 ~8μV）

Stage 4（ALSFRS-R 0: 指が動かない）:
  → まばたき入力にフォールバック（IAL による自動切替）
  → ピエゾは残存随意運動のモニタリングを継続
  → 随意運動の復帰を検出した場合はアラート
```

**自動強度モニタリング**: タップ振幅の週次平均を追跡し、下降トレンド（4週間の線形回帰傾斜 < -0.1）を検出した場合、介護者にセンサ再配置または閾値調整を通知（レポートA6）。

---

## 4. フォールバック入力階層

IAL を通じた段階的フォールバック戦略（レポートB7）:

```
Stage 1-2（筋力あり）:
  [主入力] 視線 + 指タップ（現行 VoiceReach 設計）
    |
Stage 2-3（筋力低下）:
  [第1フォールバック] 視線 + EMG 微弱信号検出
  根拠: 指が動かなくても「動かそうとする意図」の EMG 信号は残存
  デバイス: OYMotion gForcePro+（$90-150）等
    |
Stage 3-4（四肢の動きなし、眼球運動あり）:
  [第2フォールバック] 視線のみ + まばたきパターン入力
  根拠: 追加ハードウェア不要（MediaPipe で EAR 計算可能、精度 98.03%）
  操作: シングルまばたき=確定、ダブル=キャンセル、ロング=メニュー
    |
Stage 4（眼球運動の低下）:
  [第3フォールバック] 非侵襲 BCI（EEG-SSVEP/P300 + LLM）
  根拠: ChatBCI（2025, Nature Scientific Reports）が LLM 統合で
        キーストローク 53.22% 削減、ITR 229.48% 向上を実証
  デバイス: Emotiv Insight（$499）or g.tec Unicorn（~$1,200）
    |
Stage 5（完全閉じ込め状態）:
  [第4フォールバック] fNIRS-BCI or 侵襲型 BCI
  根拠: fNIRS-BCI の ALS 患者精度 81.3%（P300 の 74.0% を上回る）
        Synchron Stentrode: 低侵襲、ALS 患者の iPad 操作を実証（2025）
```

### 4.1 まばたき入力の詳細（Tier 1 フォールバック）

VoiceReach は既に MediaPipe Face Mesh を使用しているため、カメラベースまばたき検出は**追加ハードウェアなし**で統合可能（レポートB7）。

```
検出方式: Eye Aspect Ratio (EAR)
  - MediaPipe の目ランドマーク (#33, #133 等) から縦横比を計算
  - まばたき時に EAR 値が急激に低下
  - 3D ランドマーク方式で頭部傾斜への耐性を確保
  - Eyeblink 8 データセットで精度 98.03%

分類:
  - 意図的まばたき（長め・強め） vs 自然まばたき（短く軽い）
  - まばたきパターン入力（LLM 候補選択との組み合わせ）
  - モールス入力は学習コストが高い（デコード精度 62%、応答時間 18-20s）ため非推奨
```

### 4.2 EMG 入力の展望（Tier 2 フォールバック）

- **初期〜中期（発症後1-3年）**: 表面 EMG は有効。残存筋力で十分な信号振幅（>50μV）
- **中期〜後期（3-5年）**: 信号振幅が著しく低下（<10μV）。高感度アンプと適応フィルタが必要
- **Meta Neural Band**（2025年9月発売、$799）: 16ch sEMG、個人キャリブレーション不要のユニバーサルデコードを実現（200,000人データで学習）（レポートB7）

### 4.3 BCI 入力の展望（Tier 3 フォールバック）

| パラダイム | 精度 | 入力速度 | VoiceReach との親和性 |
|-----------|------|---------|---------------------|
| P300 | 74.0% ±8.9%（ALS） | 0.29 bps | ChatBCI で LLM 統合済み |
| SSVEP | 99.2% | 1.44 bps | 視覚刺激必要 |
| Motor Imagery | 81.68% | 低い | 視覚不要だが精度課題 |
| P300-SSVEP ハイブリッド | 92.72% | 28 bits/min | 最有望 |

---

## 5. UI レイアウト設計

### 5.1 フローティングバブル UI

```
従来方式: 画面四隅に固定配置 → 目が大きく動く

          [左上]──────────[右上]
            │                │
            │    ★目         │
            │                │
          [左下]──────────[右下]
          眼球移動量: 大（画面の対角線距離）

提案方式: 視線中心に候補が集まる → 目はほぼ動かない

                [候補B]
          [候補A]  ★  [候補C]
                [候補D]
          眼球移動量: 極小（半径2〜3°）
```

### 5.2 方向ジェスチャーモード

候補が4つの場合、上下左右の方向にマッピングする。

```
        [上: 候補1]
          ↑
[左: 候補4] ← ★ → [右: 候補2]
          ↓
        [下: 候補3]

操作: その方向をチラ見（0.3秒以上の滞留）+ タップで確定
```

### 5.3 文字入力 UI（フォールバック）

自由文字入力が必要な場合の2ストローク方式。

```
[第1ストローク] 行選択（上下左右 + 中央 = 5ゾーン × 2画面）

  画面1:                    画面2（右チラ見で遷移）:
  ┌────┬────┐              ┌────┬────┐
  │ あ行 │ か行 │            │ な行 │ は行 │
  ├────┼────┤              ├────┼────┤
  │ さ行 │ た行 │            │ ま行 │ や行 │
  └────┴────┘              ├────┼────┤
                            │ ら行 │ わ行 │
                            └────┴────┘

[第2ストローク] 段選択

  ┌──┬──┬──┐
  │あ │い │う │
  ├──┼──┼──┤
  │え │お │   │
  └──┴──┴──┘

→ 2操作で1文字確定
→ 確定直後にLLM予測が単語/文候補を表示
→ 候補が合えば1タップで文全体が完成
```

### 5.4 画面レイアウトのサイズ設計

```
推奨画面サイズ: 15インチ以上
視距離: 50〜70cm（設置要件: 患者の顔から40〜80cm）
最小ターゲットサイズ: 視角2°以上

  視角2° @ 60cm = 約2.1cm = 約80px（フルHD）

ゾーン分割と精度マージン:
  4分割: 各ゾーン ~480×270px (12°×7°) → 2.5°校正で余裕あり
  9分割: 各ゾーン ~320×180px (8°×4.5°) → 2.5°校正で実用範囲内
 16分割: 各ゾーン ~240×135px (6°×3.4°) → sub-2°必要、Tier 3以降
```

---

## 6. 操作フローの詳細

### 6.1 標準的な会話フロー

```
1. 介護者が話しかける
   → マイクが音声認識
   → コンテキスト統合エンジンがイベントを検出
   → 三段ハイブリッドLLM推論が返答候補4つを生成
     Stage 1 (~150ms): Qwen3-0.6B → 暫定候補即表示
     Stage 2 (~350ms): Qwen3-1.7B → 改善候補に差し替え
     Stage 3 (~800ms): Gemini 2.5 Flash → 高品質候補に差し替え

2. 候補がフローティングバブルで表示される
   患者は候補を自然に見る（明示的な操作不要）

3a. 候補が合っている場合:
    → その候補の方向にチラ見 + タップ → 確定
    → 音声合成で発話（CosyVoice 2/3、ストリーミング150ms）

3b. 候補が惜しい場合:
    → その候補をチラ見 + ロングプレス → バリエーション展開
    → 展開された候補からタップで選択

3c. 全部違う場合:
    → 何も選ばずに待つ（1.5秒）
    → システムが視線データ + 感情データを分析
    → 2巡目の候補を別の意図軸で生成

3d. 自分で入力したい場合:
    → ダブルタップ → 自由入力モードに遷移
```

### 6.2 緊急時フロー

```
トリプルタップ → 緊急モード起動
  ┌─────────────────────────────┐
  │        緊急通報              │
  │                             │
  │  [呼吸困難]    [激しい痛み]   │
  │  [吸引必要]    [人を呼んで]   │
  │                             │
  │  1タップで介護者に通知送信    │
  └─────────────────────────────┘
```

---

## 7. 夜間モード

### 7.1 課題

可視光カメラは暗所で性能が大幅に劣化する。照明をつけると患者の睡眠を妨げる。GAN ベースの画像強調（"Gaze in the Dark"、MDPI Sensors 2020）は暗所で 4.5-8.9% の精度改善を報告しているが、根本的な解決にはならない（レポートA1）。

### 7.2 解決策

```
夜間モード（自動切替 or 手動切替）:

  視覚入力: 低照度対応
    - 画面の明るさを最小限に（ダークモード、赤色ベース）
    - 画面自体の光でカメラが患者の顔を捕捉
    - ゾーン数を4以下に縮小（精度低下に対応）

  代替入力の優先度UP:
    - 指タップのみで定型文を選択可能に（視線不要）
    - まばたきパターン入力を補助的に有効化
    - 音声入力（微小な発声）の感度UP

  近赤外LED補助（Tier 3 オプション）:
    - 人体に無害な近赤外LED（850nm）を低出力で点灯（~$5）
    - 患者には見えないが、カメラは瞳孔を検出可能
    - 精度: 画面光のみ（4-6° → 4ゾーン >80%）→ 近赤外追加（2.5-4° → 4ゾーン >90%）

  GAN 低照度画像強調（Tier 2）:
    - ソフトウェアのみの解決策
    - 暗所ベースライン比 4.5-8.9% の精度改善
```

---

## 8. デプロイメントアーキテクチャ

docs/01_SYSTEM_ARCHITECTURE.md との整合性に基づく（レポートB4, B6）。

```
Electron アプリケーション（macOS / Mac mini M4 Pro）
  |
  +-- MediaPipe Face Mesh (WASM)
  |     入力: Webカメラフレーム（720p）
  |     出力: 468ランドマーク + 虹彩位置
  |     レイテンシ: ~5ms
  |     ライセンス: Apache-2.0
  |
  +-- 目パッチ抽出 (JavaScript)
  |     入力: ランドマーク
  |     出力: 64×64 左目、64×64 右目、頭部姿勢(3DoF)
  |     レイテンシ: <1ms
  |
  +-- 視線推定 CNN (ONNX Runtime for Node.js)
  |     入力: 目パッチ + 頭部姿勢
  |     モデル: MobileOne s0 + L2CS dual loss
  |     出力: 視線角度(pitch, yaw) + 信頼度
  |     レイテンシ: ~3ms
  |     メモリ: ~0.5GB
  |
  +-- 校正レイヤー (JavaScript)
  |     入力: 生の視線角度
  |     出力: 校正済み画面座標
  |     方式: アフィン変換 + 適応オフセット
  |     レイテンシ: <1ms
  |
  +-- スムージング (JavaScript)
  |     入力: 校正済み座標ストリーム
  |     出力: 平滑化座標
  |     方式: 適応型カルマンフィルタ（サッケード/注視判定付き）
  |     レイテンシ: <1ms
  |
  +-- ゾーンマッパー (JavaScript)
  |     入力: 平滑化座標
  |     出力: ゾーンID + 信頼度
  |     方式: ヒステリシス付き境界
  |     レイテンシ: <1ms
  |
  +-- まばたき検出 (JavaScript、Stage 3-4 フォールバック)
  |     入力: ランドマーク（MediaPipe 共有）
  |     出力: まばたきイベント（意図的/自然の分類）
  |     方式: EAR + ピーク検出
  |     レイテンシ: <1ms
  |
  +-- Input Abstraction Layer (JavaScript)
        入力: ゾーンID + 指タップ/まばたき/EMG イベント
        出力: 統一入力イベント（SELECT, CONFIRM 等）
        レイテンシ: <1ms

合計パイプラインレイテンシ: ~10ms（33ms フレームバジェットの 30%）
GPU使用率: ~10%（常時）
メモリ: ~0.5GB
```

---

## 9. ALS 固有の配慮事項

調査レポート A1 の ALS 専門文献に基づく（Middlesex University Review 2023、Hwang et al. 2014 等）。

### 9.1 眼瞼下垂（Ptosis）への対応

ALS 進行に伴い眼瞼が下垂し、瞳孔が部分的に隠れることがある。

- **Tier 1**: 目検出 ROI を拡大し、部分的に隠れた瞳孔からの推定に対応
- **Tier 2**: 合成データ拡張（目領域上部をランダムに遮蔽して学習）
- **Tier 3**: ALS 患者固有の学習データ収集（10-20名）によるファインチューニング

### 9.2 ドライアイへの対応

角膜反射の変化を検出し、介護者に点眼の推奨を通知する。

### 9.3 ベッド上の頭部姿勢

ALS 患者はベッドで30-45度の角度で横臥することが多い。標準的なデータセット（MPIIGaze 等）は直立姿勢を前提としている。

- **対策**: ETH-XGaze（1M+画像、極端な頭部姿勢を含む）でのプレトレーニングにより耐性を確保
- **利点**: 頭部が比較的静止しているため、校正の安定性が高い（レポートB4）

### 9.4 校正プロトコル

```
ALS 患者固有の配慮:
  - 校正ポイント滞留時間: 3秒（標準の2秒より長め）
  - ターゲットサイズ: 視角5°以上（大きく見やすく）
  - 音声フィードバック: 各ポイント完了時に音で通知
  - 体位変更検出: 自動的に簡易再校正をトリガー
  - 精度劣化検出: ゾーン数を自動縮小（16→9→4）
```

---

## 10. 学習データ戦略

レポートB4 に基づく（レポートA1のデータセットカタログも参照）。

```
プレトレーニング（大規模、人物非依存）:
  - ETH-XGaze: 1M+画像、110名、極端な頭部姿勢
  - GazeCapture: 2.5M フレーム、1,474名、モバイル/タブレット
  - MPIIGaze: 213K 画像、15名、ラップトップ使用
  - Gaze360: 172K 画像、238名、非制約環境

ファインチューニング（個人固有）:
  - 5-9点の校正画像（ユーザーの Webカメラで取得）
  - FAZE meta-learning で <1秒で適応
  - 使用中のタップ-視線ペアによる継続学習

データ拡張:
  - 明るさ/コントラストのランダム変動（照明変化シミュレーション）
  - ランダムクロップ/スケール（頭部移動シミュレーション）
  - ガウシアンノイズ（低品質 Webカメラシミュレーション）
  - 合成眼瞼下垂（目領域上部の遮蔽）
  - 左右反転（左右目の対称性）
```

---

## 11. 参考文献

### 調査レポート（本ドキュメントの根拠）

- **レポートA1**: `research/phase_a/a1_eye_gaze_tracking/report.md` -- 視線推定の先行研究・既存手法
- **レポートB4**: `research/phase_b/b4_vision_gaze_models/report.md` -- 視線推定モデルの最新動向
- **レポートA6**: `research/phase_a/a6_finger_pressure_input/report.md` -- 指入力・圧力センサ・IAL
- **レポートB7**: `research/phase_b/b7_input_devices/report.md` -- BCI/EMG/まばたき等の将来入力

### 主要論文

- Abdelrahman et al. "L2CS-Net: Fine-Grained Gaze Estimation in Unconstrained Environments," FG 2023. (3.92° on MPIIGaze)
- Park et al. "Few-Shot Adaptive Gaze Estimation (FAZE)," ICCV 2019 Oral. (meta-learning, 3.18° with k=9)
- Baltrušaitis et al. "OpenFace 3.0," arXiv 2025. (2.56° on MPIIGaze, multi-task)
- Zhu et al. "GazeFollower," ACM PACMCGIT 2024. (0.92cm after calibration)
- Battistini et al. "LeMMS Switch for ALS," J NeuroEngineering Rehab 2019. (20名ALS患者、5.45時間/日使用)
- "ChatBCI: P300 speller with LLM," Nature Scientific Reports 2025. (ITR 229.48% 向上)
- Soukupova & Cech. "Real-Time Eye Blink Detection using Facial Landmarks," CVWW 2016. (EAR 方式)
- Cai et al. "Gaze in the Dark," Sensors (MDPI) 2020. (GAN 低照度補正、4.5-8.9% 改善)

### 主要 OSS

- MediaPipe Face Mesh: https://github.com/google-ai-edge/mediapipe (Apache-2.0)
- L2CS-Net: https://github.com/Ahmednull/L2CS-Net (Apache-2.0)
- MobileGaze: https://github.com/yakhyo/gaze-estimation (MIT, MobileOne s0 実装)
- FAZE: https://github.com/NVlabs/few_shot_gaze
- OpenFace 3.0: https://github.com/CMU-MultiComp-Lab/OpenFace-3.0 (Academic license)
- GazeFollower: https://github.com/GanchengZhu/GazeFollower
