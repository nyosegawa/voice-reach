# 02 — 視線推定・入力インターフェース設計

## 1. 設計思想

### 1.1 脱 Dwell Click

従来の視線入力では「一定時間じっと見つめる」ことでクリックを行う Dwell Click が標準である。しかしこの方式には根本的な問題がある。

- **眼精疲労**: 視線を固定し続ける不自然な眼球運動が疲労を蓄積する
- **誤操作**: 画面を何気なく見ただけで意図しない操作が発生する
- **速度限界**: 1クリックに2〜3秒を要し、入力速度に上限がある
- **休めない**: 画面を見ている限り常に「入力状態」であり、視覚的な休憩ができない

VoiceReachでは「目で見る＝入力確定」という等式を切り離す。目はポインティング（どこに関心があるか）のみを担当し、確定は指タップで行う。

### 1.2 ゾーンベース設計

ピクセル精度の視線追従ではなく、画面をゾーンに分割して「どのゾーンを見ているか」を判定する。

```
Webカメラの精度: 2〜5°（画面上で3〜6cmの誤差）
フルHD画面を4×4分割: 各ゾーン約8°×5°

→ 誤差2〜5°でもゾーン判定は十分な精度で可能
```

これにより、近赤外線専用デバイスなしでも実用的な入力が実現する。

---

## 2. 視線推定パイプライン

### 2.1 処理フロー

```
Webカメラ映像（30fps、720p+）
  ↓
[Stage 1] 顔検出
  MediaPipe Face Detection → 顔バウンディングボックス
  ↓
[Stage 2] 顔ランドマーク
  MediaPipe Face Mesh → 468点ランドマーク座標
  ↓
[Stage 3] 目領域抽出
  左目: ランドマーク #33, #133, #160, #159, #158, #144, #145, #153
  右目: ランドマーク #362, #263, #387, #386, #385, #373, #374, #380
  瞳孔中心: ランドマーク #468(左), #473(右)（精度はやや低い）
  ↓
[Stage 4] 頭部姿勢推定
  6点（鼻先、顎先、左右目頭、左右口角）から
  PnP（Perspective-n-Point）で頭部の回転/並進を推定
  ↓
[Stage 5] 視線推定
  方式A: 幾何学的推定
    瞳孔中心位置 + 頭部姿勢 → 視線ベクトル → 画面交点
  方式B: 外観ベース推定（精度が高い）
    目領域画像 + 頭部姿勢 → 軽量CNN → 画面座標直接予測
  ↓
[Stage 6] スムージング
  カルマンフィルタまたは指数移動平均
  → ノイズ除去、視線ジャンプの抑制
  ↓
[Stage 7] ゾーンマッピング
  画面座標 → ゾーンID
  ゾーン境界にヒステリシスを設けて境界でのチラつきを防止
```

### 2.2 外観ベース視線推定CNN

```
入力:
  - 左目パッチ（64×64 pixels）
  - 右目パッチ（64×64 pixels）
  - 頭部姿勢ベクトル（pitch, yaw, roll）
  - 顔位置（正規化座標）

ネットワーク:
  [目パッチ] → 3層CNN → 特徴ベクトル（128次元）
  [頭部姿勢+顔位置] → FC層 → 特徴ベクトル（32次元）
  → 結合 → FC層 → 画面座標(x, y)予測

出力:
  - 画面座標（正規化: 0.0〜1.0）
  - 信頼度スコア（目が検出できない場合は低下）

パラメータ数: ~500K（モバイルでも動作可能なサイズ）
推論速度: ~5ms/frame（CPU）
```

### 2.3 キャリブレーション

```
初回キャリブレーション（~30秒）:
  画面上の5点（四隅+中央）を順に見てもらう
  各点で2秒間の視線データを収集
  → ユーザー固有の補正パラメータを算出

簡易再キャリブレーション（~5秒）:
  画面中央の1点を見るだけ
  → オフセット補正のみ更新
  体位変換後や装置移動後に実行

連続適応キャリブレーション:
  候補選択時のタップ操作と視線位置のペアを蓄積
  → バックグラウンドでキャリブレーションを微調整
  → 明示的な再キャリブレーション頻度を削減
```

---

## 3. 入力デバイス：指センサー

### 3.1 ピエゾ素子の動作原理

```
圧力/歪み → ピエゾ素子 → 電圧変化 → ADC → デジタル値
  ↓
閾値判定（ソフトウェア）
  ↓
イベント生成: TAP / DOUBLE_TAP / LONG_PRESS
```

### 3.2 操作体系

| 操作 | 検出条件 | 機能 |
|---|---|---|
| シングルタップ | 閾値超えの圧力変化（50ms〜300ms） | 選択確定 |
| ダブルタップ | 500ms以内に2回のタップ | 戻る/キャンセル |
| ロングプレス | 1秒以上の持続圧力 | コンテキストメニュー/バリエーション展開 |
| トリプルタップ | 1秒以内に3回のタップ | 緊急コール |

### 3.3 進行度適応

```
Stage 1（筋力あり）:
  閾値: 中程度
  デバウンス: 100ms
  装着位置: 指先

Stage 2（筋力低下）:
  閾値: 低（感度UP）
  デバウンス: 150ms（不随意運動フィルタ強化）
  装着位置: 手の甲、前腕

Stage 3（微小な動きのみ）:
  閾値: 最小
  デバウンス: 200ms
  不随意運動フィルタ: 圧力変化のパターン認識で意図的操作を判別
  装着位置: 残存筋力のある任意の部位

Stage 4（指が動かない）:
  → まばたき入力 or 視線ジェスチャーにフォールバック
```

### 3.4 不随意運動フィルタ

ALS患者にはファシキュレーション（筋線維束攣縮）が生じることがある。

```
意図的タップの特徴:
  - 明確な立ち上がりと立ち下がり
  - 一定のパターン（リズム）
  - 直前に視線がUI上にある

不随意運動の特徴:
  - 不規則なタイミング
  - 小さく持続的な振動
  - 視線がUI外にある場合が多い

フィルタ戦略:
  1. 視線がUI上にない場合のタップを無視（設定で切替可）
  2. 圧力変化の波形パターンで分類（短い学習期間で個人適応）
  3. 確定前のプレビュー表示（1秒のキャンセル猶予）
```

---

## 4. UIレイアウト設計

### 4.1 フローティングバブルUI

```
従来方式: 画面四隅に固定配置 → 目が大きく動く

          [左上]──────────[右上]
            │                │
            │    ★目         │
            │                │
          [左下]──────────[右下]
          眼球移動量: 大（画面の対角線距離）

提案方式: 視線中心に候補が集まる → 目はほぼ動かない

                [候補B]
          [候補A]  ★  [候補C]
                [候補D]
          眼球移動量: 極小（半径2〜3°）
```

### 4.2 方向ジェスチャーモード

候補が4つの場合、上下左右の方向にマッピングする。

```
        [上: 候補1]
          ↑
[左: 候補4] ← ★ → [右: 候補2]
          ↓
        [下: 候補3]

操作: その方向をチラ見（0.3秒以上の滞留）+ タップで確定
```

### 4.3 文字入力UI（フォールバック）

自由文字入力が必要な場合の2ストローク方式。

```
[第1ストローク] 行選択（上下左右 + 中央 = 5ゾーン × 2画面）

  画面1:                    画面2（右チラ見で遷移）:
  ┌────┬────┐              ┌────┬────┐
  │ あ行 │ か行 │            │ な行 │ は行 │
  ├────┼────┤              ├────┼────┤
  │ さ行 │ た行 │            │ ま行 │ や行 │
  └────┴────┘              ├────┼────┤
                            │ ら行 │ わ行 │
                            └────┴────┘

[第2ストローク] 段選択

  ┌──┬──┬──┐
  │あ │い │う │
  ├──┼──┼──┤
  │え │お │   │
  └──┴──┴──┘

→ 2操作で1文字確定
→ 確定直後にLLM予測が単語/文候補を表示
→ 候補が合えば1タップで文全体が完成
```

### 4.4 画面レイアウトのサイズ設計

```
推奨画面サイズ: 15インチ以上
視距離: 50〜70cm
最小ターゲットサイズ: 視角2°以上

  視角2° @ 60cm = 約2.1cm = 約80px（フルHD）

ゾーン分割:
  4分割: 各ゾーン約480×270px → 視角約12°×7° → 余裕あり
  9分割: 各ゾーン約320×180px → 視角約8°×4.5° → 実用範囲内
  16分割: 各ゾーン約240×135px → 視角約6°×3.4° → 精度的に限界
```

---

## 5. 操作フローの詳細

### 5.1 標準的な会話フロー

```
1. 介護者が話しかける
   → マイクが音声認識
   → コンテキスト統合エンジンがイベントを検出
   → LLMが返答候補4つを生成（意図軸分散）

2. 候補がフローティングバブルで表示される
   患者は候補を自然に見る（明示的な操作不要）

3a. 候補が合っている場合:
    → その候補の方向にチラ見 + タップ → 確定
    → 音声合成で発話

3b. 候補が惜しい場合:
    → その候補をチラ見 + ロングプレス → バリエーション展開
    → 展開された候補からタップで選択

3c. 全部違う場合:
    → 何も選ばずに待つ（1.5秒）
    → システムが視線データ + 感情データを分析
    → 2巡目の候補を別の意図軸で生成

3d. 自分で入力したい場合:
    → ダブルタップ → 自由入力モードに遷移
```

### 5.2 緊急時フロー

```
トリプルタップ → 緊急モード起動
  ┌─────────────────────────────┐
  │        緊急通報              │
  │                             │
  │  [呼吸困難]    [激しい痛み]   │
  │  [吸引必要]    [人を呼んで]   │
  │                             │
  │  1タップで介護者に通知送信    │
  └─────────────────────────────┘
```

---

## 6. 夜間モード

### 6.1 課題

可視光カメラは暗所で性能が大幅に劣化する。照明をつけると患者の睡眠を妨げる。

### 6.2 解決策

```
夜間モード（自動切替 or 手動切替）:

  視覚入力: 低照度対応
    - 画面の明るさを最小限に（ダークモード、赤色ベース）
    - 画面自体の光でカメラが患者の顔を捕捉
    - ゾーン数を4以下に縮小（精度低下に対応）

  代替入力の優先度UP:
    - 指タップのみで定型文を選択可能に（視線不要）
    - まばたきパターン入力を補助的に有効化
    - 音声入力（微小な発声）の感度UP

  近赤外LED補助（オプション）:
    - 人体に無害な近赤外LED（850nm）を低出力で点灯
    - 患者には見えないが、カメラは瞳孔を検出可能
    - 設計方針「カメラオンリー」からは外れるが、
      安価なLED1個の追加で夜間の安全性が大幅向上
```
