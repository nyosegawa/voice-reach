# 04 --- AI候補生成エンジン: 三段ハイブリッドLLM推論と意図空間ナビゲーション

## 1. 候補生成の設計思想

### 1.1 核心課題: 候補の多様性

従来の予測変換が苛立たせるのは、「違う」を押しても似たような候補が出続けることである。LLMの候補生成も、そのまま使えば同じ問題を起こす。

```
悪い例（同じ意図軸の変奏）:
  1. "すごいね！"
  2. "すごいじゃん！"
  3. "それはすごいね！"
  4. "太郎すごいなぁ"
  → 全部「称賛」。言いたいのは別の方向かもしれない。
```

VoiceReachでは候補を「表現の違い」ではなく「意図の違い」で分散させる。この設計は**intent-diverse generation**と呼び、セマンティックレベルの多様性制御をプロンプトで明示的に行う。

先行研究において、デコーディング多様性（temperature, top-p等）による表面的バリエーションと、意図レベルの多様性は本質的に異なることが確認されている。Nucleus Sampling (Holtzman et al., ICLR 2020) やDiverse Beam Search (Vijayakumar et al., AAAI 2018) は表面的多様性に有効だが、VoiceReachが必要とする「感情応答と質問と行動要求を同時に提示する」ような意味的多様性は、明示的なプロンプト制御によってのみ実現される（レポートA3 Section 4.1）。

### 1.2 先行研究との差別化

VoiceReachの独自性を、先行AAC+AIシステムとの比較から明確にする（レポートA3 Section 2.3, 6.1-6.3）:

| 機能 | SpeakFaster | SpeakEase | SocializeChat | VoiceReach |
|---|---|---|---|---|
| LLM活用 | Fine-tuned (x2) | Custom GPT | GPT-4 | 三段ハイブリッド (ICL + PVP) |
| パーソナライゼーション | なし | コンテキスト | ペルソナDB | PVP (~2000トークン) |
| 候補の多様性制御 | なし | なし | なし | **7意図軸フレームワーク** |
| コンテキスト利用 | 会話文脈 | 相手+感情 | 社会的関係 | 時刻+相手+環境+感情+履歴 |
| 非選択フィードバック | なし | なし | なし | **視線滞留+感情** |
| 推論アーキテクチャ | クラウド | クラウド | クラウド | **ローカル+クラウドハイブリッド** |

VoiceReachは「意図レベル」での候補生成を目指す点で、既存AAC研究の最も抽象度の高いポジションに位置する。SpeakFaster (Cai et al., Nature Communications 2024) は文字入力の加速に焦点を当て、SpeakEase (Xu et al., 2025) は1つの最適応答を目指すが、VoiceReachは「意図の異なる4候補を同時提示」するアプローチを取る（レポートA3 Section 6.1）。

---

## 2. 意図軸フレームワーク

### 2.1 7つの意図軸

| 意図軸 | 説明 | 例（相手が「太郎が表彰された」と報告した場合） |
|---|---|---|
| 感情応答 | 嬉しい/驚き/感動を返す | "すごいじゃん！" |
| 質問・深掘り | もっと知りたい | "何で表彰されたの？" |
| 自分語り | 関連する自分の経験 | "俺も小学校の時なぁ" |
| 相手への言及 | 相手の気持ちや状況に触れる | "花子さんも嬉しいでしょ" |
| 行動要求 | 何かしてほしい/したい | "太郎に電話したいな" |
| ユーモア | 冗談や軽い返し | "DNAだね、俺に似たんだ" |
| 話題転換 | 別の話をしたい | "そういえば阪神どうなった？" |

### 2.2 初回候補の生成ルール

初回の4候補は、異なる意図軸から1つずつ選ぶ。

```
LLMへの指示:
  - 4つの候補はそれぞれ異なる発話意図を持つこと
  - 使用する意図軸を[タグ]で明示すること
  - 状況からの推定確率が高い意図軸を優先すること
  - ただし、低確率でも重要な軸（行動要求、緊急）は含めること
```

意図軸の選択は状況依存で変わる。

```
相手が質問してきた場合:
  → 回答系の候補を2つ + 別軸2つ

雑談の流れの場合:
  → 感情応答 + 質問 + 自分語り + ユーモア

介護者が処置に来た場合:
  → 行動要求 + 感謝 + 体調報告 + 環境要望
```

### 2.3 intent-diverse generationの実現手法

先行研究が示すように、LLMの会話履歴（Memory）が多いほど候補の多様性が低下するリスクがある (Chu et al., EMNLP 2025 Findings)。VoiceReachでは以下のアプローチを組み合わせる（レポートA3 Section 4.2）:

#### 手法1: 明示的指示による分散（プライマリ）

最もシンプルかつ効果的。プロンプトで各候補に異なる意図タグの割り当てを要求する。

```
各候補は異なる発話意図を持つこと:
- 候補1: [感情応答]
- 候補2: [質問]
- 候補3: [自分語り]
- 候補4: [行動要求]
```

#### 手法2: コンテキスト量と多様性のバランス制御

Adaptive Prompt Pruning (Chu et al., EMNLP 2025) の知見を応用:

- 1巡目（多様性重視）: コンテキストを適度に間引く
- 2巡目以降（命中率重視）: コンテキストを詳細に含める

#### 手法3: 構造化出力フォーマットの強制

候補の意図軸ラベルを明示的に要求し、後処理での多様性検証を可能にする:

```json
{
  "candidates": [
    {"text": "すごいじゃん！", "intent_axis": "emotional_response", "confidence": 0.8},
    {"text": "何で表彰されたの？", "intent_axis": "question", "confidence": 0.7},
    {"text": "俺も小学校の時なぁ", "intent_axis": "self_reference", "confidence": 0.5},
    {"text": "太郎に電話したいな", "intent_axis": "action_request", "confidence": 0.4}
  ]
}
```

---

## 3. 三段ハイブリッドLLM推論

### 3.1 アーキテクチャ概要

VoiceReachの候補生成エンジンは、ローカルLLM 2段 + クラウドLLM 1段の三段構成を採る。この設計はレポートB1（ローカルLLM調査）とレポートB2（クラウドLLM調査）の分析結果に基づく。

```
トリガーイベント発生
  │
  ├─→ [Stage 1: ~150ms] Qwen3-0.6B (Q5_K_M) + vllm-mlx
  │    └→ 暫定候補4つ → UI即表示
  │    └→ KVキャッシュ: PVP+指示のプレフィックスキャッシュ（TTFT実質0ms）
  │    └→ temperature: 0.3-0.5（確実性重視）
  │
  ├─→ [Stage 2: ~350ms] Qwen3-1.7B (Q4_K_M) + vllm-mlx
  │    └→ 改善候補4つ → UIを差し替え（アニメーション付き）
  │    └→ 日本語品質B+、意図軸分散が概ね安定
  │    └→ temperature: 0.4-0.6（確実性と多様性のバランス）
  │
  └─→ [Stage 3: ~800ms] Gemini 2.5 Flash（クラウド）
       └→ 高品質候補4つ → UIを差し替え
       └→ 日本語品質A-、意図軸分散が安定
       └→ temperature: 0.7-0.9（多様性重視）
       └→ フォールバック: Claude Haiku 4.5 → GPT-4.1 nano
```

ユーザーが暫定候補をすぐに選んだ場合は後続Stage結果を破棄。選ばなかった場合は到着順にUIを更新する。

### 3.2 temperature戦略

ローカルLLMとクラウドLLMで異なるtemperature戦略を採る。この設計はモデルの特性差に基づく。

| Stage | モデル | temperature | top_p | 設計意図 |
|---|---|---|---|---|
| Stage 1 | Qwen3-0.6B | 0.3-0.5 | 0.85 | 小型モデルは高temperatureで品質が崩れやすい。確実に自然な候補を出す |
| Stage 2 | Qwen3-1.7B | 0.4-0.6 | 0.90 | 品質と多様性のバランス。意図軸分散の安定性を確保 |
| Stage 3 | Gemini 2.5 Flash | 0.7-0.9 | 0.95 | 高性能モデルは高temperatureでも品質を維持。多様で意外性のある候補を生成 |

**根拠**: 小型モデル（0.6B, 1.7B）は高temperatureで不自然な語彙選択や文法崩壊が起きやすい（レポートB1 Section 6.2）。一方、クラウドの大型モデルは高temperatureでも流暢性を維持でき、意図軸間の分散がより大きくなる。この差を利用し、ローカルは「確実に方向が正しい候補」、クラウドは「意外性を含む多様な候補」という役割分担を行う。

### 3.3 各Stageの詳細仕様

#### Stage 1: Qwen3-0.6B（即座応答）

| 項目 | 仕様 |
|---|---|
| モデル | Qwen3-0.6B-Instruct |
| 量子化 | **Q5_K_M**（小型のため量子化耐性が低い。品質維持優先） |
| フレームワーク | vllm-mlx |
| メモリ使用量 | ~0.5GB |
| 推定レイテンシ | ~150ms（KVキャッシュ利用、出力80トークン時） |
| 日本語品質 | C+（基礎的）。暫定候補として最低限許容可能 |
| 指示追従 | 基礎的。意図軸の混同が起きることがある |

**選定理由**（レポートB1 Section 8.2）:
- 200ms目標を達成できる唯一のモデルサイズ
- vllm-mlxのtext prefix cachingにより**TTFT 5.8x高速化**が報告されている
- Apache 2.0ライセンスで商用利用に制約なし

**品質上の制約と対策**:
- 0.6Bモデルは4候補の同時生成が不安定なことがある → Few-shot例2-3個の追加で安定化
- 意図軸の区別が困難（混同あり）→ Constrained Decodingでフォーマットを強制
- PVP語彙パターンの模倣が限定的 → Stage 1では個人性より速度を優先し、Stage 2/3で個人性を補完

#### Stage 2: Qwen3-1.7B（改善応答）

| 項目 | 仕様 |
|---|---|
| モデル | Qwen3-1.7B-Instruct |
| 量子化 | **Q4_K_M**（バランス良好。日本語品質と速度の最適点） |
| フレームワーク | vllm-mlx |
| メモリ使用量 | ~1.2GB |
| 推定レイテンシ | ~350ms（KVキャッシュ利用、出力80トークン時） |
| 日本語品質 | B+（実用的）。PVP模倣が基礎的に可能 |
| 指示追従 | 良好。意図軸分散が概ね安定。構造化出力を概ね遵守 |

**選定理由**（レポートB1 Section 8.1）:
- Qwen3-1.7Bは「Qwen2.5-3Bと同等の性能」と公式発表されており、パラメータ効率が優秀
- 1.7Bでありながら日本語指示追従能力が良好
- 意図軸分散という複雑な指示にも概ね対応可能
- オフライン時はこのモデルの出力が最終候補となるため、実用品質が必要

#### Stage 3: Gemini 2.5 Flash（高品質応答）

| 項目 | 仕様 |
|---|---|
| モデル | Gemini 2.5 Flash |
| TTFT | ~0.28s（全クラウドLLM中最速） |
| 出力速度 | ~285 tok/s（Claude Haiku 4.5の2.5倍） |
| 推定総レイテンシ | ~800ms（ネットワーク50ms + TTFT 280ms + 生成500ms） |
| 日本語品質 | A-（良好）。口語的自然さ、PVP模倣精度ともに高い |
| 月額コスト | ~$2-3（75回/日、キャッシュ利用時） |
| コンテキスト長 | 1M トークン |

**選定理由**（レポートB2 Section 5.1）:
1. 出力速度285 tok/sは全候補中最速。150トークンの出力が約0.5秒で完了
2. 入力$0.30/出力$2.50の価格帯はClaude Haiku 4.5の約1/3
3. Context Caching APIによりPVP部分のコストを実質1/10に削減
4. ストリーミング利用時は最初の候補がわずか0.4秒で表示開始

#### フォールバック構成

```
候補生成リクエスト発生
  ↓
Gemini 2.5 Flash に送信（プライマリ、全呼び出しの80-90%）
  ├→ 成功: 候補をUIに反映
  ├→ 失敗 (429/503/timeout):
  │    ↓
  │    Claude Haiku 4.5 に送信（セカンダリ、10-20%）
  │    ├→ 成功: 候補をUIに反映
  │    └→ 失敗:
  │         ↓
  │         GPT-4.1 nano に送信（ターシャリ）
  │         ├→ 成功: 候補をUIに反映
  │         └→ 失敗: ローカルLLM（Stage 2）の候補をそのまま使用
  │
  └→ レイテンシ超過 (>2秒):
       → ローカルLLMの候補をそのまま使用（クラウド結果は破棄）
```

**品質ベースの使い分け**（レポートB2 Section 5.3）:

| モード | 使用モデル | 適用場面 |
|---|---|---|
| 標準モード | Gemini 2.5 Flash | 日常的な挨拶・返答、定型的な要望、食事の選択 |
| 高品質モード | Claude Haiku 4.5 | 2巡目以降の再生成、初対面の訪問者、医療関連の会話、感情的に重要な場面 |
| 緊急モード | GPT-4.1 nano | 上位2つが利用不可の場合の最終フォールバック |

### 3.4 量子化戦略

小型モデルは大型モデルと比較して量子化の影響を強く受ける。特に日本語の低頻度トークン（方言、口語表現、固有名詞）は量子化により生成精度が低下する傾向がある（レポートB1 Section 3.2）。

| モデル | 推奨量子化 | ファイルサイズ比 | 品質低下 | メモリ | 選定理由 |
|---|---|---|---|---|---|
| Qwen3-0.6B | **Q5_K_M** | 0.37x | ~3% | ~0.5GB | 小型のため量子化耐性が低い。Q4_K_Mでは日本語品質が顕著に劣化 |
| Qwen3-1.7B | **Q4_K_M** | 0.30x | ~5-8% | ~1.2GB | バランスが良い。速度が重要な役割のためQ4を採用 |
| Qwen3-4B（将来） | Q4_K_M | 0.30x | ~5-8% | ~2.5GB | 十分な量子化耐性あり。M4 Max等での品質向上オプション |

### 3.5 KVキャッシュ最適化

三段構成のレイテンシ目標を達成する上で最も重要な最適化がKVキャッシュである。

```
┌─────────────────────────────────────────────────────────────┐
│  KVキャッシュ構造                                            │
│                                                             │
│  プレフィックスキャッシュ（常時保持、ほぼ変化なし）            │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ PVP（~2000 tok）+ 基本タスク指示（~300 tok）         │    │
│  │ → TTFT実質0ms（キャッシュヒット時）                   │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                             │
│  セッションキャッシュ（時間単位で更新）                       │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ 環境状態 + 対話相手情報（~300 tok）                    │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                             │
│  動的部分（毎回変化、キャッシュなし）                         │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ 会話履歴 + フィードバック + 最新コンテキスト（~900 tok）│    │
│  └─────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
```

**ローカルKVキャッシュ**（レポートB1 Section 5.3）:
- vllm-mlxのtext prefix cachingにより、PVP+指示部分のKVを常時メモリに保持
- TTFT 5.8x高速化が報告されており、プロンプト処理時間を実質0msに近づける
- Qwen3-0.6Bの場合: ~30ms（通常） → ~0ms（キャッシュヒット）

**クラウドプロンプトキャッシュ**（レポートB2 Section 4.2, 8.1）:
- Anthropic: キャッシュ読み込み90%OFF（5分TTL、使用のたびにリセット）
- Google: Context Caching APIで読み込み90%OFF
- PVP ~2300トークンの読み込みコストを実質1/10に削減

```yaml
# 3層キャッシュ戦略
layer_1_static:   # ほぼ変化しない（日/週単位で更新）
  content: "PVP + 基本タスク指示"
  size: ~2300 tokens
  cache: always    # ローカル: KVキャッシュ / クラウド: プロンプトキャッシュ

layer_2_session:   # セッション中変化しにくい（時間単位）
  content: "環境状態 + 対話相手情報"
  size: ~300 tokens
  cache: session   # セッション中キャッシュ

layer_3_dynamic:   # 毎回変化する
  content: "会話履歴 + フィードバック + 最新コンテキスト"
  size: ~900 tokens
  cache: none      # キャッシュしない
```

---

## 4. プロンプト設計

### 4.1 プロンプト構造

```
[System Prompt]
├── Role定義: ALS患者の代弁者（VoiceReachアシスタント）
├── PVP（状況に応じて関連部分を選択的に挿入、800-1200トークン）
│    ├→ 対話相手が妻 → PVPのrelational_styles[wife]を優先
│    ├→ 体調に関する会話 → PVPのfrequent_topics[daily_care]を含める
│    └→ 未知の人との会話 → PVPのrelational_styles[unknown] + avoidancesを含める
├── 意図軸フレームワーク定義（7軸の説明、100トークン）
├── 出力フォーマット指示（JSON構造、50トークン）
└── 制約条件（avoidances、スタイルルール、50トークン）

[User Prompt]
├── 状況コンテキスト（時刻、環境、患者状態、200トークン）
├── 対話相手情報（名前、関係、50トークン）
├── 会話履歴（直近3-5発話、200-400トークン）
├── 前巡フィードバック（視線+感情データ、あれば100トークン）
└── 生成指示（巡数・意図軸指定、50トークン）

合計: ~1500-2100トークン（入力側）→ 候補4つ出力（~80-150トークン）
```

### 4.2 PVPの選択的挿入（Retrieval-Augmented PVP）

PVP全体（~2000トークン）を毎回含めるのではなく、状況に応じて関連部分を選択的に検索・挿入する。これはLaMP benchmark (Salemi et al., ACL 2024) のRetrieval Augmented Personalizationの知見に基づく（レポートA3 Section 3.1, 7.5）。

セマンティック検索がキーワード一致検索より効果的であることが示されており、PVPの構成要素から状況に最も関連する部分を選択するRetrieval-augmented PVP方式を採用する。

### 4.3 PVP + ICL戦略の根拠

VoiceReachがファインチューニングではなくIn-Context Learning（ICL）を採用する設計は、複数の先行研究により支持される（レポートA3 Section 3.3-3.4）。

**明示的スタイル記述 > 暗黙的事例コピー**:

"Catch Me If You Can?" (Wang et al., EMNLP 2025 Findings) は、LLMの暗黙的文体模倣能力に限界があることを示した。特にカジュアル会話（AACの主要ユースケース）での模倣が最も困難である。VoiceReachのPVPは暗黙的なfew-shot事例コピーではなく、明示的なスタイル記述（語彙・語尾・思考パターン等の構造化プロファイル）を提供する設計であり、この課題を回避する。

"The less I type, the better" (Cai et al., CHI 2023) では、AACユーザー12名が「AI生成フレーズが自分のコミュニケーションスタイルと好みを反映すること」を極めて重要と回答しており、PVPの必然性を直接裏付ける。

```
非推奨（暗黙的事例コピー）:
  以下は{患者名}さんの過去の発言例です:
  - "ありがとね"
  - "まあまあだよ"

推奨（明示的スタイル記述 = PVP）:
  {患者名}さんのスタイル:
  - 感謝表現: 「ありがとね」（家族向け）、「ありがとうございます」（フォーマル）
  - 体調報告: 深刻でも軽めに「まあまあ」「ぼちぼち」と答える傾向
  - 語尾: 疑問形が多い。断定を避ける
  - 避ける表現: 過度に弱気な表現、敬語過剰
```

### 4.4 ローカルLLM向けプロンプトの簡素化

小型モデル（0.6B, 1.7B）はシステムプロンプトの複雑さに比例して指示追従能力が低下する（レポートB1 Section 6.1-6.3）。Stage 1/2ではプロンプトを以下のように簡素化する:

| プロンプト要素 | Stage 1 (0.6B) | Stage 2 (1.7B) | Stage 3 (クラウド) |
|---|---|---|---|
| PVP | 要約版（500tok） | 関連部分選択（800tok） | 全体（1200tok） |
| 意図軸指示 | 4軸をハードコード | 動的選択 | メタプロンプト付き |
| 出力フォーマット | Few-shot例2個付き | Few-shot例1個付き | 指示のみ |
| 会話履歴 | 直前1発話のみ | 直近3発話 | 直近5発話 |
| フィードバック | 省略 | 基礎的 | 詳細 |

### 4.5 意図軸選択のためのメタプロンプト（Stage 3）

クラウドLLMでは、候補生成の前段として、状況コンテキストから最も関連する意図軸を選択するメタプロンプトを実行する:

```
Step 1（メタプロンプト）:
  状況: {context}
  7つの意図軸から、この場面で最も関連する4つを選び、
  各軸の推定使用確率を出力してください。

Step 2（候補生成プロンプト）:
  選択された4意図軸に沿って、それぞれ1つずつ候補を生成してください。
```

この2段階アプローチにより意図軸選択の合理性を検証可能にする。ただし、レイテンシ制約のため、Stage 3（クラウド）でのみ使用し、Stage 1/2では事前定義された意図軸セットを使用する。

---

## 5. 多巡ナビゲーション

### 5.1 視線滞留フィードバック

候補を表示している間の視線データから、各候補への関心度を推定する。

```
視線滞留データ:
  候補A "すごいじゃん！"       → 0.3秒（チラ見）
  候補B "何で表彰されたの？"   → 1.2秒（やや長い）★
  候補C "花子さんも嬉しいでしょ" → 0.4秒
  候補D "俺も小学校の時なぁ"   → 0.5秒

解釈:
  候補Bへの関心が最も高いが、選択しなかった
  → 「質問」軸は合っているが、この質問ではない
```

### 5.2 感情シグナルの統合

視線データに加えて感情検出データを統合し、4つの判定パターンを区別する。

| パターン | 視線 | 感情信号 | 解釈 | 次の行動 |
|---|---|---|---|---|
| ヒット | 長い + タップ | ポジティブ | 選択確定 | 発話 |
| 惜しい | 長い、タップなし | ポジティブ寄り | 方向は合っている | その候補の周辺を展開 |
| 全部違う | 全て短い | ネガティブ or 無反応 | 意図軸が外れている | 別の意図軸を探索 |
| どれも微妙 | 複数を行き来 | 困惑 | 方向は合っているが表現が違う | 抽象度/温度を変えて再提示 |

### 5.3 2巡目以降の生成ロジック

```
[2巡目生成プロンプト]

前回の候補と患者の反応:
  A. "すごいじゃん！" [感情応答] → 不採用（視線0.3秒、感情:neutral）
  B. "何で表彰されたの？" [質問] → 不採用（視線1.2秒★、感情:ややpositive）
  C. "花子さんも嬉しいでしょ" [相手言及] → 不採用（視線0.4秒、感情:neutral）
  D. "俺も小学校の時なぁ" [自分語り] → 不採用（視線0.5秒、感情:neutral）

分析:
- 「質問」軸に最も関心があるが、Bでは不十分
- 未探索の意図軸: 行動要求、ユーモア、話題転換

次の候補4つを生成してください:
- 2つは「質問」軸でBとは異なる角度
- 2つは未探索の軸から
- すべて互いに意味的に遠い候補であること
```

2巡目以降のモデル選択はClaudeHaiku 4.5を優先する。フィードバック活用の精度が命中率に直結するため、指示追従能力の高いモデルが有利である（レポートB2 Section 5.3）。

### 5.4 バリエーション展開（ロングプレス）

特定の候補を「惜しい」と判断した場合、ロングプレスでその候補を軸にバリエーションを展開する。

```
"何で表彰されたの？" を長押し → 展開

  "何の賞もらったの？"     ← 言い換え
  "表彰式はいつ？"        ← 関連質問
  "写真ある？見たいな"     ← 同じ好奇心、別の表現
  "太郎に直接聞きたい"     ← メタ的な要求
```

---

## 6. 多様性制御の追加軸

### 6.1 抽象度スライダー

同じ意図でも抽象度を変えると全く違う候補になる。

```
「太郎の表彰」に対する質問軸:

具体的 ←──────────────────→ 抽象的

"何の教科？"  "どんな表彰？"  "最近学校どう？"  "太郎元気？"
```

初回は中間の抽象度。外れたら具体と抽象を振る。

### 6.2 感情温度

同じ内容でも温度感を変える。

```
クール            ニュートラル          ウォーム
"ほう、表彰か"    "何で表彰されたの？"   "太郎やるなぁ！何の表彰！？"
```

### 6.3 文の長さ

短い候補と長い候補を混ぜる。

```
短: "すごい"
中: "すごいじゃん！何の表彰？"
長: "太郎すごいなぁ、花子さんに似て頭いいんだろうね"
```

### 6.4 多様性の評価指標

| 指標 | 説明 | 適用レベル |
|---|---|---|
| **Self-BLEU** | 候補間の相互BLEU（低いほど多様） | 表面的多様性 |
| **Distinct-n** | ユニークなn-gramの割合 | 表面的多様性 |
| **Embedding distance** | 候補間のembedding距離 | 意味的多様性 |
| **Intent axis coverage** | 7意図軸のカバー率 | 意図的多様性（VoiceReach固有） |
| **Entropy@K** | top-K候補の予測エントロピー | 候補分布の多様性 |

---

## 7. レイヤー別の候補生成

### 7.1 Level 0: ゼロ操作候補（AI先読み）

1文字も入力しない段階で、状況コンテキストのみから候補を生成する。

```
[状況] 朝7:00 / 介護者入室 / 前回発話から6時間

  "おはよう"          [挨拶]
  "トイレお願い"       [行動要求]
  "水が欲しい"         [行動要求]
  "よく眠れたよ"       [状態報告]

推定命中率: 40~60%
必要操作: 1回（チラ見 + タップ）
```

### 7.2 Level 1: リスニングモード候補

相手の発話を音声認識し、返答候補を自動生成する。SpeakEase (Xu et al., 2025) のASR + LLM統合と類似した設計だが、VoiceReachは「1つの最適応答」ではなく「4つの多様な候補」を生成する点が差別化（レポートA3 Section 1.1.4）。

```
介護者: "お昼ごはん、おかゆとうどんどっちがいい？"
  ↓ ASR + LLM
自動候補:
  "おかゆ"             [選択回答]
  "うどんがいいな"      [選択回答]
  "どっちでもいいよ"    [委任]
  "まだいらない"        [拒否]

推定命中率: 70~80%（質問への回答は選択肢が限定的）
必要操作: 1回
```

### 7.3 Level 2: カテゴリ選択

Level 0/1の候補が合わない場合。

```
  "体調"  "要望"  "会話"  "緊急"
      ↓ 「要望」を選択
  "食べ物/飲み物"  "環境"  "体勢"  "その他"
      ↓ 「環境」を選択
  "エアコン下げて"  "テレビ消して"  "カーテン開けて"  "照明変えて"

必要操作: 3回
```

### 7.4 Level 3: 2ストローク文字入力

自由文字入力が必要な場合のフォールバック。Gaines & Vertanen (EMNLP 2025) のサブワードLLMからの文字確率抽出アルゴリズムは、このレベルでの予測精度向上に直接適用可能（レポートA3 Section 1.1.3）。

```
操作1: 行選択（か行）
操作2: 段選択（い段）→ 「き」確定
  → LLM予測: [今日] [気持ち] [昨日] [聞いて]
操作3: 「今日」を選択
  → 文予測: [今日は調子がいい] [今日の予定は？] [今日はありがとう]
操作4: 文を選択 → 確定

必要操作: 3~6回（予測精度次第）
```

---

## 8. 感情・トーンの付加

テキスト候補を選択した後、発話トーンを選択できる。

```
"今日は調子がいい" を選択後:

  明るく    → 音声合成: 明るいトーン
  普通に    → 音声合成: ニュートラル
  控えめに  → 音声合成: 静かなトーン
  強調して  → 音声合成: 力強いトーン

デフォルト: PVPの対人スタイルから自動選択
省略可: タップ2回で「普通に」で即発話
```

---

## 9. 「選ばない」が最も雄弁な入力

本システムの最重要設計思想。

従来のDwell方式では「選ばない=何も起きない」だったが、VoiceReachでは「選ばなかったこと」自体が次の探索方向を決定する情報になる。これは既存のAAC研究（SpeakFaster, SpeakEase, SocializeChat）には見られない、VoiceReach固有のアプローチである（レポートA3 Section 2.3）。

```
患者が明示的に操作しなくても:
  - どの候補をどれくらい見たか → 関心の方向
  - どの候補にどんな感情反応を示したか → 快不快の判定
  - 全体の視線パターン → 困惑/不満/無関心の判別

これらが自動的に次の候補生成に反映される。
患者は「違う」ボタンを押す必要すらなく、
ただ自然に画面を見ているだけでシステムが寄ってくる。
```

---

## 10. オフライン動作

### 10.1 オフライン時のモデル切り替え

VoiceReachの要件として「70%以上の機能がオフラインで動作」が必要（レポートB1 Section 9）。

```
オンライン時:
  Stage 1 (Qwen3-0.6B) → Stage 2 (Qwen3-1.7B) → Stage 3 (クラウド) → 高品質候補

オフライン時:
  Stage 1 (Qwen3-0.6B) → Stage 2 (Qwen3-1.7B) → そのまま最終候補
  追加最適化:
    - KVキャッシュを積極的に保持
    - よく使う候補のローカルキャッシュ
    - PVPに基づくテンプレート候補の事前生成
```

### 10.2 オフライン品質の確保

オフライン時はQwen3-1.7Bの出力が最終候補となるため、以下の品質確保策が重要:

1. **PVPの簡易版をローカルに最適化**: ローカルLLM向けにPVPを要約し、重要な語彙パターンと口調のみを含む短縮版を用意
2. **頻出パターンのプリコンパイル**: 朝の挨拶、食事時の選択、体調報告などの頻出シナリオは、事前にテンプレート候補を生成・保存
3. **ファインチューニングの検討**: オフライン品質が不十分な場合、Qwen3-1.7Bの日本語AAC応答に特化したLoRA/QLoRAファインチューニングを検討（レポートB1 Section 9.2）

---

## 11. コスト分析

### 11.1 クラウドAPI月額コスト

75回/日の呼び出しを想定（入力~3500トークン、出力~150トークン）（レポートB2 Section 3.2）:

| シナリオ | 月額コスト | 構成 |
|---|---|---|
| 基本使用（50回/日） | **$1.5-2.0** | Gemini 2.5 Flash + キャッシュ |
| 標準使用（75回/日） | **$2.5-4.0** | Gemini主体 + Claude時々 |
| 高頻度使用（100回/日） | **$3.5-6.0** | Gemini主体 + Claude 20% |
| 先読み込み（+バッチ） | **+$1.0-2.0** | バッチAPI 50%割引適用 |

月額$5-8程度で高品質なAAC候補生成が実現可能。ALS支援機器の月額コストとして非常に合理的。

### 11.2 プロンプトキャッシュによるコスト削減

PVP 2000トークンのキャッシュヒット率80%を想定:

| モデル | キャッシュなし/月 | キャッシュあり/月 | 節約率 |
|---|---|---|---|
| Gemini 2.5 Flash | $3.2 | ~$2.0 | 38% |
| Claude Haiku 4.5 | $9.5 | ~$5.2 | 45% |
| GPT-4.1 nano | $0.9 | ~$0.6 | 33% |

---

## 12. 予測精度の目標と評価

### 12.1 命中率目標の妥当性

先行研究との比較に基づく現実的な目標設定（レポートA3 Section 5.2）:

| レベル | 目標 | 現実的初期値 | 成熟期の目標 | 根拠 |
|---|---|---|---|---|
| Level 0（ゼロ操作） | 40-60% | 25-35% | 45-55% | 定型場面のみで60%可能、混合平均は低い |
| Level 1（リスニング） | 70-80% | 55-65% | 70-80% | 質問応答中心なら達成可能 |
| Level 2（カテゴリ） | N/A（確実） | 90%+ | 95%+ | 階層メニューのため高精度 |
| Level 3（文字入力） | N/A（確実） | 80-90% | 90%+ | LLM予測+文字入力で高精度 |

**注**: 「命中率」は「4候補中1つ以上が許容範囲内」（Top-4 accuracy）として定義する。この定義であれば上記目標は達成可能性が高い。

### 12.2 評価方法論

#### オフライン評価

1. **過去の会話データからの予測テスト**: 実際の患者の会話ログから{context, actual_response}ペアを抽出し、contextからの候補生成がactual_responseをどの程度カバーするかを評価
2. **意図軸カバレッジ**: 4候補が何種類の意図軸をカバーしているかの平均値
3. **表面多様性**: Self-BLEU、Distinct-1/2/3
4. **意味的多様性**: BERTScore間の候補ペア距離
5. **スタイル一致**: PVPに記載されたスタイル特性との一致率（human evaluation）

#### オンライン評価

1. **Top-4 Hit Rate**: 4候補中1つ以上を患者が選択した割合
2. **巡数分布**: 何巡目で意図に到達したかの分布
3. **操作到達時間**: 意図の発話完了までの時間
4. **Keystroke Saving Rate (KSR)**: 従来の文字入力と比較した操作削減率
5. **患者満足度**: 定期的なアンケート（5段階、視線入力対応）
6. **介護者評価**: 「患者らしい」と感じる割合

---

## 13. 操作負荷の比較

| 方式 | 1文の平均操作数 | 眼球移動量 | 意図到達時間 |
|---|---|---|---|
| 従来50音 Dwell | 20~30回凝視 | 大（画面全体） | 30~60秒 |
| ゾーン+タップ | 10~15回 | 中（ゾーン間移動） | 10~20秒 |
| AI先読み+タップ | 1~3回 | 小（近傍のみ） | 2~5秒 |
| リスニングモード | 1回 | 極小（方向チラ見） | 1~2秒 |
| パッシブ検出 | 0回 | なし | 自動 |

---

## 14. 将来の拡張

### 14.1 モデルの世代交代への対応

ローカルLLMとクラウドLLMの両方で急速なモデル更新が予想される（レポートB1 Section 10, B2 Section 7）。

- **抽象化レイヤー**: プロバイダー非依存のリクエスト/レスポンスインターフェースを定義し、モデル差し替えを容易にする
- **ローカル**: Qwen3シリーズの後継、Sarashina2.2-3B（日本語特化、MIT）への切り替えも検討対象（レポートB1 Section 7.1）
- **クラウド**: Gemini 3 Flash (GA)、Claude 5 Haiku等の将来モデルへの移行パスを確保

### 14.2 日本語AAC + LLMのフロントランナーとして

日本語AAC領域におけるLLM活用の学術論文は英語圏に比べ極めて限定的であり、LLMと意思伝達装置を直接組み合わせた研究は確認できなかった（レポートA3 Section 1.5）。VoiceReachは当該領域のフロントランナーとなりうる。日本語固有の課題（文字種の多さ、語順の自由度、敬語体系）への対応が差別化要素となる。

---

## 参考資料

本ドキュメントの技術判断は以下の調査レポートに基づく:

- **レポートA3** (LLM x AAC候補生成): 先行研究分析、ICLパーソナライゼーション、多様性制御技術、プロンプト設計原則
- **レポートB1** (ローカルLLM): Qwen3シリーズの性能評価、量子化戦略、KVキャッシュ最適化、MLX/vllm-mlxベンチマーク
- **レポートB2** (クラウドLLM): Gemini/Claude/GPT比較、レイテンシ分析、コスト分析、フォールバック設計

### 主要引用文献

- Cai, S. et al. (2024). "Using large language models to accelerate communication for eye gaze typing users with ALS." *Nature Communications*.
- Cai, S. et al. (2023). "The less I type, the better." *CHI 2023*.
- Xu, Y. et al. (2025). "SpeakEase: Your voice is your voice." *arXiv:2503.17479*.
- Liye et al. (2023/2025). "SocializeChat." *UbiComp 2023*.
- Wang, Z. et al. (2025). "Catch Me If You Can?" *EMNLP 2025 Findings*.
- Chu, K. et al. (2025). "Exploring and Controlling Diversity in LLM-Agent Conversation." *EMNLP 2025 Findings*.
- Salemi, A. et al. (2024). "LaMP: When LLMs Meet Personalization." *ACL 2024*.
- Gaines, D. & Vertanen, K. (2025). "Adapting LLMs for Character-based AAC." *Findings of EMNLP 2025*.
- Holtzman, A. et al. (2020). "The Curious Case of Neural Text Degeneration." *ICLR 2020*.
- Vijayakumar, A.K. et al. (2018). "Diverse Beam Search." *AAAI 2018*.
- Zhang, Z. et al. (2025). "Personalization of LLMs: A Survey." *TMLR*.
