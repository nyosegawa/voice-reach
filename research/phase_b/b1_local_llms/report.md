# B1 -- ローカルLLM調査レポート

**調査日**: 2026-02-17
**対象**: VoiceReach 二段構成推論の Stage 1（即座応答）に最適なローカルLLMの選定

---

## 1. エグゼクティブサマリー

VoiceReachのStage 1推論は、トリガーイベント発生から200ms以内に4つの候補を生成する必要がある。システムプロンプト（PVP + タスク指示）が約2000トークン、コンテキストが約500トークン、出力が4候補で約100-150トークンと想定される。

調査の結果、**Qwen3-1.7B (Q4_K_M) + MLX** の組み合わせが、Apple Silicon上でのVoiceReach Stage 1推論に最も適していると結論づける。日本語品質、推論速度、メモリ使用量のバランスが最も優れている。

---

## 2. モデル比較マトリクス

### 2.1 候補モデル一覧

| モデル | パラメータ数 | 日本語品質 | M2での推論速度(tok/s) | M3/M4での推論速度(tok/s) | VRAM (Q4) | ライセンス |
|---|---|---|---|---|---|---|
| **Qwen3-0.6B** | 0.6B | C+ (基礎的) | ~440 | ~525 | ~0.5GB | Apache 2.0 |
| **Qwen3-1.7B** | 1.7B | B+ (実用的) | ~250 | ~320 | ~1.2GB | Apache 2.0 |
| **Qwen3-4B** | 4B | A- (高品質) | ~130 | ~160 | ~2.5GB | Apache 2.0 |
| **Qwen2.5-3B** | 3B | B+ (実用的) | ~150 | ~200 | ~2.0GB | Apache 2.0 |
| **Gemma 3 1B** | 1B | B- (許容範囲) | ~350 | ~420 | ~0.8GB | Gemma License |
| **Gemma 3 4B** | 4B | B+ (実用的) | ~120 | ~150 | ~2.5GB | Gemma License |
| **Llama 3.2 1B** | 1B | C (弱い) | ~350 | ~430 | ~0.8GB | Llama License |
| **Llama 3.2 3B** | 3B | C+ (基礎的) | ~155 | ~200 | ~2.0GB | Llama License |
| **Phi-4 Mini** | 3.8B | B (許容範囲) | ~130 | ~160 | ~2.3GB | MIT |
| **Sarashina2.2-3B** | 3B | A- (高品質) | ~150 | ~195 | ~2.0GB | MIT |
| **LLM-jp-3-3.7B** | 3.7B | B+ (実用的) | ~120 | ~155 | ~2.3GB | Apache 2.0 |

**注記**: 推論速度はMLXフレームワーク、Q4_K_M量子化、トークン生成時の値。プロンプト処理速度は別途。日本語品質スコアは複数ベンチマークの総合評価。

### 2.2 日本語品質の詳細評価

| モデル | JGLUE (相対) | 日本語指示追従 | 敬語/方言対応 | 短文生成品質 | PVP活用能力 |
|---|---|---|---|---|---|
| Qwen3-0.6B | 低 | 基礎的 | 不十分 | 許容範囲 | 限定的 |
| **Qwen3-1.7B** | **中** | **良好** | **基礎的** | **良好** | **実用的** |
| Qwen3-4B | 高 | 優秀 | 良好 | 高品質 | 高い |
| Qwen2.5-3B | 中+ | 良好 | 基礎的 | 良好 | 実用的 |
| Gemma 3 1B | 中- | 中程度 | 限定的 | 許容範囲 | 限定的 |
| Gemma 3 4B | 中+ | 良好 | 基礎的 | 良好 | 実用的 |
| Llama 3.2 1B | 低 | 弱い | 非対応 | 弱い | 弱い |
| Llama 3.2 3B | 低+ | 基礎的 | 非対応 | 基礎的 | 限定的 |
| Phi-4 Mini | 中 | 中程度 | 限定的 | 中程度 | 中程度 |
| **Sarashina2.2-3B** | **高** | **良好** | **良好** | **高品質** | **実用的** |
| LLM-jp-3-3.7B | 中+ | 良好 | 良好 | 良好 | 実用的 |

**評価基準**:
- **JGLUE**: Japanese General Language Understanding Evaluation の相対スコア
- **日本語指示追従**: 複雑なシステムプロンプト（意図軸分散指示等）を日本語で正しく理解し実行できるか
- **敬語/方言対応**: PVPに含まれる患者固有の語彙パターン（「ぼちぼち」「だよなぁ」等）を模倣できるか
- **短文生成品質**: 1-2文の自然な日本語候補を生成できるか
- **PVP活用能力**: In-Context Learningで個人プロファイルを活用できるか

---

## 3. 量子化の影響分析

### 3.1 量子化レベル別の特性

| 量子化 | ファイルサイズ比 | 品質低下 | 推論速度向上 | 日本語への影響 |
|---|---|---|---|---|
| FP16 (基準) | 1.0x | なし | 基準 | なし |
| Q8_0 | 0.53x | 極小 (~1%) | 1.3-1.5x | ほぼなし |
| Q6_K | 0.43x | 小 (~2%) | 1.5-1.7x | 微小 |
| **Q5_K_M** | **0.37x** | **小 (~3%)** | **1.6-1.8x** | **許容範囲** |
| **Q4_K_M** | **0.30x** | **中 (~5-8%)** | **1.8-2.2x** | **注意が必要** |
| Q3_K_M | 0.25x | 大 (~10-15%) | 2.0-2.5x | 顕著な劣化 |
| Q2_K | 0.20x | 非常に大 | 2.3-2.8x | 使用不可レベル |

### 3.2 小型モデルにおける量子化の注意点

小型モデル（3B以下）は大型モデルと比較して量子化の影響を強く受ける。特に以下の点に注意が必要：

1. **1B以下のモデル**: Q4_K_Mでも日本語品質が顕著に劣化する可能性がある。Q5_K_M以上を推奨。
2. **1.5B-3Bのモデル**: Q4_K_Mが実用下限。Q5_K_Mであれば品質低下は許容範囲内。
3. **日本語固有の問題**: 量子化により低頻度トークン（方言、口語表現、固有名詞）の生成精度が低下する傾向がある。PVPに含まれる個人特有の表現パターンへの影響を個別にテストする必要がある。

### 3.3 推奨量子化レベル

| モデルサイズ | VoiceReach推奨量子化 | 理由 |
|---|---|---|
| 0.6B | Q5_K_M or Q6_K | 小型のため量子化耐性が低い |
| 1.5B-1.7B | Q4_K_M or Q5_K_M | バランスが良い。速度が重要なら Q4_K_M |
| 3B-4B | Q4_K_M | 十分な量子化耐性あり。メモリ節約が重要 |

---

## 4. 推論フレームワーク比較

### 4.1 Apple Silicon向けフレームワーク比較

| フレームワーク | トークン生成速度 | TTFT | メモリ効率 | 成熟度 | API品質 | 備考 |
|---|---|---|---|---|---|---|
| **MLX** | **最速** (基準の1.3-1.5x) | 良好 | 高 | 高 | 優秀 | Apple公式。Apple Siliconに最適化 |
| **vllm-mlx** | **最速** (MLX+batching) | 優秀 | 高 | 中+ | 優秀 | OpenAI互換API。連続バッチング対応 |
| llama.cpp (Metal) | 速い (基準) | 良好 | 中 | 非常に高 | 良好 | 最も広いモデルサポート |
| MLC-LLM | 速い | **最速** | 高 | 中 | 良好 | 最低TTFTだがモデル変換が必要 |
| Ollama | やや遅い | 中 | 中 | 高 | 非常に良好 | llama.cppベース。導入が最も簡単 |
| PyTorch MPS | 遅い | 遅い | 低 | 高 | 良好 | 開発/デバッグ用途 |

### 4.2 ベンチマークデータ（MLXフレームワーク、M4 Max）

学術論文（arXiv:2601.19139, arXiv:2511.05502）およびApple Machine Learning Researchの公開データに基づく:

| モデル | フレームワーク | トークン生成速度 (tok/s) | プロンプト処理速度 (tok/s) |
|---|---|---|---|
| Qwen3-0.6B (4-bit) | vllm-mlx | 525.5 | 高速 |
| Llama-3.2-1B (4-bit) | vllm-mlx | 461.9 | 高速 |
| Llama-3.2-1B (4-bit) | llama.cpp | 331.3 | 中程度 |
| Llama-3.2-3B (4-bit) | vllm-mlx | 203.6 | 中程度 |
| Llama-3.2-3B (4-bit) | llama.cpp | 155.8 | 中程度 |
| Qwen3-4B (4-bit) | vllm-mlx | ~159 | 中程度 |

### 4.3 チップ世代別メモリ帯域幅

推論速度はメモリ帯域幅に強く依存する:

| チップ | メモリ帯域幅 | 推論速度の相対値 | 備考 |
|---|---|---|---|
| M1 | 68.25 GB/s | 0.34x | 最低限動作 |
| M1 Pro/Max | 200-400 GB/s | 1.0-2.0x | 実用的 |
| M2 | 100 GB/s | 0.50x | 小型モデルなら可 |
| M2 Pro/Max | 200-400 GB/s | 1.0-2.0x | 実用的 |
| M3 | 100 GB/s | 0.50x | 小型モデルなら可 |
| M3 Pro/Max | 150-400 GB/s | 0.75-2.0x | 実用的 |
| M4 | 120 GB/s | 0.60x | 小型モデルなら可 |
| M4 Pro/Max | 273-546 GB/s | 1.37-2.73x | 推奨 |
| M5 | 153 GB/s | 0.77x | M4比19-27%改善 |

---

## 5. 200ms以内の候補生成は実現可能か

### 5.1 レイテンシの構成要素

候補生成の総レイテンシは以下の合計:

```
総レイテンシ = プロンプト処理時間 + トークン生成時間

プロンプト処理時間:
  = (システムプロンプト + コンテキスト) / プロンプト処理速度
  = ~2500トークン / プロンプト処理速度

トークン生成時間:
  = 出力トークン数 / トークン生成速度
  = ~100-150トークン / トークン生成速度
```

### 5.2 モデル別レイテンシ推定（M4 Pro/Max, MLX, Q4_K_M）

| モデル | プロンプト処理 | トークン生成 | 推定総レイテンシ | 200ms達成 |
|---|---|---|---|---|
| Qwen3-0.6B | ~30ms | ~250ms (130tok) | **~280ms** | 惜しい |
| Qwen3-1.7B | ~60ms | ~400ms (130tok) | **~460ms** | 未達 |
| Qwen3-4B | ~100ms | ~810ms (130tok) | **~910ms** | 未達 |
| Llama-3.2-1B | ~30ms | ~280ms (130tok) | **~310ms** | 未達 |
| Llama-3.2-3B | ~50ms | ~640ms (130tok) | **~690ms** | 未達 |

### 5.3 200ms達成のための最適化戦略

**重要な発見**: 素のままでは200ms以内に4候補を生成することは困難。以下の最適化が必須：

#### 戦略A: プロンプトキャッシュ（KVキャッシュ再利用）
- システムプロンプト（PVP）は頻繁に変わらないため、KVキャッシュを保持し再利用
- vllm-mlxのtext prefix cachingにより**TTFT 5.8x高速化**が報告されている
- プロンプト処理時間を実質ゼロに近づけられる

```
最適化後レイテンシ = ~0ms (キャッシュヒット) + トークン生成時間
```

#### 戦略B: 出力トークン数の削減
- 4候補をそれぞれ短文（10-20トークン/候補）に制限
- 意図軸タグを含めても60-80トークン程度に圧縮可能

```
最適化後 (Qwen3-0.6B): ~0ms + ~150ms (80tok/525tok/s) = ~150ms ✅
最適化後 (Qwen3-1.7B): ~0ms + ~250ms (80tok/320tok/s) = ~250ms (惜しい)
```

#### 戦略C: 並列生成 + 早期表示
- 4候補を逐次ではなく並列ストリーミングで生成
- 最初の候補が完成次第UIに表示（プログレッシブ表示）
- 体感レイテンシを大幅に短縮

#### 戦略D: モデルの使い分け
- 最初の2候補: Qwen3-0.6B（超高速、品質は妥協）
- 残り2候補: Qwen3-1.7B（やや遅いが高品質）
- バックグラウンドでクラウドLLMが全4候補を再生成

### 5.4 現実的なレイテンシ目標

| ハードウェア | モデル | KVキャッシュ | 出力80tok | 推定レイテンシ | 評価 |
|---|---|---|---|---|---|
| M4 Pro/Max | Qwen3-0.6B | あり | 80tok | **~150ms** | 200ms達成 |
| M4 Pro/Max | Qwen3-1.7B | あり | 80tok | **~250ms** | 惜しい（Stage 1としては許容可能） |
| M3 Pro/Max | Qwen3-0.6B | あり | 80tok | **~200ms** | ギリギリ達成 |
| M2 Pro/Max | Qwen3-0.6B | あり | 80tok | **~250ms** | 許容範囲 |
| M2/M3/M4 (無印) | Qwen3-0.6B | あり | 80tok | **~350ms** | 厳しい |

---

## 6. 小型モデルの複雑な指示追従能力

### 6.1 VoiceReachのシステムプロンプト要件

VoiceReachのシステムプロンプトは以下の要素で構成される:

1. **PVP (Personal Voice Profile)**: 語彙パターン、口調、対人スタイル（~2000トークン）
2. **意図軸分散指示**: 7つの意図軸から4つを選び、それぞれ異なる軸の候補を生成する指示
3. **出力フォーマット指示**: タグ付き候補の構造化出力
4. **コンテキスト活用指示**: 環境情報、会話履歴の活用方法

### 6.2 モデルサイズ別の指示追従評価

| 能力 | 0.6Bモデル | 1.5-1.7Bモデル | 3-4Bモデル |
|---|---|---|---|
| 基本的な日本語生成 | 可能 | 良好 | 優秀 |
| 4候補の同時生成 | 不安定 | 概ね安定 | 安定 |
| 意図軸の区別 | 困難（混同あり） | 概ね区別可能 | 明確に区別 |
| PVP語彙パターンの模倣 | 限定的 | 基礎的な模倣可能 | 良好な模倣 |
| 構造化出力の遵守 | 不安定 | 概ね遵守 | 安定して遵守 |
| 2巡目以降のフィードバック活用 | 困難 | 基礎的に可能 | 良好 |

### 6.3 実用上の対策

1. **プロンプトの簡素化**: ローカルLLM用のシステムプロンプトはクラウド版より簡潔にする
2. **Few-shot例の追加**: 2-3個の出力例を含めることで構造化出力の安定性が大幅に向上
3. **Constrained Decoding**: llama.cppやvllm-mlxのJSON/グラマー制約機能を活用し、出力フォーマットを強制
4. **フォールバック**: 出力パースに失敗した場合はデフォルト候補セットを表示

---

## 7. 日本語特化モデルの検討

### 7.1 Sarashina2.2-3B

SB Intuitionsが開発した日本語特化モデル。10兆トークンでの事前学習と合成データ強化が特徴。

**利点**:
- NIILC（日本語QA）で7Bや70Bモデルを上回る性能
- JMMLU（日本語マルチタスク言語理解）で競争力のあるスコア
- MITライセンスで商用利用可
- 敬語・方言・口語表現の品質が高い

**欠点**:
- 3Bモデルのため推論速度はQwen3-0.6Bより遅い（~195 tok/s on M4 Pro/Max）
- MLX対応状況の確認が必要
- 英語タスクでの性能はQwen3に劣る（多言語対応が弱い）

### 7.2 LLM-jp-3-3.7B

国立情報学研究所（NII）が開発。llm-jp-corpus v3で学習。

**利点**:
- 日本語の指示追従能力が高い（LLM-jp-3.1シリーズで大幅改善）
- 研究用途でのエコシステムが充実

**欠点**:
- 3.7BとやAや大きく、200ms目標には厳しい
- 一般的なベンチマークではQwen3-4Bに劣る傾向

### 7.3 判断

日本語品質を最重視する場合はSarashina2.2-3Bが有力だが、VoiceReachでは「暫定候補」としての役割であるため、速度を優先してQwen3シリーズを推奨する。Stage 2のクラウドLLMが高品質候補で上書きするため、Stage 1では「方向性の正しい候補」を高速に出すことが最重要。

---

## 8. 推奨構成

### 8.1 プライマリ推奨: Qwen3-1.7B (Q4_K_M) + MLX/vllm-mlx

| 項目 | 詳細 |
|---|---|
| モデル | Qwen3-1.7B-Instruct |
| 量子化 | Q4_K_M (GGUF) / 4-bit (MLX) |
| フレームワーク | vllm-mlx (推奨) または MLX直接 |
| 推定レイテンシ | 250-350ms (KVキャッシュ利用時) |
| メモリ使用量 | ~1.2GB |
| 対応ハードウェア | M2 Pro以上推奨、M1 Pro以上で動作 |

**選定理由**:
- Qwen3シリーズは日本語を含む100以上の言語をサポートし、多言語指示追従能力が高い
- 1.7Bは「Qwen2.5-3Bと同等の性能」と公式発表されており、パラメータ効率が優秀
- 意図軸分散という複雑な指示にも概ね対応可能
- Apache 2.0ライセンスで商用利用に制約なし

### 8.2 セカンダリ推奨（速度最優先時）: Qwen3-0.6B (Q5_K_M) + vllm-mlx

| 項目 | 詳細 |
|---|---|
| モデル | Qwen3-0.6B-Instruct |
| 量子化 | Q5_K_M（小型のため量子化を緩める） |
| フレームワーク | vllm-mlx |
| 推定レイテンシ | 150-200ms (KVキャッシュ利用時) |
| メモリ使用量 | ~0.5GB |
| 対応ハードウェア | M1以上で動作 |

**選定理由**:
- 200ms目標を達成できる唯一のモデルサイズ
- 日本語品質は「暫定候補」として最低限許容可能
- メモリ使用量が非常に少なく、他のコンポーネント（視線推定、音声認識等）との共存が容易

### 8.3 ハイブリッド戦略（最も推奨）

```
トリガーイベント発生
  ├→ [即座: ~150ms] Qwen3-0.6B → 暫定候補4つ → UI即表示
  ├→ [短遅延: ~350ms] Qwen3-1.7B → 改善候補4つ → UI差し替え
  └→ [背景: 1-2秒] クラウドLLM → 高品質候補4つ → UI差し替え
```

この三段構成により:
- 150msで「何か」が表示される（応答性）
- 350msで「それなりに良い」候補に更新される（品質向上）
- 1-2秒で「最高品質」の候補に到達する（最終品質）

### 8.4 フレームワーク推奨

| 優先度 | フレームワーク | 理由 |
|---|---|---|
| 1 | **vllm-mlx** | OpenAI互換API、KVキャッシュ、連続バッチング、MLX比21-87%高速 |
| 2 | MLX (mlx-lm) | Apple公式、安定性が高い、ドキュメント充実 |
| 3 | llama.cpp (Metal) | クロスプラットフォーム対応、最多のモデルサポート |

---

## 9. オフライン動作の設計

### 9.1 オフライン時のモデル切り替え

VoiceReachの要件として「70%以上の機能がオフラインで動作」が必要。

```
オンライン時:
  Stage 1 (ローカル) → Stage 2 (クラウド) → 高品質候補

オフライン時:
  Stage 1 (ローカル) → そのまま使用 → ローカル品質候補
  追加最適化:
    - KVキャッシュを積極的に保持
    - よく使う候補のローカルキャッシュ
    - PVPに基づくテンプレート候補の事前生成
```

### 9.2 オフライン品質の確保

オフライン時はQwen3-1.7Bの出力がそのまま最終候補となるため、以下の品質確保策が重要:

1. **PVPの簡易版をローカルに最適化**: ローカルLLM向けにPVPを要約し、重要な語彙パターンと口調のみを含む短縮版を用意
2. **頻出パターンのプリコンパイル**: 朝の挨拶、食事時の選択、体調報告などの頻出シナリオは、事前にテンプレート候補を生成・保存
3. **ファインチューニングの検討**: オフライン品質が不十分な場合、Qwen3-1.7Bの日本語AAC応答に特化したLoRA/QLoRAファインチューニングを検討

---

## 10. リスクと課題

| リスク | 影響 | 対策 |
|---|---|---|
| 小型モデルの日本語品質不足 | 候補が不自然で使えない | Sarashina2.2-3Bへの切り替え、またはQwen3-4Bの採用 |
| 200msの厳格な達成困難 | ユーザー体感の遅延 | ハイブリッド三段構成（150ms+350ms+1-2秒）で対応 |
| Apple Silicon以外のハードウェア | Windows/Linuxでの性能低下 | llama.cppによるクロスプラットフォーム対応 |
| モデルの急速な世代交代 | 選定モデルの陳腐化 | 抽象化レイヤーを設け、モデル差し替えを容易に |
| PVP活用の品質 | 個人性の再現不足 | クラウドLLMへの依存度を上げる設計 |

---

## 11. 参考資料

- [Production-Grade Local LLM Inference on Apple Silicon (arXiv:2511.05502)](https://arxiv.org/abs/2511.05502)
- [Native LLM and MLLM Inference at Scale on Apple Silicon (arXiv:2601.19139)](https://arxiv.org/html/2601.19139v1)
- [Benchmarking On-Device Machine Learning on Apple Silicon with MLX (arXiv:2510.18921)](https://arxiv.org/abs/2510.18921)
- [Qwen3 Technical Report](https://qwenlm.github.io/blog/qwen3/)
- [Qwen3 GitHub Repository](https://github.com/QwenLM/Qwen3)
- [vllm-mlx GitHub Repository](https://github.com/waybarrios/vllm-mlx)
- [Gemma 3 Technical Report](https://arxiv.org/html/2503.19786v1)
- [Performance of llama.cpp on Apple Silicon M-series](https://github.com/ggml-org/llama.cpp/discussions/4167)
- [Small Language Models 2026 Guide](https://localaimaster.com/blog/small-language-models-guide-2026)
- [Swallow LLM Leaderboard (Japanese LLM Evaluation)](https://swallow-llm.github.io/leaderboard/about.en.html)
- [Overview of Japanese LLMs (awesome-japanese-llm)](https://github.com/llm-jp/awesome-japanese-llm)
- [Sarashina2.2-3B (SB Intuitions)](https://www.sbintuitions.co.jp/en/news/press/20240614_01/)
- [LLM-jp-3 Series (NII)](https://llm-jp.nii.ac.jp/en/news/release-of-llm-jp-3-1-series-instruct4/)
- [Apple MLX Framework](https://machinelearning.apple.com/research/exploring-llms-mlx-m5)
- [GGUF Quantization Guide](https://local-ai-zone.github.io/guides/what-is-ai-quantization-q4-k-m-q8-gguf-guide-2025.html)
- [The Rakuda Benchmark (Japanese LLM Evaluation)](https://yuzuai.jp/blog/rakuda)
